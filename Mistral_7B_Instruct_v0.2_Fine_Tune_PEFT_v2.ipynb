{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/Mistral_7B_Instruct_v0.2_Fine_Tune_PEFT_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:13:30.388726Z",
          "iopub.execute_input": "2024-12-04T11:13:30.388998Z",
          "iopub.status.idle": "2024-12-04T11:14:19.194784Z",
          "shell.execute_reply.started": "2024-12-04T11:13:30.388971Z",
          "shell.execute_reply": "2024-12-04T11:14:19.193613Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-12-04T11:14:19.196716Z",
          "iopub.execute_input": "2024-12-04T11:14:19.197008Z",
          "iopub.status.idle": "2024-12-04T11:14:38.414623Z",
          "shell.execute_reply.started": "2024-12-04T11:14:19.196978Z",
          "shell.execute_reply": "2024-12-04T11:14:38.413674Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-12-04T11:14:38.415757Z",
          "iopub.execute_input": "2024-12-04T11:14:38.416027Z",
          "iopub.status.idle": "2024-12-04T11:14:38.422838Z",
          "shell.execute_reply.started": "2024-12-04T11:14:38.416001Z",
          "shell.execute_reply": "2024-12-04T11:14:38.421855Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n",
        "#model_name = url.split('.co/')[-1]\n",
        "\n",
        "model_name = 'unsloth/mistral-7b-instruct-v0.2'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:14:38.425170Z",
          "iopub.execute_input": "2024-12-04T11:14:38.425524Z",
          "iopub.status.idle": "2024-12-04T11:14:38.451536Z",
          "shell.execute_reply.started": "2024-12-04T11:14:38.425479Z",
          "shell.execute_reply": "2024-12-04T11:14:38.450857Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "  if base == True:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype = torch.float16,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "  else:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = True,\n",
        "      bnb_4bit_quant_type = 'nf4',\n",
        "      bnb_4bit_compute_dtype = torch.float16,\n",
        "      bnb_4bit_use_double_quant = True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config = bnb_config,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:14:38.452519Z",
          "iopub.execute_input": "2024-12-04T11:14:38.452868Z",
          "iopub.status.idle": "2024-12-04T11:14:38.465007Z",
          "shell.execute_reply.started": "2024-12-04T11:14:38.452825Z",
          "shell.execute_reply": "2024-12-04T11:14:38.464259Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:14:38.465959Z",
          "iopub.execute_input": "2024-12-04T11:14:38.466193Z",
          "iopub.status.idle": "2024-12-04T11:22:36.335078Z",
          "shell.execute_reply.started": "2024-12-04T11:14:38.466170Z",
          "shell.execute_reply": "2024-12-04T11:22:36.334182Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:36.336346Z",
          "iopub.execute_input": "2024-12-04T11:22:36.336622Z",
          "iopub.status.idle": "2024-12-04T11:22:36.344235Z",
          "shell.execute_reply.started": "2024-12-04T11:22:36.336596Z",
          "shell.execute_reply": "2024-12-04T11:22:36.343328Z"
        },
        "outputId": "44c7b5fc-13ff-42a4-a0ed-7e7f232e7102"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3752071168\nTrainable parameters : 262410240\nTrainable percentage: 6.99%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:36.345398Z",
          "iopub.execute_input": "2024-12-04T11:22:36.345656Z",
          "iopub.status.idle": "2024-12-04T11:22:37.638982Z",
          "shell.execute_reply.started": "2024-12-04T11:22:36.345632Z",
          "shell.execute_reply": "2024-12-04T11:22:37.638275Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n",
        "#dataset_name = url.split('datasets/')[-1]\n",
        "\n",
        "dataset_name = 'yahma/alpaca-cleaned'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:37.639935Z",
          "iopub.execute_input": "2024-12-04T11:22:37.640202Z",
          "iopub.status.idle": "2024-12-04T11:22:37.644073Z",
          "shell.execute_reply.started": "2024-12-04T11:22:37.640176Z",
          "shell.execute_reply": "2024-12-04T11:22:37.643259Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:37.647936Z",
          "iopub.execute_input": "2024-12-04T11:22:37.648173Z",
          "iopub.status.idle": "2024-12-04T11:22:37.657161Z",
          "shell.execute_reply.started": "2024-12-04T11:22:37.648150Z",
          "shell.execute_reply": "2024-12-04T11:22:37.656390Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:37.658195Z",
          "iopub.execute_input": "2024-12-04T11:22:37.658762Z",
          "iopub.status.idle": "2024-12-04T11:22:40.321565Z",
          "shell.execute_reply.started": "2024-12-04T11:22:37.658735Z",
          "shell.execute_reply": "2024-12-04T11:22:40.320763Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(10000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.322525Z",
          "iopub.execute_input": "2024-12-04T11:22:40.322777Z",
          "iopub.status.idle": "2024-12-04T11:22:40.328701Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.322752Z",
          "shell.execute_reply": "2024-12-04T11:22:40.327966Z"
        },
        "id": "NVk7NVpxiN01"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.329881Z",
          "iopub.execute_input": "2024-12-04T11:22:40.330658Z",
          "iopub.status.idle": "2024-12-04T11:22:40.350939Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.330616Z",
          "shell.execute_reply": "2024-12-04T11:22:40.350143Z"
        },
        "outputId": "8a4f48dd-52de-4b95-ff14-0270d2b8896c"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              output input  \\\n0  1. Eat a balanced and nutritious diet: Make su...         \n1  The three primary colors are red, blue, and ye...         \n2  An atom is the basic building block of all mat...         \n3  There are several ways to reduce air pollution...         \n4  I had to make a difficult decision when I was ...         \n\n                                         instruction  \n0               Give three tips for staying healthy.  \n1                 What are the three primary colors?  \n2                 Describe the structure of an atom.  \n3                   How can we reduce air pollution?  \n4  Pretend you are a project manager of a constru...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>output</th>\n      <th>input</th>\n      <th>instruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n      <td></td>\n      <td>Give three tips for staying healthy.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The three primary colors are red, blue, and ye...</td>\n      <td></td>\n      <td>What are the three primary colors?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>An atom is the basic building block of all mat...</td>\n      <td></td>\n      <td>Describe the structure of an atom.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>There are several ways to reduce air pollution...</td>\n      <td></td>\n      <td>How can we reduce air pollution?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I had to make a difficult decision when I was ...</td>\n      <td></td>\n      <td>Pretend you are a project manager of a constru...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.351786Z",
          "iopub.execute_input": "2024-12-04T11:22:40.352023Z",
          "iopub.status.idle": "2024-12-04T11:22:40.359764Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.351999Z",
          "shell.execute_reply": "2024-12-04T11:22:40.358926Z"
        },
        "outputId": "e57f616f-12fb-46f5-8523-85a13bdcba5f"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n 'input': '',\n 'instruction': 'Give three tips for staying healthy.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.360712Z",
          "iopub.execute_input": "2024-12-04T11:22:40.361002Z",
          "iopub.status.idle": "2024-12-04T11:22:40.369216Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.360978Z",
          "shell.execute_reply": "2024-12-04T11:22:40.368422Z"
        },
        "outputId": "ff566d76-4ae8-402e-aa50-9f89eee6841f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['output', 'input', 'instruction']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.370094Z",
          "iopub.execute_input": "2024-12-04T11:22:40.370333Z",
          "iopub.status.idle": "2024-12-04T11:22:40.380963Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.370310Z",
          "shell.execute_reply": "2024-12-04T11:22:40.380142Z"
        },
        "id": "5dDr4FUkiN01"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def preprocess(examples):\n",
        "  instruction = examples['instruction']\n",
        "  input = examples['input']\n",
        "  output = examples['output']\n",
        "\n",
        "  text = prompt_format.format(instruction, input, output) + EOS_TOKEN\n",
        "  return {'prompt' : text}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.381828Z",
          "iopub.execute_input": "2024-12-04T11:22:40.382088Z",
          "iopub.status.idle": "2024-12-04T11:22:40.389902Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.382049Z",
          "shell.execute_reply": "2024-12-04T11:22:40.389047Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(preprocess, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.390823Z",
          "iopub.execute_input": "2024-12-04T11:22:40.391089Z",
          "iopub.status.idle": "2024-12-04T11:22:40.934840Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.391065Z",
          "shell.execute_reply": "2024-12-04T11:22:40.932349Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.935839Z",
          "iopub.execute_input": "2024-12-04T11:22:40.936088Z",
          "iopub.status.idle": "2024-12-04T11:22:40.941047Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.936063Z",
          "shell.execute_reply": "2024-12-04T11:22:40.940121Z"
        },
        "outputId": "6cecbac4-3d00-4d3f-f073-ec5ddaf522cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.942303Z",
          "iopub.execute_input": "2024-12-04T11:22:40.942969Z",
          "iopub.status.idle": "2024-12-04T11:22:40.967815Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.942929Z",
          "shell.execute_reply": "2024-12-04T11:22:40.967064Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:40.968859Z",
          "iopub.execute_input": "2024-12-04T11:22:40.969517Z",
          "iopub.status.idle": "2024-12-04T11:22:49.034701Z",
          "shell.execute_reply.started": "2024-12-04T11:22:40.969479Z",
          "shell.execute_reply": "2024-12-04T11:22:49.033742Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.035792Z",
          "iopub.execute_input": "2024-12-04T11:22:49.036061Z",
          "iopub.status.idle": "2024-12-04T11:22:49.043325Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.036035Z",
          "shell.execute_reply": "2024-12-04T11:22:49.042297Z"
        },
        "outputId": "200fc190-c2c7-4a3a-bf5f-815eed101c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.044426Z",
          "iopub.execute_input": "2024-12-04T11:22:49.044676Z",
          "iopub.status.idle": "2024-12-04T11:22:49.101886Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.044652Z",
          "shell.execute_reply": "2024-12-04T11:22:49.100928Z"
        },
        "outputId": "5b704464-3f7e-4739-c21c-9d1349c31958"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.102821Z",
          "iopub.execute_input": "2024-12-04T11:22:49.103077Z",
          "iopub.status.idle": "2024-12-04T11:22:49.108753Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.103053Z",
          "shell.execute_reply": "2024-12-04T11:22:49.108029Z"
        },
        "outputId": "2bfe6fdb-dc87-4734-ea13-210711a94bbb"
      },
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.109641Z",
          "iopub.execute_input": "2024-12-04T11:22:49.109945Z",
          "iopub.status.idle": "2024-12-04T11:22:49.132008Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.109916Z",
          "shell.execute_reply": "2024-12-04T11:22:49.131297Z"
        },
        "outputId": "e71c9d26-fba1-4640-d696-5655f13801a5"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  Below is an instruction that describes a task,...   \n1  Below is an instruction that describes a task,...   \n2  Below is an instruction that describes a task,...   \n3  Below is an instruction that describes a task,...   \n4  Below is an instruction that describes a task,...   \n\n                                           input_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                      attention_mask  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.132840Z",
          "iopub.execute_input": "2024-12-04T11:22:49.133094Z",
          "iopub.status.idle": "2024-12-04T11:22:49.137994Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.133069Z",
          "shell.execute_reply": "2024-12-04T11:22:49.137158Z"
        },
        "outputId": "5ad25ab2-f081-4430-c28c-a6fd7ec9b2ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the given poem and identify its main theme.\n\n### Input:\nTwo roads diverged in a yellow wood,\\nAnd sorry I could not travel both\\nAnd be one traveler, long I stood\\nAnd looked down one as far as I could\\nTo where it bent in the undergrowth;\\n\\nThen took the other, as just as fair,\\nAnd having perhaps the better claim,\\nBecause it was grassy and wanted wear;\\nThough as for that the passing there\\nHad worn them really about the same,\\nThe roads that morning equally lay\\nIn leaves no step had trodden black.\\nOh, I left the first for another day!\\nYet knowing how way leads on to way,\\nI doubted if I should ever come back.\\n\\nI shall be telling this with a sigh\\nSomewhere ages and ages hence:\\nTwo roads diverged in a wood, and I—\\nI took the one less traveled by,\\nAnd that has made all the difference.\n\n### Response:\nThe main theme of the poem is the importance of making choices and the impact of those choices on one's life. The speaker is faced with a decision between two paths and ultimately chooses the one less traveled, which ultimately shapes their life experience.</s>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.138876Z",
          "iopub.execute_input": "2024-12-04T11:22:49.139104Z",
          "iopub.status.idle": "2024-12-04T11:22:49.151882Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.139080Z",
          "shell.execute_reply": "2024-12-04T11:22:49.151148Z"
        },
        "outputId": "2bc30726-de77-411c-9b1c-4aa420932e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 20811, 349, 396, 13126, 369, 13966, 264, 3638, 28725, 5881, 1360, 395, 396, 2787, 369, 5312, 3629, 2758, 28723, 12018, 264, 2899, 369, 6582, 1999, 2691, 274, 272, 2159, 28723, 13, 13, 27332, 3133, 3112, 28747, 13, 27554, 1374, 272, 2078, 16067, 304, 9051, 871, 2191, 7335, 28723, 13, 13, 27332, 11232, 28747, 13, 13849, 15014, 19002, 2560, 297, 264, 9684, 4768, 2142, 28711, 2467, 7371, 315, 829, 459, 4530, 1560, 28756, 28711, 2467, 347, 624, 4530, 263, 28725, 1043, 315, 4857, 28756, 28711, 2467, 2382, 1060, 624, 390, 2082, 390, 315, 829, 28756, 28711, 1551, 970, 378, 16127, 297, 272, 916, 20621, 362, 8511, 28711, 28756, 28711, 11341, 2056, 272, 799, 28725, 390, 776, 390, 4968, 2142, 28711, 2467, 2461, 5230, 272, 1873, 3452, 2142, 28711, 17098, 378, 403, 10109, 28724, 304, 2613, 7656, 8511, 28711, 1227, 900, 390, 354, 369, 272, 9720, 736, 28756, 28711, 28769, 316, 15903, 706, 1528, 684, 272, 1348, 2142, 28711, 1014, 15014, 369, 3970, 13387, 4897, 28756, 28711, 657, 8049, 708, 3707, 553, 4056, 1036, 269, 2687, 5923, 28711, 6155, 28725, 315, 1749, 272, 907, 354, 1698, 1370, 8263, 28711, 28802, 299, 8215, 910, 1069, 8681, 356, 298, 1069, 2142, 28711, 28737, 6217, 286, 513, 315, 1023, 2270, 1567, 852, 5923, 28711, 28756, 28711, 28737, 4579, 347, 7124, 456, 395, 264, 19553, 28756, 28711, 11600, 2956, 14506, 304, 14506, 12211, 9941, 28711, 13849, 15014, 19002, 2560, 297, 264, 4768, 28725, 304, 315, 28821, 28756, 28711, 28737, 2056, 272, 624, 2108, 19948, 486, 2142, 28711, 2467, 369, 659, 1269, 544, 272, 5133, 28723, 13, 13, 27332, 12107, 28747, 13, 1014, 2191, 7335, 302, 272, 16067, 349, 272, 9545, 302, 2492, 10475, 304, 272, 5088, 302, 1395, 10475, 356, 624, 28742, 28713, 1411, 28723, 415, 17153, 349, 12565, 395, 264, 5161, 1444, 989, 12924, 304, 12665, 2183, 14779, 272, 624, 2108, 19948, 28725, 690, 12665, 17187, 652, 1411, 2659, 28723, 2]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.157858Z",
          "iopub.execute_input": "2024-12-04T11:22:49.158111Z",
          "iopub.status.idle": "2024-12-04T11:22:49.163206Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.158086Z",
          "shell.execute_reply": "2024-12-04T11:22:49.162337Z"
        },
        "outputId": "074da6e7-2022-4306-bfeb-3d403a080a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.164135Z",
          "iopub.execute_input": "2024-12-04T11:22:49.164695Z",
          "iopub.status.idle": "2024-12-04T11:22:49.172268Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.164670Z",
          "shell.execute_reply": "2024-12-04T11:22:49.171585Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.173346Z",
          "iopub.execute_input": "2024-12-04T11:22:49.173695Z",
          "iopub.status.idle": "2024-12-04T11:22:49.184845Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.173660Z",
          "shell.execute_reply": "2024-12-04T11:22:49.184086Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.185733Z",
          "iopub.execute_input": "2024-12-04T11:22:49.186000Z",
          "iopub.status.idle": "2024-12-04T11:22:49.194192Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.185976Z",
          "shell.execute_reply": "2024-12-04T11:22:49.193482Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 32\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.195078Z",
          "iopub.execute_input": "2024-12-04T11:22:49.195326Z",
          "iopub.status.idle": "2024-12-04T11:22:49.206139Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.195302Z",
          "shell.execute_reply": "2024-12-04T11:22:49.205297Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:49.207216Z",
          "iopub.execute_input": "2024-12-04T11:22:49.207498Z",
          "iopub.status.idle": "2024-12-04T11:22:50.405750Z",
          "shell.execute_reply.started": "2024-12-04T11:22:49.207474Z",
          "shell.execute_reply": "2024-12-04T11:22:50.404826Z"
        },
        "outputId": "d0b823fe-1af4-462a-c4e8-fb6a213eae7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ikF6Yfkz1myd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:50.406960Z",
          "iopub.execute_input": "2024-12-04T11:22:50.407265Z",
          "iopub.status.idle": "2024-12-04T11:22:50.426795Z",
          "shell.execute_reply.started": "2024-12-04T11:22:50.407235Z",
          "shell.execute_reply": "2024-12-04T11:22:50.425829Z"
        },
        "outputId": "94c3caa3-b82a-4767-b195-ed9268180e88"
      },
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:50.427857Z",
          "iopub.execute_input": "2024-12-04T11:22:50.428126Z",
          "iopub.status.idle": "2024-12-04T11:22:50.450673Z",
          "shell.execute_reply.started": "2024-12-04T11:22:50.428099Z",
          "shell.execute_reply": "2024-12-04T11:22:50.449849Z"
        },
        "outputId": "3d2452b3-a0e8-4858-daa5-adaea2bb3e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3835957248\nTrainable parameters : 83886080\nTrainable percentage: 2.19%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:50.451773Z",
          "iopub.execute_input": "2024-12-04T11:22:50.452108Z",
          "iopub.status.idle": "2024-12-04T11:22:50.460339Z",
          "shell.execute_reply.started": "2024-12-04T11:22:50.452072Z",
          "shell.execute_reply": "2024-12-04T11:22:50.459639Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 2\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:50.461351Z",
          "iopub.execute_input": "2024-12-04T11:22:50.461739Z",
          "iopub.status.idle": "2024-12-04T11:22:50.508993Z",
          "shell.execute_reply.started": "2024-12-04T11:22:50.461704Z",
          "shell.execute_reply": "2024-12-04T11:22:50.508109Z"
        },
        "outputId": "0ff67b66-2301-4d98-8745-60d5638d3d3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec04_11-22-50_4818e49eef10,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:50.510188Z",
          "iopub.execute_input": "2024-12-04T11:22:50.510574Z",
          "iopub.status.idle": "2024-12-04T11:22:52.444909Z",
          "shell.execute_reply.started": "2024-12-04T11:22:50.510534Z",
          "shell.execute_reply": "2024-12-04T11:22:52.444215Z"
        },
        "outputId": "aeb4b347-ad5e-4f19-8e61-0205919409cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7db190543b50>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T11:22:52.446066Z",
          "iopub.execute_input": "2024-12-04T11:22:52.446444Z",
          "iopub.status.idle": "2024-12-04T12:54:14.907205Z",
          "shell.execute_reply.started": "2024-12-04T11:22:52.446405Z",
          "shell.execute_reply": "2024-12-04T12:54:14.906446Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:54:14.908829Z",
          "iopub.execute_input": "2024-12-04T12:54:14.909249Z",
          "iopub.status.idle": "2024-12-04T12:57:27.335893Z",
          "shell.execute_reply.started": "2024-12-04T12:54:14.909210Z",
          "shell.execute_reply": "2024-12-04T12:57:27.335128Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:27.336948Z",
          "iopub.execute_input": "2024-12-04T12:57:27.337188Z",
          "iopub.status.idle": "2024-12-04T12:57:29.627540Z",
          "shell.execute_reply.started": "2024-12-04T12:57:27.337165Z",
          "shell.execute_reply": "2024-12-04T12:57:29.626827Z"
        },
        "outputId": "4c44a2c1-45b0-4168-c8a0-9a5fbc3bdf94"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--mistral-7b-instruct-v0.2/snapshots/5d88c8e8dbf7446e665a3633b2f047689f4e6547/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"unsloth/Mistral-7B-Instruct-v0.2\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--mistral-7b-instruct-v0.2/snapshots/5d88c8e8dbf7446e665a3633b2f047689f4e6547/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"unsloth/Mistral-7B-Instruct-v0.2\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:29.628615Z",
          "iopub.execute_input": "2024-12-04T12:57:29.628883Z",
          "iopub.status.idle": "2024-12-04T12:57:29.914624Z",
          "shell.execute_reply.started": "2024-12-04T12:57:29.628857Z",
          "shell.execute_reply": "2024-12-04T12:57:29.913611Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:29.915811Z",
          "iopub.execute_input": "2024-12-04T12:57:29.916096Z",
          "iopub.status.idle": "2024-12-04T12:57:29.928304Z",
          "shell.execute_reply.started": "2024-12-04T12:57:29.916068Z",
          "shell.execute_reply": "2024-12-04T12:57:29.927409Z"
        },
        "id": "BnH6da0fiN04",
        "outputId": "b4e287cf-619f-4b62-d7fd-a8092941d3f8"
      },
      "outputs": [
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:29.929185Z",
          "iopub.execute_input": "2024-12-04T12:57:29.929477Z",
          "iopub.status.idle": "2024-12-04T12:57:31.340199Z",
          "shell.execute_reply.started": "2024-12-04T12:57:29.929451Z",
          "shell.execute_reply": "2024-12-04T12:57:31.339108Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "oEa3eXrniN05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:31.341141Z",
          "iopub.execute_input": "2024-12-04T12:57:31.341442Z",
          "iopub.status.idle": "2024-12-04T12:57:49.381213Z",
          "shell.execute_reply.started": "2024-12-04T12:57:31.341414Z",
          "shell.execute_reply": "2024-12-04T12:57:49.380395Z"
        },
        "id": "xq9tjTSziN05"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:49.382480Z",
          "iopub.execute_input": "2024-12-04T12:57:49.382867Z",
          "iopub.status.idle": "2024-12-04T12:57:49.393170Z",
          "shell.execute_reply.started": "2024-12-04T12:57:49.382825Z",
          "shell.execute_reply": "2024-12-04T12:57:49.392296Z"
        },
        "id": "Z6KV7FnfiN05",
        "outputId": "241b180f-90a5-4765-c4ac-d25c2c5a91b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3752071168\nTrainable parameters : 262410240\nTrainable percentage: 6.99%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:49.394208Z",
          "iopub.execute_input": "2024-12-04T12:57:49.394498Z",
          "iopub.status.idle": "2024-12-04T12:57:49.422130Z",
          "shell.execute_reply.started": "2024-12-04T12:57:49.394468Z",
          "shell.execute_reply": "2024-12-04T12:57:49.421338Z"
        },
        "id": "twlkaOr_iN05",
        "outputId": "491130aa-06c8-4991-b9e1-8be27f9a2de9"
      },
      "outputs": [
        {
          "execution_count": 47,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:49.423101Z",
          "iopub.execute_input": "2024-12-04T12:57:49.423358Z",
          "iopub.status.idle": "2024-12-04T12:57:49.446731Z",
          "shell.execute_reply.started": "2024-12-04T12:57:49.423333Z",
          "shell.execute_reply": "2024-12-04T12:57:49.445988Z"
        },
        "id": "n4lt1miyiN05",
        "outputId": "125d515f-3baa-47b0-ae9b-1fdeeab56442"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3919843328\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt, inputs):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      inputs,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:49.447687Z",
          "iopub.execute_input": "2024-12-04T12:57:49.447932Z",
          "iopub.status.idle": "2024-12-04T12:57:49.454351Z",
          "shell.execute_reply.started": "2024-12-04T12:57:49.447902Z",
          "shell.execute_reply": "2024-12-04T12:57:49.453695Z"
        },
        "id": "XOHtejR4iN05"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt, inputs):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      inputs,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:49.455594Z",
          "iopub.execute_input": "2024-12-04T12:57:49.455935Z",
          "iopub.status.idle": "2024-12-04T12:57:49.464828Z",
          "shell.execute_reply.started": "2024-12-04T12:57:49.455896Z",
          "shell.execute_reply": "2024-12-04T12:57:49.463927Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:57:49.466120Z",
          "iopub.execute_input": "2024-12-04T12:57:49.467028Z",
          "iopub.status.idle": "2024-12-04T12:57:49.474388Z",
          "shell.execute_reply.started": "2024-12-04T12:57:49.466989Z",
          "shell.execute_reply": "2024-12-04T12:57:49.473510Z"
        },
        "id": "XuDkF6fJiN05"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:09:25.230009Z",
          "iopub.execute_input": "2024-12-04T13:09:25.230628Z",
          "iopub.status.idle": "2024-12-04T13:10:46.886507Z",
          "shell.execute_reply.started": "2024-12-04T13:09:25.230590Z",
          "shell.execute_reply": "2024-12-04T13:10:46.885606Z"
        },
        "outputId": "c1c89ec2-05b5-4b1d-aa3c-c6677541fd40"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Three      |  completes the request.  ### Instruction: Three    \npositive integers have a sum of 72 and are in the   |  positive integers have a sum of 72 and are in the \nratio 1:3:4. What is the least of these three       |  ratio 1:3:4. What is the least of these three     \nintegers?  ### Input:   ### Response: To find the   |  integers?  ### Input:   ### Response:  To find the\nleast of the three integers, we first need to       |  least integer, we first need to find the          \ndetermine their values based on the given ratio of  |  individual integers that satisfy the given        \n1:3:4. Let x be the smallest integer, then the      |  conditions. Let x be the smallest integer, y be   \nother two integers are 3x and 4x. We know that x +  |  the integer that is three times smaller than x,   \n3x + 4x = 72, so 8x = 72. Dividing both sides by    |  and z be the integer that is four times smaller   \n8, we get x = 9. Therefore, the least integer is    |  than x. We know that x + y + z = 72.  Substituting\n9, and the other two integers are 3x = 27 and 4x =  |  the given ratios into the equation, we get: x +   \n36.                                                 |  (x/3) + (x/4) = 72  Multiplying both sides by 12  \n                                                    |  (the least common multiple of 3 and 4) to         \n                                                    |  eliminate the fractions, we get: 12x + 4x + 3x =  \n                                                    |  864  Combining like terms, we get: 19x = 864      \n                                                    |  Dividing both sides by 19, we find that x = 45.53 \n                                                    |  (approximately). However, since x is a positive   \n                                                    |  integer, this solution is not valid.  Let's try   \n                                                    |  another approach by assuming that x is an integer \n                                                    |  multiple of 3 and 4. Since the smallest possible  \n                                                    |  integer that can be expressed as a multiple of    \n                                                    |  both 3 and 4 is 12, let's assume x = 12. Then, y =\n                                                    |  x/3 = 4, and z = x/4 = 3.  Checking that the sum  \n                                                    |  of these integers is 72, we have: x + y + z = 12 +\n                                                    |  4 + 3 = 19  Since 19 is not equal to 72, this     \n                                                    |  assumption was incorrect. Let's try another       \n                                                    |  multiple of 3 and 4, such as x = 18. Then, y = x/3\n                                                    |  = 6, and z = x/4 = 4.5. Again, this solution is   \n                                                    |  not valid since z is not an integer.  Let's try x \n                                                    |  = 24 instead. Then, y = x/3 = 8, and z = x/4 = 6. \n                                                    |  The sum of these integers is: x + y + z = 24 + 8 +\n                                                    |  6 = 38  Since 38 is not equal to 72, this         \n                                                    |  assumption was also incorrect.  Let's try one more\n                                                    |  assumption, x = 36. Then, y = x/3 = 12, and z =   \n                                                    |  x/4 = 9. The sum of these integers is: x + y + z =\n                                                    |  36 + 12 + 9 = 57  Finally, we have found a        \n                                                    |  solution where the sum of the integers is equal to\n                                                    |  72. Therefore, the least integer is x = 36.       \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:16:35.516814Z",
          "iopub.execute_input": "2024-12-04T13:16:35.517482Z",
          "iopub.status.idle": "2024-12-04T13:17:08.780034Z",
          "shell.execute_reply.started": "2024-12-04T13:16:35.517451Z",
          "shell.execute_reply": "2024-12-04T13:17:08.779196Z"
        },
        "outputId": "05cc9e3f-9bdc-4d7c-9127-cba1c5db8df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Describe   |  completes the request.  ### Instruction: Describe \nthe personality traits of an introvert  ### Input:  |  the personality traits of an introvert  ### Input:\n### Response: Introverts are typically              |  ### Response: Introverts are typically            \ncharacterized by their preference for quiet,        |  characterized by their preference for quiet,      \nsolitary activities. They tend to be                |  solitary activities. They tend to be              \nintrospective, meaning they spend a lot of time     |  introspective, meaning they spend a lot of time   \nthinking about their own thoughts and feelings.     |  thinking about their own thoughts and feelings.   \nIntroverts often recharge by spending time alone,   |  Introverts often feel drained by social           \nand they may find large social gatherings           |  interactions and may need alone time to recharge. \ndraining. They value depth and authenticity in      |  They value depth and authenticity in their        \ntheir relationships, preferring to have a few       |  relationships and may take a more reserved        \nclose friends rather than a large social circle.    |  approach when meeting new people. Introverts are  \nIntroverts are often highly creative and            |  often creative, imaginative, and deep thinkers,   \nindependent, and they may be deeply absorbed in     |  and they may be more focused on their inner world \ntheir work or hobbies. They can be very perceptive  |  than on external stimuli. They can be very loyal  \nand empathetic, and they often have a rich inner    |  and dedicated to the people and causes they care  \nworld. Introverts may appear reserved or shy in     |  about, and they may be highly sensitive to their  \nsocial situations, but they are often deeply        |  environment and the emotions of others. Overall,  \nthoughtful and insightful individuals.              |  introverts are complex individuals who value their\n                                                    |  inner thoughts and feelings, and they may seek out\n                                                    |  quiet, reflective environments to recharge and    \n                                                    |  thrive.                                           \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T12:58:04.749801Z",
          "iopub.execute_input": "2024-12-04T12:58:04.750078Z",
          "iopub.status.idle": "2024-12-04T12:59:12.609214Z",
          "shell.execute_reply.started": "2024-12-04T12:58:04.750038Z",
          "shell.execute_reply": "2024-12-04T12:59:12.608403Z"
        },
        "outputId": "cb6deb32-fcd3-45eb-c233-05825e19afee"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Construct  |  completes the request.  ### Instruction: Construct\na story of love and loss.  ### Input:   ###         |  a story of love and loss.  ### Input:   ###       \nResponse:  Once upon a time, in the quaint little   |  Response:  Once upon a time, in the quaint little \nvillage of Verona, lived two young lovers, Romeo    |  village of Verona, lived two young lovers, Romeo  \nand Juliet. Their love was a beautiful,             |  and Juliet. Their love was as passionate as it was\npassionate, and forbidden tale. Romeo was from the  |  forbidden, for their families, the Montagues and  \nMontague family, and Juliet was from the Capulet    |  Capulets, were locked in a bitter feud. Despite   \nfamily. The Montagues and Capulets were bitter      |  the danger, their hearts beat only for each other.\nenemies, and their feud had been raging for         |  They met in secret, stealing moments of joy and   \ngenerations.  Despite the animosity between their   |  happiness amidst the chaos of their families'     \nfamilies, Romeo and Juliet's hearts beat as one.    |  hatred. Their love was a beacon of hope in the    \nThey met secretly, away from the prying eyes of     |  dark world they lived in. But fate, cruel and     \ntheir families and the watchful eyes of their       |  unforgiving, had other plans.  One fateful night, \nenemies. Their love was a beacon of hope in the     |  a brawl broke out between the Montagues and       \nmidst of the chaos and hatred that surrounded       |  Capulets at a masquerade ball. In the chaos,      \nthem.  But fate, as cruel as it can be, had other   |  Romeo, believing Juliet to be dead, took his own  \nplans. A series of unfortunate events unfolded,     |  life. Juliet, heartbroken and devastated, followed\nand the two lovers found themselves torn apart.     |  suit. Their tragic end brought an end to the feud,\nRomeo, in a fit of anger and despair, took his own  |  but at what cost?  The village of Verona was left \nlife, believing that Juliet was dead. Juliet,       |  in mourning, the once passionate love now replaced\nheartbroken and devastated, followed suit, unable   |  by a deep sense of loss. The Montagues and        \nto bear the thought of living without her beloved   |  Capulets, who had spent generations hating each   \nRomeo.  The news of their deaths reached their      |  other, came together to bury their children. The  \nfamilies, and the feud that had been raging for so  |  love story of Romeo and Juliet served as a        \nlong came to an end. The Montagues and Capulets,    |  reminder of the power of love and the devastating \nnow mourning their own children, realized the       |  consequences of hate.  And so, the tale of love   \nsenselessness of their long-standing feud. They     |  and loss in Verona lived on, passed down through  \ncame together, united in their grief, and vowed to  |  generations, a testament to the enduring power of \nlive in peace and harmony.  But the love story of   |  love and the heartbreaking reality of loss.       \nRomeo and Juliet was not just one of loss. It was   |                                                    \nalso a story of hope, of a love so strong that it   |                                                    \ncould transcend even the deepest of hatreds. Their  |                                                    \nlove lived on, not just in the memories of their    |                                                    \nfamilies, but in the hearts of the people of        |                                                    \nVerona, who would tell their story for generations  |                                                    \nto come.                                            |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:24:38.102028Z",
          "iopub.execute_input": "2024-12-04T13:24:38.102840Z",
          "iopub.status.idle": "2024-12-04T13:25:46.624169Z",
          "shell.execute_reply.started": "2024-12-04T13:24:38.102804Z",
          "shell.execute_reply": "2024-12-04T13:25:46.623236Z"
        },
        "outputId": "15dd359f-1093-4997-a54a-84dce4c852fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Factor     |  completes the request.  ### Instruction: Factor   \nthe following expression: 145b^2 +29b.  ### Input:  |  the following expression: 145b^2 +29b.  ### Input:\n### Response: To factor the given expression        |  ### Response:  To factor the given expression     \n145b^2 + 29b, we need to find two binomials that,   |  145b^2 + 29b, we need to find two binomials that, \nwhen multiplied, will give us the original          |  when multiplied, will give us the original        \nexpression. Let's first factor out the common term  |  expression.  First, let's get rid of the b term by\nb from each term:  145b^2 = 145b * b 29b = 1 * 29b  |  completing the square for the quadratic term.     \nNow, we can factor out the common term 145b from    |  145b^2 + 29b = 145b^2 + 14b^2 - 14b^2 + 29b       \nthe first term:  145b * b = 145b(b)  Since 145 and  |  = 14(b^2 + 7b^2/14 - b/14) + 29b/14  Now, we can  \n29 are prime numbers, and there is no common        |  factor the quadratic term inside the parentheses: \nfactor between them other than 1, we cannot factor  |  b^2 + 7b^2/14 - b/14 = (b^2 + 7b^2/14 + 7b/14) -  \nthem further. Therefore, the expression 145b^2 +    |  (7b/14)                  = (b + 7b/14)² - (7b/14)²\n29b cannot be factored into two binomials. It is    |  So, we have:  145b^2 + 29b = 14(b + 7b/14)² -     \ncalled a trinomial square, and it is not possible   |  14(7b/14)²  Now, we can factor out the common term\nto express it as a perfect square trinomial.        |  14:  145b^2 + 29b = 14[ (b + 7b/14)² - (7b/14)² ] \nHowever, if you meant to ask for the quadratic      |  Therefore, the factored form of the expression    \nformula of the given expression, it is:  x = [( -b  |  145b^2 + 29b is 14[ (b + 7b/14)² - (7b/14)² ].    \n+/- sqrt(b^2 - 4*145*29) ) / ( 2 * 145 )]  This     |                                                    \nformula will give you the solutions of the          |                                                    \nquadratic equation represented by the given         |                                                    \nexpression.                                         |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-04T13:29:14.536615Z",
          "iopub.execute_input": "2024-12-04T13:29:14.537328Z",
          "iopub.status.idle": "2024-12-04T13:29:31.127556Z",
          "shell.execute_reply.started": "2024-12-04T13:29:14.537297Z",
          "shell.execute_reply": "2024-12-04T13:29:31.126656Z"
        },
        "outputId": "ff363bc5-959f-432d-fa38-cec21c9ec293"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: What is    |  completes the request.  ### Instruction: What is  \nan example of a physical observation?  ### Input:   |  an example of a physical observation?  ### Input: \n### Response: An example of a physical observation  |  ### Response: An example of a physical observation\nis observing the color change of a lit match as it  |  is observing the color change of a lit match as it\nburns. This observation involves noticing the       |  burns. This observation involves noticing the     \ntransformation from the initial matchstick's color  |  transformation from a bright matchstick to a      \nto its charred state after being ignited. Another   |  glowing tip and eventually to an ash. The color   \nexample could be observing the melting of an ice    |  change is a result of a chemical reaction         \ncube in a glass of water, noticing how the ice      |  occurring within the match, which can be seen with\ngradually loses its solid form and turns into       |  the naked eye.                                    \nwater. These observations are based on the senses,  |                                                    \nspecifically sight, and provide valuable            |                                                    \ninformation about the physical world.               |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}