{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"#!pip install --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-24T01:11:02.113156Z","iopub.execute_input":"2024-11-24T01:11:02.114134Z","iopub.status.idle":"2024-11-24T01:11:36.386420Z","shell.execute_reply.started":"2024-11-24T01:11:02.114095Z","shell.execute_reply":"2024-11-24T01:11:36.385132Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-11-24T01:11:36.388348Z","iopub.execute_input":"2024-11-24T01:11:36.388637Z","iopub.status.idle":"2024-11-24T01:11:43.948615Z","shell.execute_reply.started":"2024-11-24T01:11:36.388610Z","shell.execute_reply":"2024-11-24T01:11:43.947717Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-11-24T01:11:43.949963Z","iopub.execute_input":"2024-11-24T01:11:43.950273Z","iopub.status.idle":"2024-11-24T01:11:43.956920Z","shell.execute_reply.started":"2024-11-24T01:11:43.950245Z","shell.execute_reply":"2024-11-24T01:11:43.956048Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n#model_name = url.split('.co/')[-1]\n\nmodel_name = 'HuggingFaceTB/SmolLM2-360M-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-24T01:11:43.958940Z","iopub.execute_input":"2024-11-24T01:11:43.959275Z","iopub.status.idle":"2024-11-24T01:11:43.971732Z","shell.execute_reply.started":"2024-11-24T01:11:43.959227Z","shell.execute_reply":"2024-11-24T01:11:43.970919Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-11-24T01:11:43.972742Z","iopub.execute_input":"2024-11-24T01:11:43.973021Z","iopub.status.idle":"2024-11-24T01:11:43.984103Z","shell.execute_reply.started":"2024-11-24T01:11:43.972994Z","shell.execute_reply":"2024-11-24T01:11:43.983306Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:11:43.985053Z","iopub.execute_input":"2024-11-24T01:11:43.985311Z","iopub.status.idle":"2024-11-24T01:12:04.232399Z","shell.execute_reply.started":"2024-11-24T01:11:43.985279Z","shell.execute_reply":"2024-11-24T01:12:04.231479Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc297a466fb404e97234bcc1a2fa63b"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed7e81fc5114394bdfd65fc5b2e6650"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0b71e617084a15bd70f05500684791"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=960, out_features=960, bias=False)\n          (k_proj): Linear4bit(in_features=960, out_features=320, bias=False)\n          (v_proj): Linear4bit(in_features=960, out_features=320, bias=False)\n          (o_proj): Linear4bit(in_features=960, out_features=960, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=960, out_features=2560, bias=False)\n          (up_proj): Linear4bit(in_features=960, out_features=2560, bias=False)\n          (down_proj): Linear4bit(in_features=2560, out_features=960, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((960,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:04.233446Z","iopub.execute_input":"2024-11-24T01:12:04.233724Z","iopub.status.idle":"2024-11-24T01:12:04.241431Z","shell.execute_reply.started":"2024-11-24T01:12:04.233697Z","shell.execute_reply":"2024-11-24T01:12:04.240532Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 204534720\nTrainable parameters : 47248320\nTrainable percentage: 23.10%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:04.242633Z","iopub.execute_input":"2024-11-24T01:12:04.242885Z","iopub.status.idle":"2024-11-24T01:12:05.226393Z","shell.execute_reply.started":"2024-11-24T01:12:04.242861Z","shell.execute_reply":"2024-11-24T01:12:05.225407Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4941e874301491397e3e2410f07f196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1a568896fb4f008adea5741f626798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0015a524994bce9b48cf4bad47ef04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9aafb246f3843d0938eb0cae1cfba67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c63db75994f499aadd9173d01c4aa13"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n#dataset_name = url.split('datasets/')[-1]\n\ndataset_name = 'yahma/alpaca-cleaned'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:05.227548Z","iopub.execute_input":"2024-11-24T01:12:05.227867Z","iopub.status.idle":"2024-11-24T01:12:05.232103Z","shell.execute_reply.started":"2024-11-24T01:12:05.227839Z","shell.execute_reply":"2024-11-24T01:12:05.231099Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 512","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:05.235848Z","iopub.execute_input":"2024-11-24T01:12:05.236102Z","iopub.status.idle":"2024-11-24T01:12:05.242476Z","shell.execute_reply.started":"2024-11-24T01:12:05.236077Z","shell.execute_reply":"2024-11-24T01:12:05.241623Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:05.243390Z","iopub.execute_input":"2024-11-24T01:12:05.243673Z","iopub.status.idle":"2024-11-24T01:12:06.304044Z","shell.execute_reply.started":"2024-11-24T01:12:05.243648Z","shell.execute_reply":"2024-11-24T01:12:06.303213Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['output', 'input', 'instruction'],\n    num_rows: 51760\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.305056Z","iopub.execute_input":"2024-11-24T01:12:06.305330Z","iopub.status.idle":"2024-11-24T01:12:06.311732Z","shell.execute_reply.started":"2024-11-24T01:12:06.305302Z","shell.execute_reply":"2024-11-24T01:12:06.310881Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.313023Z","iopub.execute_input":"2024-11-24T01:12:06.313869Z","iopub.status.idle":"2024-11-24T01:12:06.334338Z","shell.execute_reply.started":"2024-11-24T01:12:06.313825Z","shell.execute_reply":"2024-11-24T01:12:06.333468Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                              output input  \\\n0  1. Eat a balanced and nutritious diet: Make su...         \n1  The three primary colors are red, blue, and ye...         \n2  An atom is the basic building block of all mat...         \n3  There are several ways to reduce air pollution...         \n4  I had to make a difficult decision when I was ...         \n\n                                         instruction  \n0               Give three tips for staying healthy.  \n1                 What are the three primary colors?  \n2                 Describe the structure of an atom.  \n3                   How can we reduce air pollution?  \n4  Pretend you are a project manager of a constru...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>output</th>\n      <th>input</th>\n      <th>instruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n      <td></td>\n      <td>Give three tips for staying healthy.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The three primary colors are red, blue, and ye...</td>\n      <td></td>\n      <td>What are the three primary colors?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>An atom is the basic building block of all mat...</td>\n      <td></td>\n      <td>Describe the structure of an atom.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>There are several ways to reduce air pollution...</td>\n      <td></td>\n      <td>How can we reduce air pollution?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I had to make a difficult decision when I was ...</td>\n      <td></td>\n      <td>Pretend you are a project manager of a constru...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.335372Z","iopub.execute_input":"2024-11-24T01:12:06.335700Z","iopub.status.idle":"2024-11-24T01:12:06.341551Z","shell.execute_reply.started":"2024-11-24T01:12:06.335671Z","shell.execute_reply":"2024-11-24T01:12:06.340704Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n 'input': '',\n 'instruction': 'Give three tips for staying healthy.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.342770Z","iopub.execute_input":"2024-11-24T01:12:06.343027Z","iopub.status.idle":"2024-11-24T01:12:06.352024Z","shell.execute_reply.started":"2024-11-24T01:12:06.342996Z","shell.execute_reply":"2024-11-24T01:12:06.351140Z"}},"outputs":[{"name":"stdout","text":"['output', 'input', 'instruction']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.353224Z","iopub.execute_input":"2024-11-24T01:12:06.353640Z","iopub.status.idle":"2024-11-24T01:12:06.360674Z","shell.execute_reply.started":"2024-11-24T01:12:06.353599Z","shell.execute_reply":"2024-11-24T01:12:06.359995Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  instruction = examples['instruction']\n  input = examples['input']\n  output = examples['output']\n  \n  text = prompt_format.format(instruction, input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.361735Z","iopub.execute_input":"2024-11-24T01:12:06.362016Z","iopub.status.idle":"2024-11-24T01:12:06.372793Z","shell.execute_reply.started":"2024-11-24T01:12:06.361989Z","shell.execute_reply":"2024-11-24T01:12:06.371656Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.373783Z","iopub.execute_input":"2024-11-24T01:12:06.374080Z","iopub.status.idle":"2024-11-24T01:12:06.386285Z","shell.execute_reply.started":"2024-11-24T01:12:06.374051Z","shell.execute_reply":"2024-11-24T01:12:06.385398Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.387295Z","iopub.execute_input":"2024-11-24T01:12:06.387554Z","iopub.status.idle":"2024-11-24T01:12:06.393384Z","shell.execute_reply.started":"2024-11-24T01:12:06.387521Z","shell.execute_reply":"2024-11-24T01:12:06.392643Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.394325Z","iopub.execute_input":"2024-11-24T01:12:06.394607Z","iopub.status.idle":"2024-11-24T01:12:06.402341Z","shell.execute_reply.started":"2024-11-24T01:12:06.394550Z","shell.execute_reply":"2024-11-24T01:12:06.401597Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:06.403315Z","iopub.execute_input":"2024-11-24T01:12:06.403603Z","iopub.status.idle":"2024-11-24T01:12:15.134594Z","shell.execute_reply.started":"2024-11-24T01:12:06.403550Z","shell.execute_reply":"2024-11-24T01:12:15.133600Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0780a03f7c28480a89507107baeb50de"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.135776Z","iopub.execute_input":"2024-11-24T01:12:15.136065Z","iopub.status.idle":"2024-11-24T01:12:15.141768Z","shell.execute_reply.started":"2024-11-24T01:12:15.136037Z","shell.execute_reply":"2024-11-24T01:12:15.140624Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.142960Z","iopub.execute_input":"2024-11-24T01:12:15.143276Z","iopub.status.idle":"2024-11-24T01:12:15.223470Z","shell.execute_reply.started":"2024-11-24T01:12:15.143245Z","shell.execute_reply":"2024-11-24T01:12:15.222560Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.224789Z","iopub.execute_input":"2024-11-24T01:12:15.225088Z","iopub.status.idle":"2024-11-24T01:12:15.232883Z","shell.execute_reply.started":"2024-11-24T01:12:15.225059Z","shell.execute_reply":"2024-11-24T01:12:15.231782Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.234276Z","iopub.execute_input":"2024-11-24T01:12:15.234951Z","iopub.status.idle":"2024-11-24T01:12:15.264656Z","shell.execute_reply.started":"2024-11-24T01:12:15.234905Z","shell.execute_reply":"2024-11-24T01:12:15.263633Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Below is an instruction that describes a task,...   \n1  Below is an instruction that describes a task,...   \n2  Below is an instruction that describes a task,...   \n3  Below is an instruction that describes a task,...   \n4  Below is an instruction that describes a task,...   \n\n                                           input_ids  \\\n0  [19798, 314, 354, 5785, 338, 6601, 253, 3856, ...   \n1  [19798, 314, 354, 5785, 338, 6601, 253, 3856, ...   \n2  [19798, 314, 354, 5785, 338, 6601, 253, 3856, ...   \n3  [19798, 314, 354, 5785, 338, 6601, 253, 3856, ...   \n4  [19798, 314, 354, 5785, 338, 6601, 253, 3856, ...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[19798, 314, 354, 5785, 338, 6601, 253, 3856, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[19798, 314, 354, 5785, 338, 6601, 253, 3856, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[19798, 314, 354, 5785, 338, 6601, 253, 3856, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[19798, 314, 354, 5785, 338, 6601, 253, 3856, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[19798, 314, 354, 5785, 338, 6601, 253, 3856, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.265706Z","iopub.execute_input":"2024-11-24T01:12:15.265982Z","iopub.status.idle":"2024-11-24T01:12:15.272282Z","shell.execute_reply.started":"2024-11-24T01:12:15.265955Z","shell.execute_reply":"2024-11-24T01:12:15.271275Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the given poem and identify its main theme.\n\n### Input:\nTwo roads diverged in a yellow wood,\\nAnd sorry I could not travel both\\nAnd be one traveler, long I stood\\nAnd looked down one as far as I could\\nTo where it bent in the undergrowth;\\n\\nThen took the other, as just as fair,\\nAnd having perhaps the better claim,\\nBecause it was grassy and wanted wear;\\nThough as for that the passing there\\nHad worn them really about the same,\\nThe roads that morning equally lay\\nIn leaves no step had trodden black.\\nOh, I left the first for another day!\\nYet knowing how way leads on to way,\\nI doubted if I should ever come back.\\n\\nI shall be telling this with a sigh\\nSomewhere ages and ages hence:\\nTwo roads diverged in a wood, and I—\\nI took the one less traveled by,\\nAnd that has made all the difference.\n\n### Response:\nThe main theme of the poem is the importance of making choices and the impact of those choices on one's life. The speaker is faced with a decision between two paths and ultimately chooses the one less traveled, which ultimately shapes their life experience.<|im_end|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.274198Z","iopub.execute_input":"2024-11-24T01:12:15.274650Z","iopub.status.idle":"2024-11-24T01:12:15.282665Z","shell.execute_reply.started":"2024-11-24T01:12:15.274532Z","shell.execute_reply":"2024-11-24T01:12:15.281635Z"}},"outputs":[{"name":"stdout","text":"[19798, 314, 354, 5785, 338, 6601, 253, 3856, 28, 20054, 351, 354, 3007, 338, 2433, 2030, 2468, 30, 9517, 253, 2426, 338, 13674, 32873, 260, 3116, 30, 198, 198, 3757, 20880, 42, 198, 24499, 2404, 260, 1836, 8216, 284, 2669, 624, 1085, 7374, 30, 198, 198, 3757, 18100, 42, 198, 10345, 8364, 11172, 2984, 281, 253, 5724, 3180, 29323, 94, 3528, 22657, 339, 856, 441, 2827, 1062, 76, 94, 3528, 325, 582, 32779, 28, 986, 339, 9318, 76, 94, 3528, 5328, 1187, 582, 347, 1869, 347, 339, 856, 76, 94, 2068, 837, 357, 18178, 281, 260, 656, 19477, 41227, 94, 76, 94, 10039, 2637, 260, 550, 28, 347, 915, 347, 3506, 29323, 94, 3528, 1953, 4012, 260, 1365, 2766, 29323, 94, 8653, 357, 436, 42661, 284, 4146, 5777, 41227, 94, 12908, 347, 327, 338, 260, 7685, 665, 76, 94, 39855, 11742, 601, 2159, 563, 260, 1142, 29323, 94, 504, 8364, 338, 5738, 7582, 2060, 76, 94, 788, 3711, 787, 1833, 761, 5855, 5745, 2632, 23113, 94, 16912, 28, 339, 2049, 260, 808, 327, 1372, 1194, 17, 76, 94, 16075, 6040, 638, 970, 4733, 335, 288, 970, 29323, 94, 57, 4995, 1132, 585, 339, 868, 2042, 1690, 1056, 23113, 94, 76, 94, 57, 3786, 325, 8932, 451, 351, 253, 44452, 76, 94, 4449, 2942, 6399, 284, 6399, 8913, 19199, 94, 10345, 8364, 11172, 2984, 281, 253, 3180, 28, 284, 339, 1265, 76, 94, 57, 2637, 260, 582, 1181, 12581, 411, 29323, 94, 3528, 338, 553, 1135, 511, 260, 3193, 30, 198, 198, 3757, 14212, 42, 198, 504, 1085, 7374, 282, 260, 8216, 314, 260, 2979, 282, 1625, 4975, 284, 260, 1645, 282, 967, 4975, 335, 582, 506, 1029, 30, 378, 10831, 314, 5263, 351, 253, 3062, 826, 827, 9146, 284, 5354, 23046, 260, 582, 1181, 12581, 28, 527, 5354, 6146, 480, 1029, 1786, 30, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.288350Z","iopub.execute_input":"2024-11-24T01:12:15.288748Z","iopub.status.idle":"2024-11-24T01:12:15.296220Z","shell.execute_reply.started":"2024-11-24T01:12:15.288718Z","shell.execute_reply":"2024-11-24T01:12:15.295223Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.297747Z","iopub.execute_input":"2024-11-24T01:12:15.298045Z","iopub.status.idle":"2024-11-24T01:12:15.308000Z","shell.execute_reply.started":"2024-11-24T01:12:15.298017Z","shell.execute_reply":"2024-11-24T01:12:15.307083Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.309333Z","iopub.execute_input":"2024-11-24T01:12:15.309961Z","iopub.status.idle":"2024-11-24T01:12:15.318977Z","shell.execute_reply.started":"2024-11-24T01:12:15.309930Z","shell.execute_reply":"2024-11-24T01:12:15.318153Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.319933Z","iopub.execute_input":"2024-11-24T01:12:15.320238Z","iopub.status.idle":"2024-11-24T01:12:15.329757Z","shell.execute_reply.started":"2024-11-24T01:12:15.320212Z","shell.execute_reply":"2024-11-24T01:12:15.329037Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.330759Z","iopub.execute_input":"2024-11-24T01:12:15.331035Z","iopub.status.idle":"2024-11-24T01:12:15.339313Z","shell.execute_reply.started":"2024-11-24T01:12:15.331003Z","shell.execute_reply":"2024-11-24T01:12:15.338554Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:15.340323Z","iopub.execute_input":"2024-11-24T01:12:15.340658Z","iopub.status.idle":"2024-11-24T01:12:16.035852Z","shell.execute_reply.started":"2024-11-24T01:12:15.340621Z","shell.execute_reply":"2024-11-24T01:12:16.034842Z"}},"outputs":[{"name":"stdout","text":"trainable params: 34,734,080 || all params: 396,555,200 || trainable%: 8.7590\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"model","metadata":{"id":"ikF6Yfkz1myd","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:16.036953Z","iopub.execute_input":"2024-11-24T01:12:16.037249Z","iopub.status.idle":"2024-11-24T01:12:16.053366Z","shell.execute_reply.started":"2024-11-24T01:12:16.037223Z","shell.execute_reply":"2024-11-24T01:12:16.052505Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=320, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=320, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2560, out_features=960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2560, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((960,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:16.054445Z","iopub.execute_input":"2024-11-24T01:12:16.054714Z","iopub.status.idle":"2024-11-24T01:12:16.081125Z","shell.execute_reply.started":"2024-11-24T01:12:16.054688Z","shell.execute_reply":"2024-11-24T01:12:16.080073Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 239268800\nTrainable parameters : 34734080\nTrainable percentage: 14.52%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:16.082323Z","iopub.execute_input":"2024-11-24T01:12:16.082779Z","iopub.status.idle":"2024-11-24T01:12:16.091724Z","shell.execute_reply.started":"2024-11-24T01:12:16.082721Z","shell.execute_reply":"2024-11-24T01:12:16.090957Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 2\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:16.092844Z","iopub.execute_input":"2024-11-24T01:12:16.093201Z","iopub.status.idle":"2024-11-24T01:12:16.138185Z","shell.execute_reply.started":"2024-11-24T01:12:16.093161Z","shell.execute_reply":"2024-11-24T01:12:16.137267Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Nov24_01-12-16_e085d4847dbc,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:16.139343Z","iopub.execute_input":"2024-11-24T01:12:16.139673Z","iopub.status.idle":"2024-11-24T01:12:17.309410Z","shell.execute_reply.started":"2024-11-24T01:12:16.139645Z","shell.execute_reply":"2024-11-24T01:12:17.308644Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x790ea00e4a60>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:12:17.310633Z","iopub.execute_input":"2024-11-24T01:12:17.310987Z","iopub.status.idle":"2024-11-24T01:23:15.317266Z","shell.execute_reply.started":"2024-11-24T01:12:17.310946Z","shell.execute_reply":"2024-11-24T01:23:15.316489Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 2\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 34,734,080\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112818488881102, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"553e1b6ccc49426a80baf650c7ee938f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241124_011218-5rkymi12</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/5rkymi12' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/5rkymi12' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/5rkymi12</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 10:51, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.856500</td>\n      <td>1.796757</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.806800</td>\n      <td>1.757225</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.722300</td>\n      <td>1.703034</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.659900</td>\n      <td>1.637554</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.612500</td>\n      <td>1.569196</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.540300</td>\n      <td>1.500392</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.486000</td>\n      <td>1.445740</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.371500</td>\n      <td>1.412700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.403400</td>\n      <td>1.400776</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.312800</td>\n      <td>1.399217</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.5772096729278564, metrics={'train_runtime': 657.5623, 'train_samples_per_second': 2.433, 'train_steps_per_second': 0.304, 'total_flos': 1887944835072000.0, 'train_loss': 1.5772096729278564, 'epoch': 0.17777777777777778})"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:15.318938Z","iopub.execute_input":"2024-11-24T01:23:15.319300Z","iopub.status.idle":"2024-11-24T01:23:34.710786Z","shell.execute_reply.started":"2024-11-24T01:23:15.319260Z","shell.execute_reply":"2024-11-24T01:23:34.709921Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 1.3992174863815308, 'eval_runtime': 19.3794, 'eval_samples_per_second': 10.32, 'eval_steps_per_second': 2.58, 'epoch': 0.17777777777777778}\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:34.711882Z","iopub.execute_input":"2024-11-24T01:23:34.712162Z","iopub.status.idle":"2024-11-24T01:23:35.878549Z","shell.execute_reply.started":"2024-11-24T01:23:34.712134Z","shell.execute_reply":"2024-11-24T01:23:35.877624Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/5193942854fd5c374c4c8d9532c651b7df79046b/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 960,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2560,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 15,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 5,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/5193942854fd5c374c4c8d9532c651b7df79046b/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 960,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2560,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 15,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 5,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:35.879936Z","iopub.execute_input":"2024-11-24T01:23:35.880300Z","iopub.status.idle":"2024-11-24T01:23:36.059931Z","shell.execute_reply.started":"2024-11-24T01:23:35.880248Z","shell.execute_reply":"2024-11-24T01:23:36.058974Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:36.061211Z","iopub.execute_input":"2024-11-24T01:23:36.061646Z","iopub.status.idle":"2024-11-24T01:23:36.075948Z","shell.execute_reply.started":"2024-11-24T01:23:36.061601Z","shell.execute_reply":"2024-11-24T01:23:36.075053Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:36.076958Z","iopub.execute_input":"2024-11-24T01:23:36.077180Z","iopub.status.idle":"2024-11-24T01:23:36.859961Z","shell.execute_reply.started":"2024-11-24T01:23:36.077157Z","shell.execute_reply":"2024-11-24T01:23:36.859258Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:36.860941Z","iopub.execute_input":"2024-11-24T01:23:36.861169Z","iopub.status.idle":"2024-11-24T01:23:38.667240Z","shell.execute_reply.started":"2024-11-24T01:23:36.861146Z","shell.execute_reply":"2024-11-24T01:23:38.666418Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/5193942854fd5c374c4c8d9532c651b7df79046b/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 960,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2560,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 15,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 5,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/5193942854fd5c374c4c8d9532c651b7df79046b/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-360M-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/5193942854fd5c374c4c8d9532c651b7df79046b/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=960, out_features=960, bias=False)\n          (k_proj): Linear4bit(in_features=960, out_features=320, bias=False)\n          (v_proj): Linear4bit(in_features=960, out_features=320, bias=False)\n          (o_proj): Linear4bit(in_features=960, out_features=960, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=960, out_features=2560, bias=False)\n          (up_proj): Linear4bit(in_features=960, out_features=2560, bias=False)\n          (down_proj): Linear4bit(in_features=2560, out_features=960, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((960,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:38.668420Z","iopub.execute_input":"2024-11-24T01:23:38.668806Z","iopub.status.idle":"2024-11-24T01:23:38.679208Z","shell.execute_reply.started":"2024-11-24T01:23:38.668766Z","shell.execute_reply":"2024-11-24T01:23:38.678347Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 204534720\nTrainable parameters : 47248320\nTrainable percentage: 23.10%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:38.680338Z","iopub.execute_input":"2024-11-24T01:23:38.680787Z","iopub.status.idle":"2024-11-24T01:23:38.710267Z","shell.execute_reply.started":"2024-11-24T01:23:38.680746Z","shell.execute_reply":"2024-11-24T01:23:38.709203Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(49152, 960, padding_idx=2)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=960, bias=False)\n                  (default): Linear(in_features=64, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=320, bias=False)\n                  (default): Linear(in_features=64, out_features=320, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=320, bias=False)\n                  (default): Linear(in_features=64, out_features=320, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=960, bias=False)\n                  (default): Linear(in_features=64, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n                  (default): Linear(in_features=64, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n                  (default): Linear(in_features=64, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2560, out_features=64, bias=False)\n                  (default): Linear(in_features=2560, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=960, bias=False)\n                  (default): Linear(in_features=64, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((960,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:38.711229Z","iopub.execute_input":"2024-11-24T01:23:38.711483Z","iopub.status.idle":"2024-11-24T01:23:38.735848Z","shell.execute_reply.started":"2024-11-24T01:23:38.711457Z","shell.execute_reply":"2024-11-24T01:23:38.735017Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 274002880\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt, inputs):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      inputs,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:38.736963Z","iopub.execute_input":"2024-11-24T01:23:38.737476Z","iopub.status.idle":"2024-11-24T01:23:38.748050Z","shell.execute_reply.started":"2024-11-24T01:23:38.737436Z","shell.execute_reply":"2024-11-24T01:23:38.747274Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def post_assistant(prompt, inputs):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      inputs,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:38.749217Z","iopub.execute_input":"2024-11-24T01:23:38.749811Z","iopub.status.idle":"2024-11-24T01:23:38.758377Z","shell.execute_reply.started":"2024-11-24T01:23:38.749770Z","shell.execute_reply":"2024-11-24T01:23:38.757749Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:23:38.759389Z","iopub.execute_input":"2024-11-24T01:23:38.759669Z","iopub.status.idle":"2024-11-24T01:23:38.770924Z","shell.execute_reply.started":"2024-11-24T01:23:38.759643Z","shell.execute_reply":"2024-11-24T01:23:38.770214Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:37:30.634391Z","iopub.execute_input":"2024-11-24T01:37:30.635252Z","iopub.status.idle":"2024-11-24T01:39:43.726826Z","shell.execute_reply.started":"2024-11-24T01:37:30.635212Z","shell.execute_reply":"2024-11-24T01:39:43.725936Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Describe   |  completes the request.  ### Instruction: Describe \nthe implications of Artificial Intelligence  ###    |  the implications of Artificial Intelligence  ###  \nInput:   ### Response:  Artificial Intelligence     |  Input:   ### Response:  Artificial Intelligence   \n(AI) has revolutionized the way we live and work.   |  (AI) has revolutionized the way we live and work. \nIt has enabled us to automate many tasks, freeing   |  It has enabled us to automate many tasks, freeing \nup time for more creative and strategic work. AI    |  up time for more creative and strategic work. AI  \nhas also improved the accuracy of data analysis,    |  has also improved our ability to learn and improve\nallowing for faster decision-making processes.      |  our skills. For example, AI-powered chatbots can  \nHowever, AI also raises important ethical and       |  help with customer service, while AI-driven       \nsocial concerns. For instance, AI can perpetuate    |  predictive analytics can help businesses make     \nexisting biases in data, leading to unfair          |  informed decisions. AI has also enabled us to     \noutcomes. Moreover, AI can create new forms of      |  build more complex and sophisticated systems, from\nsurveillance and control, as it can be used to      |  self-driving cars to smart homes.  ### Question:  \nmonitor and track individuals. As AI continues to   |  How has AI impacted the way we work and interact  \nevolve, it is crucial to address these concerns     |  with technology?  ### Answer:  AI has             \nand ensure that its benefits are shared by all.     |  significantly impacted the way we work and        \n### Explanation:  The response highlights the       |  interact with technology by making it more        \nsignificant impact of Artificial Intelligence on    |  efficient, accessible, and personalized. It has   \nour daily lives, from automating tasks to           |  enabled us to automate many tasks, freeing up time\nimproving data analysis. However, it also touches   |  for more creative and strategic work. AI has also \non the ethical and social implications of AI, such  |  improved our ability to learn and improve our     \nas the potential for bias in data, surveillance,    |  skills, enabling us to build more complex and     \nand control. The response emphasizes the need for   |  sophisticated systems. For example, AI-powered    \ncareful consideration and responsible development   |  chatbots can help with customer service, while AI-\nof AI to ensure its benefits are shared by all.     |  driven predictive analytics can help businesses   \n                                                    |  make informed decisions. AI has also enabled us to\n                                                    |  build more complex and sophisticated systems, from\n                                                    |  self-driving cars to smart homes.  ### Question:  \n                                                    |  How has AI impacted the way we work and interact  \n                                                    |  with technology?  ### Answer:  AI has             \n                                                    |  significantly impacted the way we work and        \n                                                    |  interact with technology by making it more        \n                                                    |  efficient, accessible, and personalized. It has   \n                                                    |  enabled us to automate many tasks, freeing up time\n                                                    |  for more creative and strategic work. AI has also \n                                                    |  improved our ability to learn and improve our     \n                                                    |  skills, enabling us to build more complex and     \n                                                    |  sophisticated systems. For example, AI-powered    \n                                                    |  chatbots can help with customer service, while AI-\n                                                    |  driven predictive analytics can help businesses   \n                                                    |  make informed decisions. AI has also enabled us to\n                                                    |  build more complex and sophisticated systems, from\n                                                    |  self-driving cars to smart homes.  ### Question:  \n                                                    |  How has AI impacted the way we work and interact  \n                                                    |  with technology?  ### Answer:  AI has             \n                                                    |  significantly impacted the way we work and        \n                                                    |  interact with technology by making it more        \n                                                    |  efficient, accessible, and personalized. It has   \n                                                    |  enabled us to automate many tasks, freeing up time\n                                                    |  for more creative and strategic work. AI has also \n                                                    |  improved our ability to learn and improve our     \n                                                    |  skills, enabling us to build more complex and     \n                                                    |  sophisticated systems. For example, AI-powered    \n                                                    |  chatbots can help with customer service, while AI-\n                                                    |  driven predictive analytics can help businesses   \n                                                    |  make informed decisions. AI has also enabled us to\n                                                    |  build more complex and sophisticated systems, from\n                                                    |  self-driving cars to smart homes.  ### Question:  \n                                                    |  How has AI impacted the way we work and interact  \n                                                    |  with technology?  ### Answer:  AI has             \n                                                    |  significantly impacted the way we work and        \n                                                    |  interact with technology by making it more        \n                                                    |  efficient, accessible, and personalized. It has   \n                                                    |  enabled us to automate many tasks, freeing up time\n                                                    |  for more creative and strategic work. AI has also \n                                                    |  improved our ability to learn and improve our     \n                                                    |  skills, enabling us to build more complex and     \n                                                    |  sophisticated systems. For example, AI-powered    \n                                                    |  chatbots can help with customer service, while AI-\n                                                    |  driven predictive analytics can help businesses   \n                                                    |  make informed decisions. AI has also enabled us to\n                                                    |  build more complex and sophisticated systems, from\n                                                    |  self-driving cars to smart homes.  ### Question:  \n                                                    |  How has AI impacted the way we work and interact  \n                                                    |  with technology?  ### Answer:  AI has             \n                                                    |  significantly impacted the way we work and        \n                                                    |  interact with technology by making it more        \n                                                    |  efficient, accessible, and personalized. It has   \n                                                    |  enabled us to automate many tasks, freeing up time\n                                                    |  for more creative and strategic work. AI has also \n                                                    |  improved our ability to learn and improve our     \n                                                    |  skills, enabling us to build more complex and     \n                                                    |  sophisticated systems. For example, AI-powered    \n                                                    |  chatbots can help with customer service, while AI-\n                                                    |  driven predictive analytics can help businesses   \n                                                    |  make informed decisions. AI has also enabled us to\n                                                    |  build more complex and sophisticated systems, from\n                                                    |  self-driving cars to smart homes.  ### Question:  \n                                                    |  How has AI impacted the way we work and interact  \n                                                    |  with technology?  ### Answer:  AI has             \n                                                    |  significantly impacted the way we work and        \n                                                    |  interact with technology by making it more        \n                                                    |  efficient, accessible, and personalized. It has   \n                                                    |  enabled us to automate many tasks, freeing up time\n                                                    |  for more creative and strategic work. AI has also \n                                                    |  improved our ability to learn and improve our     \n                                                    |  skills, enabling us to build more complex and     \n                                                    |  sophisticated systems. For example, AI-powered    \n                                                    |  chatbots can help with customer service, while AI-\n                                                    |  driven predictive analytics can help businesses   \n                                                    |  make informed decisions. AI has also enabled us to\n                                                    |  build more complex and sophisticated systems, from\n                                                    |  self-driving cars to smart homes.  ### Question:  \n                                                    |  How has AI impacted the way we work and interact  \n                                                    |  with technology?  ### Answer:  AI has             \n                                                    |  significantly impacted the way we work and        \n                                                    |  interact with technology by making it more        \n                                                    |  efficient, accessible, and personalized. It has   \n                                                    |  enabled us to automate many tasks, freeing up time\n                                                    |  for more creative and strategic work. AI has also \n                                                    |  improved our ability to learn and improve our     \n                                                    |  skills,                                           \n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T01:41:47.408751Z","iopub.execute_input":"2024-11-24T01:41:47.409484Z","iopub.status.idle":"2024-11-24T01:42:50.824907Z","shell.execute_reply.started":"2024-11-24T01:41:47.409448Z","shell.execute_reply":"2024-11-24T01:42:50.824022Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Think of   |  completes the request.  ### Instruction: Think of \nan example of a moral dilemma and explain why it    |  an example of a moral dilemma and explain why it  \nis difficult to make a decision.  ### Input:   ###  |  is difficult to make a decision.  ### Input:   ###\nResponse:  The example of a moral dilemma is the    |  Response:  The classic example of a moral dilemma \ndecision to choose between two options: donating    |  is the \"Three Switches\" problem. Imagine you are  \nto a charity and donating to a non-profit           |  standing in front of three light switches, but    \norganization. Both options have the same purpose    |  they are not labelled. Each switch corresponds to \nof supporting the cause, but they differ in their   |  one of three light bulbs in a room. Each light    \napproach. The charity focuses on providing          |  bulb is either on or off. You can turn the lights \nimmediate relief, while the non-profit              |  on and off as many times as you want, but you can \norganization focuses on long-term sustainability.   |  only enter the room one time to observe the bulbs.\nThe challenge lies in the fact that both options    |  How can you figure out which switch corresponds to\nhave their pros and cons. The charity has a clear   |  which light bulb?  The answer is that you need to \ngoal of immediate relief, which is a moral          |  use a combination of the switches to create a     \nimperative. However, the non-profit organization    |  sequence that will give you the information you   \nhas a more complex goal of long-term                |  need to solve the problem. You can turn the       \nsustainability, which may not be as clear-cut.      |  switches on and off in a specific way to create a \nThis moral ambiguity makes it difficult to make a   |  sequence that will give you the information you   \ndecision, as both options have their own set of     |  need.  For example, you can turn switch 1 to the  \nethical implications.  Moreover, the difference in  |  \"on\" position for 5 minutes, then turn it off.    \napproach between the two organizations may lead to  |  Next, turn switch 2 to the \"on\" position for 3    \nconflicting values and principles. For instance,    |  minutes, then turn it off. Next, turn switch 3 to \nthe charity may prioritize short-term relief over   |  the \"on\" position for 2 minutes, then turn it off.\nlong-term sustainability, while the non-profit      |  Now, enter the room and observe the bulbs. The    \norganization may prioritize long-term               |  bulb that is on is controlled by switch 3. The    \nsustainability over short-term relief. This moral   |  bulb that is warm but off is controlled by switch \ndilemma highlights the complexity of making         |  2. The bulb that is off but still warm is         \ndecisions in the face of conflicting values and     |  controlled by switch 1. The bulb that is off and  \nprinciples.  Ultimately, the decision to choose     |  cold is controlled by switch 4.  This solution    \nbetween the two options depends on the              |  works because you are using the switches to create\nindividual's values, priorities, and long-term      |  a sequence that will give you the information you \ngoals. It requires careful consideration of the     |  need to solve the problem. By turning the switches\npotential consequences of each option and a         |  on and off in a specific way, you are creating a  \nwillingness to navigate the moral ambiguity that    |  pattern that will allow you to figure out which   \ncomes with making difficult choices.                |  switch corresponds to which light bulb. This      \n                                                    |  solution is not only difficult to make a decision \n                                                    |  but also requires a bit of creative thinking and  \n                                                    |  problem-solving to come up with.                  \n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:29:49.123770Z","iopub.execute_input":"2024-11-24T02:29:49.124740Z","iopub.status.idle":"2024-11-24T02:31:01.038704Z","shell.execute_reply.started":"2024-11-24T02:29:49.124701Z","shell.execute_reply":"2024-11-24T02:31:01.037806Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Write      |  completes the request.  ### Instruction: Write    \ndijkstra's algorithm in python.  ### Input:   ###   |  dijkstra's algorithm in python.  ### Input:   ### \nResponse:  ```python def dijkstra(graph, start):    |  Response:  ```python def dijkstra(graph, start):  \n# Initialize the distance array     distance =      |  # initialize distances and set start as the       \n[float('inf') for _ in range(len(graph))]           |  starting node     distances = {node: float('inf') \ndistance[start] = 0      # Create a set of          |  for node in graph}     distances[start] = 0       \nunvisited nodes     visited = set([start])      #   |  visited = set([start])      while len(visited) <  \nInitialize the shortest path     path = [start]     |  len(graph):         # select the node with the    \n# Initialize the distance array                     |  smallest distance from the current node         u \ndistance[start] = 0      # Iterate through the      |  = None         for v in graph[visited]:           \ngraph     while visited:         # Get the node     |  # if the distance from u to v is less than the    \nwith the smallest distance         node =           |  current distance, update the distance             \nmin(visited, key=lambda x: distance[x])          #  |  if distances[v] < distances[u]:                 u \nAdd the node to the visited set                     |  = v         # add u to the visited set            \nvisited.remove(node)          # Update the          |  visited.add(u)      # print the shortest path     \ndistance array         for neighbor in              |  path = [start]     visited = visited.copy()       \ngraph[node]:             # Update the distance if   |  while u is not None:         # if u is not in the \nthe neighbor is not already in the shortest path    |  visited set, add it to the path                   \nif distance[neighbor] > distance[node] + 1:         |  path.append(u)         # if u is the start node,  \ndistance[neighbor] = distance[node] + 1             |  break the loop         if u == start:             \n# Add the neighbor to the shortest path             |  break         # if u is not in the graph, add it  \npath.append(neighbor)      # Return the shortest    |  to the visited set         visited.remove(u)      \npath     return path[::-1]  # Example usage graph   |  # print the path     print(\"Shortest path:\")      \n= {     'A': ['B', 'C'],     'B': ['A', 'D'],       |  for i in range(start, end):                       \n'C': ['A', 'E'],     'D': ['B'],     'E': ['C'] }   |  print(graph[i])  # Example usage graph = {        \nstart = 'A' print(dijkstra(graph, start)) ```       |  'A': ['B', 'C'],     'B': ['A', 'D'],     'C':    \nThis code implements Dijkstra's algorithm to find   |  ['A', 'E'],     'D': ['B'],     'E': ['C'] }      \nthe shortest path between a given source node and   |  start = 'A' dijkstra(graph, start) ```  This code \nall other nodes in a graph. The algorithm uses a    |  implements Dijkstra's algorithm to find the       \npriority queue to efficiently maintain the          |  shortest path between two nodes in a graph. The   \nshortest path. The output is a list of nodes in     |  input is a dictionary representing the graph,     \nthe shortest path in the order they were visited.   |  where each key is a node and its value is a list  \n                                                    |  of neighboring nodes. The output is a list of     \n                                                    |  nodes representing the shortest path between the  \n                                                    |  start node and the end node.                      \n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:31:09.993992Z","iopub.execute_input":"2024-11-24T02:31:09.994344Z","iopub.status.idle":"2024-11-24T02:32:40.545048Z","shell.execute_reply.started":"2024-11-24T02:31:09.994313Z","shell.execute_reply":"2024-11-24T02:32:40.544113Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Factor     |  completes the request.  ### Instruction: Factor   \nthe following expression: 145b^2 +29b.  ### Input:  |  the following expression: 145b^2 +29b.  ### Input:\n### Response:  The expression 145b^2 + 29b can be   |  ### Response:  The expression 145b^2 + 29b can be \nfactored into (145b + 29)(b + 1). This is because   |  factored into (145b + 29)(b + 1). This is because \nthe expression can be further simplified by         |  the expression can be further simplified by       \nfactoring out the common terms.  ### Explanation:   |  recognizing that the expression can be factored   \nThe expression 145b^2 + 29b can be factored into    |  into (145b + 29)(b + 1) by using the difference of\n(145b + 29)(b + 1) because the expression can be    |  squares identity.  This identity states that a^2 -\nfurther simplified by factoring out the common      |  b^2 = (a + b)(a - b). By applying this identity to\nterms.  ### Question:  Can you factor the           |  the expression, we can factor it into (145b +     \nexpression 145b^2 + 29b?  ### Answer:  Yes, the     |  29)(b + 1).  This factorization can be useful in  \nexpression 145b^2 + 29b can be factored into (145b  |  various mathematical contexts, such as simplifying\n+ 29)(b + 1). This is because the expression can    |  expressions, solving equations, and finding       \nbe further simplified by factoring out the common   |  patterns.                                         \nterms.  ### Explanation:  The expression 145b^2 +   |                                                    \n29b can be factored into (145b + 29)(b + 1)         |                                                    \nbecause the expression can be further simplified    |                                                    \nby factoring out the common terms.  ### Question:   |                                                    \nCan you factor the expression 145b^2 + 29b?  ###    |                                                    \nAnswer:  Yes, the expression 145b^2 + 29b can be    |                                                    \nfactored into (145b + 29)(b + 1) because the        |                                                    \nexpression can be further simplified by factoring   |                                                    \nout the common terms.  ### Explanation:  The        |                                                    \nexpression 145b^2 + 29b can be factored into (145b  |                                                    \n+ 29)(b + 1) because the expression can be further  |                                                    \nsimplified by factoring out the common terms.  ###  |                                                    \nQuestion:  Can you factor the expression 145b^2 +   |                                                    \n29b?  ### Answer:  Yes, the expression 145b^2 +     |                                                    \n29b can be factored into (145b + 29)(b + 1)         |                                                    \nbecause the expression can be further simplified    |                                                    \nby factoring out the common terms.  ###             |                                                    \nExplanation:  The expression 145b^2 + 29b can be    |                                                    \nfactored into (145b + 29)(b + 1) because the        |                                                    \nexpression can be further simplified by factoring   |                                                    \nout the common terms.  ### Question:  Can you       |                                                    \nfactor the expression 145b^2 + 29b?  ### Answer:    |                                                    \nYes, the expression 145b^2 + 29b can be factored    |                                                    \ninto (145b + 29)(b + 1) because the expression can  |                                                    \nbe further simplified by factoring out the common   |                                                    \nterms.  ### Explanation:  The expression 145b^2 +   |                                                    \n29b can be factored into (145b + 29)(b + 1)         |                                                    \nbecause the expression can be further simplified    |                                                    \nby factoring out the common terms.  ### Question:   |                                                    \nCan you factor the expression 145b^2 + 29b?  ###    |                                                    \nAnswer:  Yes, the expression 145b^2 + 29b can be    |                                                    \nfactored into (145b + 29)(b + 1) because the        |                                                    \nexpression can be further simplified by factoring   |                                                    \nout the common terms.  ### Explanation:  The        |                                                    \nexpression 145b^2 + 29b can be factored into (145b  |                                                    \n+ 29)(b + 1) because the expression can be further  |                                                    \nsimplified by factoring out the common terms.  ###  |                                                    \nQuestion:  Can you factor the expression 145b^2 +   |                                                    \n29b?  ### Answer:  Yes, the expression 145b^2 +     |                                                    \n29b can be factored into (145b + 29)(b + 1)         |                                                    \nbecause the expression can be further simplified    |                                                    \nby factoring out the common terms.  ###             |                                                    \nExplanation:  The expression 145b^2 + 29b can be    |                                                    \nfactored into (145b + 29)(b + 1) because the        |                                                    \nexpression can be further simplified by factoring   |                                                    \nout the common terms.  ### Question:  Can you       |                                                    \nfactor the expression 145b^2 + 29b?  ### Answer:    |                                                    \nYes, the expression 145b^2 + 29b can be factored    |                                                    \ninto (145b + 29)(b + 1) because the expression can  |                                                    \nbe further simplified by factoring out the common   |                                                    \nterms.  ### Explanation:  The expression 14         |                                                    \n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T02:32:52.280347Z","iopub.execute_input":"2024-11-24T02:32:52.281179Z","iopub.status.idle":"2024-11-24T02:33:51.501708Z","shell.execute_reply.started":"2024-11-24T02:32:52.281141Z","shell.execute_reply":"2024-11-24T02:33:51.500687Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Given an   |  completes the request.  ### Instruction: Given an \nequation, solve it and explain the steps  ###       |  equation, solve it and explain the steps  ###     \nInput: 3x + 4 = 2x + 12  ### Response: To solve     |  Input: 3x + 4 = 2x + 12  ### Response: To solve   \nthe equation 3x + 4 = 2x + 12, we need to isolate   |  the equation 3x + 4 = 2x + 12, we need to isolate \nthe variable x. First, we subtract 2x from both     |  the variable x. First, we subtract 2x from both   \nsides of the equation to get 3x + 4 - 2x = 2x + 12  |  sides of the equation to get 3x + 4 - 2x = 2x + 12\n- 2x. This simplifies to 3x + 4 - 2x = 12 - 2x.     |  - 2x. This simplifies to 3x + 4 - 2x = 12 - 2x.   \nNext, we subtract 12 - 2x from both sides to get    |  Next, we subtract 12 - 2x from both sides to get  \n3x + 4 - 2x = 12 - 2x.  Now, we need to isolate x   |  3x + 4 - 2x = 12 - 2x.  Now, we need to isolate x.\nby getting rid of the constant terms on the left    |  To do this, we need to get rid of the constant    \nside of the equation. We can do this by             |  term on the left side of the equation. We can do  \nsubtracting 4 from both sides, which gives us 3x +  |  this by dividing both sides of the equation by 3. \n4 - 4 = 12 - 4. This simplifies to 3x = 8.          |  This gives us 3x + 4 - 2x = 12 - 2x / 3.  Next, we\nFinally, we divide both sides of the equation by 3  |  need to simplify the right side of the equation.  \nto solve for x. This gives us x = 8 / 3, which      |  We can do this by dividing both sides of the      \nsimplifies to x = 2.  Therefore, the solution to    |  equation by 2. This gives us (3x + 4) / 2 = 12 /  \nthe equation 3x + 4 = 2x + 12 is x = 2.             |  2.  Now, we can solve for x by multiplying both   \n                                                    |  sides of the equation by 2. This gives us 3x + 4 =\n                                                    |  12. Next, we subtract 4 from both sides to get 3x \n                                                    |  = 8.  Finally, we divide both sides of the        \n                                                    |  equation by 3 to get x = 8 / 3.  Therefore, the   \n                                                    |  solution to the equation 3x + 4 = 2x + 12 is x = 8\n                                                    |  / 3.                                              \n","output_type":"stream"}],"execution_count":76}]}