{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/OpenHermes2.5_Mistral7B_Fine_Tune_PEFT_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:46:07.891846Z",
          "iopub.execute_input": "2024-12-01T11:46:07.892827Z",
          "iopub.status.idle": "2024-12-01T11:46:57.538050Z",
          "shell.execute_reply.started": "2024-12-01T11:46:07.892783Z",
          "shell.execute_reply": "2024-12-01T11:46:57.536854Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-12-01T11:46:57.540418Z",
          "iopub.execute_input": "2024-12-01T11:46:57.540818Z",
          "iopub.status.idle": "2024-12-01T11:47:16.076996Z",
          "shell.execute_reply.started": "2024-12-01T11:46:57.540777Z",
          "shell.execute_reply": "2024-12-01T11:47:16.076063Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-12-01T11:47:16.078172Z",
          "iopub.execute_input": "2024-12-01T11:47:16.078457Z",
          "iopub.status.idle": "2024-12-01T11:47:16.085148Z",
          "shell.execute_reply.started": "2024-12-01T11:47:16.078431Z",
          "shell.execute_reply": "2024-12-01T11:47:16.084307Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n",
        "#model_name = url.split('.co/')[-1]\n",
        "\n",
        "model_name = 'unsloth/OpenHermes-2.5-Mistral-7B'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:47:16.087515Z",
          "iopub.execute_input": "2024-12-01T11:47:16.087849Z",
          "iopub.status.idle": "2024-12-01T11:47:16.110694Z",
          "shell.execute_reply.started": "2024-12-01T11:47:16.087797Z",
          "shell.execute_reply": "2024-12-01T11:47:16.109931Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "  if base == True:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype = torch.float16,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "  else:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = True,\n",
        "      bnb_4bit_quant_type = 'nf4',\n",
        "      bnb_4bit_compute_dtype = torch.float16,\n",
        "      bnb_4bit_use_double_quant = True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config = bnb_config,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:47:16.111644Z",
          "iopub.execute_input": "2024-12-01T11:47:16.111930Z",
          "iopub.status.idle": "2024-12-01T11:47:16.120046Z",
          "shell.execute_reply.started": "2024-12-01T11:47:16.111905Z",
          "shell.execute_reply": "2024-12-01T11:47:16.119389Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:47:16.121099Z",
          "iopub.execute_input": "2024-12-01T11:47:16.121377Z",
          "iopub.status.idle": "2024-12-01T11:53:43.675310Z",
          "shell.execute_reply.started": "2024-12-01T11:47:16.121353Z",
          "shell.execute_reply": "2024-12-01T11:53:43.674587Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:43.676531Z",
          "iopub.execute_input": "2024-12-01T11:53:43.676795Z",
          "iopub.status.idle": "2024-12-01T11:53:43.684152Z",
          "shell.execute_reply.started": "2024-12-01T11:53:43.676770Z",
          "shell.execute_reply": "2024-12-01T11:53:43.683328Z"
        },
        "outputId": "4f6343f3-c685-4f95-8011-d2c08252be07"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3752087552\nTrainable parameters : 262426624\nTrainable percentage: 6.99%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:43.685381Z",
          "iopub.execute_input": "2024-12-01T11:53:43.685729Z",
          "iopub.status.idle": "2024-12-01T11:53:45.615916Z",
          "shell.execute_reply.started": "2024-12-01T11:53:43.685692Z",
          "shell.execute_reply": "2024-12-01T11:53:45.615090Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n",
        "#dataset_name = url.split('datasets/')[-1]\n",
        "\n",
        "dataset_name = 'yahma/alpaca-cleaned'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:45.616954Z",
          "iopub.execute_input": "2024-12-01T11:53:45.617235Z",
          "iopub.status.idle": "2024-12-01T11:53:45.621298Z",
          "shell.execute_reply.started": "2024-12-01T11:53:45.617209Z",
          "shell.execute_reply": "2024-12-01T11:53:45.620387Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:45.624151Z",
          "iopub.execute_input": "2024-12-01T11:53:45.624436Z",
          "iopub.status.idle": "2024-12-01T11:53:45.632673Z",
          "shell.execute_reply.started": "2024-12-01T11:53:45.624411Z",
          "shell.execute_reply": "2024-12-01T11:53:45.631929Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:45.633506Z",
          "iopub.execute_input": "2024-12-01T11:53:45.633745Z",
          "iopub.status.idle": "2024-12-01T11:53:48.383143Z",
          "shell.execute_reply.started": "2024-12-01T11:53:45.633714Z",
          "shell.execute_reply": "2024-12-01T11:53:48.382313Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(10000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:48.384292Z",
          "iopub.execute_input": "2024-12-01T11:53:48.384596Z",
          "iopub.status.idle": "2024-12-01T11:53:48.390886Z",
          "shell.execute_reply.started": "2024-12-01T11:53:48.384568Z",
          "shell.execute_reply": "2024-12-01T11:53:48.390054Z"
        },
        "id": "p1LdtNMPo3ko"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:48.391865Z",
          "iopub.execute_input": "2024-12-01T11:53:48.392141Z",
          "iopub.status.idle": "2024-12-01T11:53:48.517482Z",
          "shell.execute_reply.started": "2024-12-01T11:53:48.392116Z",
          "shell.execute_reply": "2024-12-01T11:53:48.516420Z"
        },
        "outputId": "17a65ef4-cbd9-493f-e6a5-aebda80c05af"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              output input  \\\n0  1. Eat a balanced and nutritious diet: Make su...         \n1  The three primary colors are red, blue, and ye...         \n2  An atom is the basic building block of all mat...         \n3  There are several ways to reduce air pollution...         \n4  I had to make a difficult decision when I was ...         \n\n                                         instruction  \n0               Give three tips for staying healthy.  \n1                 What are the three primary colors?  \n2                 Describe the structure of an atom.  \n3                   How can we reduce air pollution?  \n4  Pretend you are a project manager of a constru...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>output</th>\n      <th>input</th>\n      <th>instruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n      <td></td>\n      <td>Give three tips for staying healthy.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The three primary colors are red, blue, and ye...</td>\n      <td></td>\n      <td>What are the three primary colors?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>An atom is the basic building block of all mat...</td>\n      <td></td>\n      <td>Describe the structure of an atom.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>There are several ways to reduce air pollution...</td>\n      <td></td>\n      <td>How can we reduce air pollution?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I had to make a difficult decision when I was ...</td>\n      <td></td>\n      <td>Pretend you are a project manager of a constru...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:48.518523Z",
          "iopub.execute_input": "2024-12-01T11:53:48.518790Z",
          "iopub.status.idle": "2024-12-01T11:53:48.526169Z",
          "shell.execute_reply.started": "2024-12-01T11:53:48.518765Z",
          "shell.execute_reply": "2024-12-01T11:53:48.525256Z"
        },
        "outputId": "4ca7529c-0e54-425e-edfc-a15fe9dbc4c6"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n 'input': '',\n 'instruction': 'Give three tips for staying healthy.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:48.527403Z",
          "iopub.execute_input": "2024-12-01T11:53:48.527757Z",
          "iopub.status.idle": "2024-12-01T11:53:48.537944Z",
          "shell.execute_reply.started": "2024-12-01T11:53:48.527708Z",
          "shell.execute_reply": "2024-12-01T11:53:48.537182Z"
        },
        "outputId": "b2c7586d-29da-482c-bc6f-3649ee935eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['output', 'input', 'instruction']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:48.538876Z",
          "iopub.execute_input": "2024-12-01T11:53:48.539088Z",
          "iopub.status.idle": "2024-12-01T11:53:48.547018Z",
          "shell.execute_reply.started": "2024-12-01T11:53:48.539067Z",
          "shell.execute_reply": "2024-12-01T11:53:48.546094Z"
        },
        "id": "sz_B8Z00o3kp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def preprocess(examples):\n",
        "  instruction = examples['instruction']\n",
        "  input = examples['input']\n",
        "  output = examples['output']\n",
        "\n",
        "  text = prompt_format.format(instruction, input, output) + EOS_TOKEN\n",
        "  return {'prompt' : text}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:48.548018Z",
          "iopub.execute_input": "2024-12-01T11:53:48.548261Z",
          "iopub.status.idle": "2024-12-01T11:53:48.556434Z",
          "shell.execute_reply.started": "2024-12-01T11:53:48.548238Z",
          "shell.execute_reply": "2024-12-01T11:53:48.555621Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(preprocess, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:48.557459Z",
          "iopub.execute_input": "2024-12-01T11:53:48.557801Z",
          "iopub.status.idle": "2024-12-01T11:53:49.108879Z",
          "shell.execute_reply.started": "2024-12-01T11:53:48.557766Z",
          "shell.execute_reply": "2024-12-01T11:53:49.107692Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:49.109868Z",
          "iopub.execute_input": "2024-12-01T11:53:49.110118Z",
          "iopub.status.idle": "2024-12-01T11:53:49.115173Z",
          "shell.execute_reply.started": "2024-12-01T11:53:49.110095Z",
          "shell.execute_reply": "2024-12-01T11:53:49.114197Z"
        },
        "outputId": "17b28c8f-1093-4c5a-ab2b-674ab0a8c918"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:49.116578Z",
          "iopub.execute_input": "2024-12-01T11:53:49.116894Z",
          "iopub.status.idle": "2024-12-01T11:53:49.135947Z",
          "shell.execute_reply.started": "2024-12-01T11:53:49.116870Z",
          "shell.execute_reply": "2024-12-01T11:53:49.135048Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:49.136809Z",
          "iopub.execute_input": "2024-12-01T11:53:49.137022Z",
          "iopub.status.idle": "2024-12-01T11:53:57.623257Z",
          "shell.execute_reply.started": "2024-12-01T11:53:49.137001Z",
          "shell.execute_reply": "2024-12-01T11:53:57.622395Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.624449Z",
          "iopub.execute_input": "2024-12-01T11:53:57.624815Z",
          "iopub.status.idle": "2024-12-01T11:53:57.632009Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.624776Z",
          "shell.execute_reply": "2024-12-01T11:53:57.631012Z"
        },
        "outputId": "8a805cba-e102-41bc-c21b-abf489017f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.632976Z",
          "iopub.execute_input": "2024-12-01T11:53:57.633211Z",
          "iopub.status.idle": "2024-12-01T11:53:57.695992Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.633188Z",
          "shell.execute_reply": "2024-12-01T11:53:57.695038Z"
        },
        "outputId": "af0ccdd7-4cec-4ce7-8d5f-d9bf6540d023"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.697021Z",
          "iopub.execute_input": "2024-12-01T11:53:57.697293Z",
          "iopub.status.idle": "2024-12-01T11:53:57.702751Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.697244Z",
          "shell.execute_reply": "2024-12-01T11:53:57.701866Z"
        },
        "outputId": "bdedca0d-af03-4ac6-d0e2-937b0be58453"
      },
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.703786Z",
          "iopub.execute_input": "2024-12-01T11:53:57.704052Z",
          "iopub.status.idle": "2024-12-01T11:53:57.726652Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.704029Z",
          "shell.execute_reply": "2024-12-01T11:53:57.725917Z"
        },
        "outputId": "cd867cc0-85f1-4788-fdf5-924f303c64e9"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  Below is an instruction that describes a task,...   \n1  Below is an instruction that describes a task,...   \n2  Below is an instruction that describes a task,...   \n3  Below is an instruction that describes a task,...   \n4  Below is an instruction that describes a task,...   \n\n                                           input_ids  \\\n0  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n1  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n2  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n3  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n4  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.727507Z",
          "iopub.execute_input": "2024-12-01T11:53:57.727719Z",
          "iopub.status.idle": "2024-12-01T11:53:57.732651Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.727697Z",
          "shell.execute_reply": "2024-12-01T11:53:57.731808Z"
        },
        "outputId": "a29dfdcd-c77b-4f07-e3e4-cffba1938e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the given poem and identify its main theme.\n\n### Input:\nTwo roads diverged in a yellow wood,\\nAnd sorry I could not travel both\\nAnd be one traveler, long I stood\\nAnd looked down one as far as I could\\nTo where it bent in the undergrowth;\\n\\nThen took the other, as just as fair,\\nAnd having perhaps the better claim,\\nBecause it was grassy and wanted wear;\\nThough as for that the passing there\\nHad worn them really about the same,\\nThe roads that morning equally lay\\nIn leaves no step had trodden black.\\nOh, I left the first for another day!\\nYet knowing how way leads on to way,\\nI doubted if I should ever come back.\\n\\nI shall be telling this with a sigh\\nSomewhere ages and ages hence:\\nTwo roads diverged in a wood, and I—\\nI took the one less traveled by,\\nAnd that has made all the difference.\n\n### Response:\nThe main theme of the poem is the importance of making choices and the impact of those choices on one's life. The speaker is faced with a decision between two paths and ultimately chooses the one less traveled, which ultimately shapes their life experience.<|im_end|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.733654Z",
          "iopub.execute_input": "2024-12-01T11:53:57.733888Z",
          "iopub.status.idle": "2024-12-01T11:53:57.742629Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.733867Z",
          "shell.execute_reply": "2024-12-01T11:53:57.741875Z"
        },
        "outputId": "e0e1e178-4836-4c53-c850-ed0a54e33eec"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 20811, 349, 396, 13126, 369, 13966, 264, 3638, 28725, 5881, 1360, 395, 396, 2787, 369, 5312, 3629, 2758, 28723, 12018, 264, 2899, 369, 6582, 1999, 2691, 274, 272, 2159, 28723, 13, 13, 27332, 3133, 3112, 28747, 13, 27554, 1374, 272, 2078, 16067, 304, 9051, 871, 2191, 7335, 28723, 13, 13, 27332, 11232, 28747, 13, 13849, 15014, 19002, 2560, 297, 264, 9684, 4768, 2142, 28711, 2467, 7371, 315, 829, 459, 4530, 1560, 28756, 28711, 2467, 347, 624, 4530, 263, 28725, 1043, 315, 4857, 28756, 28711, 2467, 2382, 1060, 624, 390, 2082, 390, 315, 829, 28756, 28711, 1551, 970, 378, 16127, 297, 272, 916, 20621, 362, 8511, 28711, 28756, 28711, 11341, 2056, 272, 799, 28725, 390, 776, 390, 4968, 2142, 28711, 2467, 2461, 5230, 272, 1873, 3452, 2142, 28711, 17098, 378, 403, 10109, 28724, 304, 2613, 7656, 8511, 28711, 1227, 900, 390, 354, 369, 272, 9720, 736, 28756, 28711, 28769, 316, 15903, 706, 1528, 684, 272, 1348, 2142, 28711, 1014, 15014, 369, 3970, 13387, 4897, 28756, 28711, 657, 8049, 708, 3707, 553, 4056, 1036, 269, 2687, 5923, 28711, 6155, 28725, 315, 1749, 272, 907, 354, 1698, 1370, 8263, 28711, 28802, 299, 8215, 910, 1069, 8681, 356, 298, 1069, 2142, 28711, 28737, 6217, 286, 513, 315, 1023, 2270, 1567, 852, 5923, 28711, 28756, 28711, 28737, 4579, 347, 7124, 456, 395, 264, 19553, 28756, 28711, 11600, 2956, 14506, 304, 14506, 12211, 9941, 28711, 13849, 15014, 19002, 2560, 297, 264, 4768, 28725, 304, 315, 28821, 28756, 28711, 28737, 2056, 272, 624, 2108, 19948, 486, 2142, 28711, 2467, 369, 659, 1269, 544, 272, 5133, 28723, 13, 13, 27332, 12107, 28747, 13, 1014, 2191, 7335, 302, 272, 16067, 349, 272, 9545, 302, 2492, 10475, 304, 272, 5088, 302, 1395, 10475, 356, 624, 28742, 28713, 1411, 28723, 415, 17153, 349, 12565, 395, 264, 5161, 1444, 989, 12924, 304, 12665, 2183, 14779, 272, 624, 2108, 19948, 28725, 690, 12665, 17187, 652, 1411, 2659, 28723, 32000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.748587Z",
          "iopub.execute_input": "2024-12-01T11:53:57.748963Z",
          "iopub.status.idle": "2024-12-01T11:53:57.753145Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.748938Z",
          "shell.execute_reply": "2024-12-01T11:53:57.752330Z"
        },
        "outputId": "17042b17-278d-46d7-c17d-a8b7cdf7d673"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.754396Z",
          "iopub.execute_input": "2024-12-01T11:53:57.754738Z",
          "iopub.status.idle": "2024-12-01T11:53:57.763511Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.754692Z",
          "shell.execute_reply": "2024-12-01T11:53:57.762786Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.764352Z",
          "iopub.execute_input": "2024-12-01T11:53:57.764619Z",
          "iopub.status.idle": "2024-12-01T11:53:57.772246Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.764594Z",
          "shell.execute_reply": "2024-12-01T11:53:57.771532Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.773254Z",
          "iopub.execute_input": "2024-12-01T11:53:57.773561Z",
          "iopub.status.idle": "2024-12-01T11:53:57.780720Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.773525Z",
          "shell.execute_reply": "2024-12-01T11:53:57.779900Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 32\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.781732Z",
          "iopub.execute_input": "2024-12-01T11:53:57.782044Z",
          "iopub.status.idle": "2024-12-01T11:53:57.789724Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.782005Z",
          "shell.execute_reply": "2024-12-01T11:53:57.788958Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:57.790682Z",
          "iopub.execute_input": "2024-12-01T11:53:57.791005Z",
          "iopub.status.idle": "2024-12-01T11:53:58.991786Z",
          "shell.execute_reply.started": "2024-12-01T11:53:57.790959Z",
          "shell.execute_reply": "2024-12-01T11:53:58.990797Z"
        },
        "outputId": "3effa8f7-af59-47fd-c709-54cdc8100318"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 83,886,080 || all params: 7,325,634,560 || trainable%: 1.1451\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ikF6Yfkz1myd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:58.993168Z",
          "iopub.execute_input": "2024-12-01T11:53:58.993587Z",
          "iopub.status.idle": "2024-12-01T11:53:59.010213Z",
          "shell.execute_reply.started": "2024-12-01T11:53:58.993547Z",
          "shell.execute_reply": "2024-12-01T11:53:59.009353Z"
        },
        "outputId": "8adeda45-d161-4004-e10a-e91feda771d6"
      },
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32002, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:59.011434Z",
          "iopub.execute_input": "2024-12-01T11:53:59.012138Z",
          "iopub.status.idle": "2024-12-01T11:53:59.114308Z",
          "shell.execute_reply.started": "2024-12-01T11:53:59.012097Z",
          "shell.execute_reply": "2024-12-01T11:53:59.113567Z"
        },
        "outputId": "b341022a-b41c-4ad6-e1f8-9ea12276903d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3835973632\nTrainable parameters : 83886080\nTrainable percentage: 2.19%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:59.115400Z",
          "iopub.execute_input": "2024-12-01T11:53:59.115709Z",
          "iopub.status.idle": "2024-12-01T11:53:59.126703Z",
          "shell.execute_reply.started": "2024-12-01T11:53:59.115679Z",
          "shell.execute_reply": "2024-12-01T11:53:59.125851Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 2\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:59.127661Z",
          "iopub.execute_input": "2024-12-01T11:53:59.127946Z",
          "iopub.status.idle": "2024-12-01T11:53:59.174918Z",
          "shell.execute_reply.started": "2024-12-01T11:53:59.127921Z",
          "shell.execute_reply": "2024-12-01T11:53:59.174078Z"
        },
        "outputId": "52a12a43-19df-4f26-9c72-4b47aef2ca8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec01_11-53-59_c5906002e26d,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:53:59.175970Z",
          "iopub.execute_input": "2024-12-01T11:53:59.176220Z",
          "iopub.status.idle": "2024-12-01T11:54:01.118045Z",
          "shell.execute_reply.started": "2024-12-01T11:53:59.176195Z",
          "shell.execute_reply": "2024-12-01T11:54:01.117174Z"
        },
        "outputId": "976925b2-ea3c-4081-84a6-94f43ee41688"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7da5a354e770>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T11:54:01.118989Z",
          "iopub.execute_input": "2024-12-01T11:54:01.119242Z",
          "iopub.status.idle": "2024-12-01T13:30:07.780473Z",
          "shell.execute_reply.started": "2024-12-01T11:54:01.119216Z",
          "shell.execute_reply": "2024-12-01T13:30:07.779802Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:30:07.781628Z",
          "iopub.execute_input": "2024-12-01T13:30:07.781917Z",
          "iopub.status.idle": "2024-12-01T13:33:20.116240Z",
          "shell.execute_reply.started": "2024-12-01T13:30:07.781890Z",
          "shell.execute_reply": "2024-12-01T13:33:20.115279Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:20.117699Z",
          "iopub.execute_input": "2024-12-01T13:33:20.118095Z",
          "iopub.status.idle": "2024-12-01T13:33:22.103151Z",
          "shell.execute_reply.started": "2024-12-01T13:33:20.118058Z",
          "shell.execute_reply": "2024-12-01T13:33:22.102281Z"
        },
        "outputId": "aba25b35-1f76-4a42-b769-86a17a696703"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--OpenHermes-2.5-Mistral-7B/snapshots/8ec3fc1a46933db0641d40594ddac3dc8921834a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32002\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--OpenHermes-2.5-Mistral-7B/snapshots/8ec3fc1a46933db0641d40594ddac3dc8921834a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32002\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:22.104469Z",
          "iopub.execute_input": "2024-12-01T13:33:22.104752Z",
          "iopub.status.idle": "2024-12-01T13:33:22.423479Z",
          "shell.execute_reply.started": "2024-12-01T13:33:22.104725Z",
          "shell.execute_reply": "2024-12-01T13:33:22.422547Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:22.424723Z",
          "iopub.execute_input": "2024-12-01T13:33:22.425091Z",
          "iopub.status.idle": "2024-12-01T13:33:22.437911Z",
          "shell.execute_reply.started": "2024-12-01T13:33:22.425053Z",
          "shell.execute_reply": "2024-12-01T13:33:22.437008Z"
        },
        "id": "6QhHruSCo3ky",
        "outputId": "a45c9af0-aa09-447a-c160-fb5580f5229d"
      },
      "outputs": [
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:22.438965Z",
          "iopub.execute_input": "2024-12-01T13:33:22.439223Z",
          "iopub.status.idle": "2024-12-01T13:33:23.854865Z",
          "shell.execute_reply.started": "2024-12-01T13:33:22.439199Z",
          "shell.execute_reply": "2024-12-01T13:33:23.854158Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "RRE7be4io3ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:23.855909Z",
          "iopub.execute_input": "2024-12-01T13:33:23.856248Z",
          "iopub.status.idle": "2024-12-01T13:33:42.210863Z",
          "shell.execute_reply.started": "2024-12-01T13:33:23.856211Z",
          "shell.execute_reply": "2024-12-01T13:33:42.210051Z"
        },
        "id": "Cz7G_lifo3ky"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:42.212044Z",
          "iopub.execute_input": "2024-12-01T13:33:42.212663Z",
          "iopub.status.idle": "2024-12-01T13:33:42.222568Z",
          "shell.execute_reply.started": "2024-12-01T13:33:42.212622Z",
          "shell.execute_reply": "2024-12-01T13:33:42.221782Z"
        },
        "id": "y5_FNjBHo3ky",
        "outputId": "1c29ec5a-8de6-4ff7-f5f8-5878056b12cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3752087552\nTrainable parameters : 262426624\nTrainable percentage: 6.99%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:42.223466Z",
          "iopub.execute_input": "2024-12-01T13:33:42.223715Z",
          "iopub.status.idle": "2024-12-01T13:33:42.252983Z",
          "shell.execute_reply.started": "2024-12-01T13:33:42.223689Z",
          "shell.execute_reply": "2024-12-01T13:33:42.252053Z"
        },
        "id": "5vnSfBIdo3ky",
        "outputId": "fc8a4466-0402-4bf0-ec75-b439984ea9fa"
      },
      "outputs": [
        {
          "execution_count": 47,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32002, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:42.254018Z",
          "iopub.execute_input": "2024-12-01T13:33:42.254228Z",
          "iopub.status.idle": "2024-12-01T13:33:42.285433Z",
          "shell.execute_reply.started": "2024-12-01T13:33:42.254206Z",
          "shell.execute_reply": "2024-12-01T13:33:42.284636Z"
        },
        "id": "yhFNpqkSo3ky",
        "outputId": "bcc85e56-09e3-4eef-8a3f-035dd461ce53"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3919859712\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt, inputs):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      inputs,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:42.286646Z",
          "iopub.execute_input": "2024-12-01T13:33:42.287312Z",
          "iopub.status.idle": "2024-12-01T13:33:42.293175Z",
          "shell.execute_reply.started": "2024-12-01T13:33:42.287250Z",
          "shell.execute_reply": "2024-12-01T13:33:42.292320Z"
        },
        "id": "QF8FxUsto3ky"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt, inputs):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      inputs,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:42.294337Z",
          "iopub.execute_input": "2024-12-01T13:33:42.294674Z",
          "iopub.status.idle": "2024-12-01T13:33:42.305181Z",
          "shell.execute_reply.started": "2024-12-01T13:33:42.294639Z",
          "shell.execute_reply": "2024-12-01T13:33:42.304307Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:33:42.306219Z",
          "iopub.execute_input": "2024-12-01T13:33:42.306534Z",
          "iopub.status.idle": "2024-12-01T13:33:42.315351Z",
          "shell.execute_reply.started": "2024-12-01T13:33:42.306509Z",
          "shell.execute_reply": "2024-12-01T13:33:42.314575Z"
        },
        "id": "ds6-Koqwo3ky"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:46:18.755059Z",
          "iopub.execute_input": "2024-12-01T13:46:18.755793Z",
          "iopub.status.idle": "2024-12-01T13:46:32.503183Z",
          "shell.execute_reply.started": "2024-12-01T13:46:18.755757Z",
          "shell.execute_reply": "2024-12-01T13:46:32.502303Z"
        },
        "outputId": "670d7785-53d9-4761-e52b-b9a1b8b37c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: List two   |  completes the request.  ### Instruction: List two \nproblems caused by climate change  ### Input:       |  problems caused by climate change  ### Input:     \n### Response:  Two problems caused by climate       |  ### Response:  Two problems caused by climate     \nchange are rising sea levels and increased          |  change are rising sea levels and increased        \nfrequency of extreme weather events. Rising sea     |  frequency of extreme weather events. Rising sea   \nlevels are caused by the melting of polar ice caps  |  levels can lead to coastal flooding and erosion,  \nand glaciers, which results in flooding of coastal  |  while increased frequency of extreme weather      \nareas and loss of land. Increased frequency of      |  events can cause damage to infrastructure and     \nextreme weather events, such as hurricanes,         |  property.                                         \ndroughts, and heatwaves, can lead to loss of life,  |                                                    \nproperty damage, and disruption of food and water   |                                                    \nsupplies.                                           |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T13:56:05.241547Z",
          "iopub.execute_input": "2024-12-01T13:56:05.242428Z",
          "iopub.status.idle": "2024-12-01T13:57:19.568936Z",
          "shell.execute_reply.started": "2024-12-01T13:56:05.242391Z",
          "shell.execute_reply": "2024-12-01T13:57:19.568001Z"
        },
        "outputId": "5601b802-f03a-4feb-c3bc-07918442c4d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Generate   |  completes the request.  ### Instruction: Generate \na rap verse about the given topic.  ### Input:      |  a rap verse about the given topic.  ### Input:    \nTopic: The importance of learning  ### Response:    |  Topic: The importance of learning  ### Response:  \nYo, listen up, I'm here to spit some truth, About   |  Yo, listen up, I'm here to spit some truth, About \nthe importance of learning, it's the key to truth,  |  the importance of learning, it's the key to truth,\nKnowledge is power, it's the key to success,        |  Knowledge is power, it's the key to success,      \nWithout it, you're lost, like a ship without a      |  Without it, you're lost, like a ship without a    \ncompass,  It's the foundation of our society, The   |  compass,  It's the foundation of our society, The \nbasis of our culture, our history, It's the reason  |  basis of our culture, our history, It's the reason\nwe've come so far, And the reason we'll continue    |  we've come so far, And the reason we'll continue  \nto go even further,  So don't take it for granted,  |  to go even further,  So don't take it for granted,\ndon't let it slip away, Educate yourself, every     |  don't let it slip away, Educate yourself, every   \nsingle day, The more you learn, the more you'll     |  single day, The more you learn, the more you'll   \ngrow, And the more you'll be able to show,  That    |  grow, And the more you'll be able to show,  That  \nyou're a force to be reckoned with, A leader in     |  you're a force to be reckoned with, A leader in   \nyour own right, with a gift, Of knowledge and       |  your own right, with a gift, Of knowledge and     \nwisdom, that you'll pass on, To the next            |  wisdom, that you'll pass on, To the next          \ngeneration, and keep the cycle strong,  So don't    |  generation, and keep the cycle strong,  So don't  \nbe afraid to ask questions, to seek the truth, To   |  be afraid to ask questions, to seek the truth, To \nlearn from your mistakes, and to be your own        |  learn from your mistakes, and to be your own      \nproof, That education is the key to a better life,  |  proof, That education is the key to a better life,\nAnd the only way to reach new heights,  So keep on  |  And the only way to reach new heights,  So keep on\nlearning, keep on growing, And you'll be            |  learning, keep on growing, And you'll be          \nunstoppable, like a mighty flowing, River that      |  unstoppable, like a mighty flowing, River that    \ncan't be stopped, that won't be tamed, And you'll   |  can't be stopped, that won't be tamed, And you'll \nbe the one to make a difference, and leave your     |  leave your mark, like a trail of fame.            \nmark,  On the world, and show them what you're all  |                                                    \nabout, And that learning is the most important      |                                                    \nthing, That you can do, so don't waste another      |                                                    \nminute, And start learning, and start winning,      |                                                    \nThat's the rap verse about the importance of        |                                                    \nlearning, So listen up, and start earning,          |                                                    \nKnowledge is power, and it's yours for the taking,  |                                                    \nSo don't wait, start learning, and start making.    |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T14:21:16.960604Z",
          "iopub.execute_input": "2024-12-01T14:21:16.961443Z",
          "iopub.status.idle": "2024-12-01T14:21:38.038230Z",
          "shell.execute_reply.started": "2024-12-01T14:21:16.961410Z",
          "shell.execute_reply": "2024-12-01T14:21:38.037316Z"
        },
        "outputId": "d600475f-b659-44ce-ff44-f1baab0a8ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Write a    |  completes the request.  ### Instruction: Write a  \nfunction to find the maximum element of a given     |  function to find the maximum element of a given   \narray.  ### Input: int array[] = {8, 3, 5, 9, 3,    |  array.  ### Input: int array[] = {8, 3, 5, 9, 3,  \n1, 5};  ### Response: Here is a function that can   |  1, 5};  ### Response: int maxElement(int array[]) \nfind the maximum element of a given array:          |  {     int max = array[0];     for (int i = 1; i < \n```python def find_max(array):     max_element =    |  array.length; i++) {         if (array[i] > max) {\narray[0]     for element in array:         if       |  max = array[i];         }     }     return max; } \nelement > max_element:             max_element =    |                                                    \nelement     return max_element ```  To use this     |                                                    \nfunction with the given input, you can call it      |                                                    \nlike this:  ```python array = [8, 3, 5, 9, 3, 1,    |                                                    \n5] max_element = find_max(array)                    |                                                    \nprint(max_element) ```  This will output `9`,       |                                                    \nwhich is the maximum element in the given array.    |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T14:21:48.540850Z",
          "iopub.execute_input": "2024-12-01T14:21:48.541556Z",
          "iopub.status.idle": "2024-12-01T14:22:33.915191Z",
          "shell.execute_reply.started": "2024-12-01T14:21:48.541522Z",
          "shell.execute_reply": "2024-12-01T14:22:33.914199Z"
        },
        "outputId": "9d7c0894-536e-4075-f781-8d7fdd661512"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: List four  |  completes the request.  ### Instruction: List four\nmeasures to prevent the spread of COVID-19.  ###    |  measures to prevent the spread of COVID-19.  ###  \nInput:   ### Response:  1. Wear a mask: Wearing a   |  Input:   ### Response:  1. Wear a mask: Wearing a \nmask is one of the most effective ways to prevent   |  mask is one of the most effective ways to prevent \nthe spread of COVID-19. It helps to prevent         |  the spread of COVID-19. It helps to prevent       \nrespiratory droplets from being expelled into the   |  respiratory droplets from being expelled into the \nair and onto surfaces, reducing the risk of         |  air and onto surfaces, reducing the risk of       \ntransmission.  2. Practice social distancing:       |  transmission.  2. Practice social distancing:     \nMaintaining a distance of at least 6 feet from      |  Maintaining a distance of at least 6 feet from    \nothers can help to reduce the risk of               |  others can help to reduce the risk of             \ntransmission. This is especially important in       |  transmission. This is especially important in     \ncrowded places or during large gatherings.  3.      |  crowded places or during large gatherings.  3.    \nWash your hands frequently: Washing your hands      |  Wash your hands frequently: Washing your hands    \nwith soap and water for at least 20 seconds can     |  with soap and water for at least 20 seconds can   \nhelp to remove any viruses that may be on your      |  help to remove any viruses that may be on your    \nhands. If soap and water are not available, using   |  hands, reducing the risk of transmission. If soap \nhand sanitizer with at least 60% alcohol can also   |  and water are not available, using hand sanitizer \nbe effective.  4. Avoid close contact: Avoiding     |  with at least 60% alcohol can also be effective.  \nclose contact with people who are sick or have      |  4. Avoid close contact: Avoiding close contact    \nsymptoms of COVID-19 can help to prevent the        |  with people who are sick or have symptoms of      \nspread of the virus. Additionally, it is            |  COVID-19 can help to prevent the spread of the    \nrecommended to avoid touching your face,            |  virus. Additionally, avoiding large gatherings and\nespecially your eyes, nose, and mouth, as this can  |  crowded places can also help to reduce the risk of\nincrease the risk of transmission.                  |  transmission.                                     \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['instruction']\n",
        "inputs = dataset[loc]['input']\n",
        "pre_text = pre_assistant(prompt, inputs)\n",
        "post_text = post_assistant(prompt, inputs)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-01T14:41:03.657514Z",
          "iopub.execute_input": "2024-12-01T14:41:03.657857Z",
          "iopub.status.idle": "2024-12-01T14:41:20.364522Z",
          "shell.execute_reply.started": "2024-12-01T14:41:03.657829Z",
          "shell.execute_reply": "2024-12-01T14:41:20.363625Z"
        },
        "outputId": "87ed4747-8a36-42ec-d358-06d3b220e5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: How does   |  completes the request.  ### Instruction: How does \nfriction help us walk?  ### Input:   ### Response:  |  friction help us walk?  ### Input:   ### Response:\nFriction is a force that opposes motion between     |  Friction is a force that opposes motion between   \ntwo surfaces in contact. When we walk, our feet     |  two surfaces in contact. When we walk, our feet   \npush against the ground, and the ground pushes      |  push against the ground, and the ground pushes    \nback with an equal and opposite force. This force   |  back with an equal and opposite force. This force \nis friction, which helps us to maintain our         |  is friction, which helps us to maintain our       \nbalance and move forward without slipping. Without  |  balance and move forward. Without friction, our   \nfriction, it would be difficult to walk, as our     |  feet would slip out from under us, and we would   \nfeet would slide out from under us with every       |  not be able to walk.                              \nstep.                                               |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}