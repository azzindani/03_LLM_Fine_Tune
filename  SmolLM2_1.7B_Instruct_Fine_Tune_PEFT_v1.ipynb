{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/%20SmolLM2_1.7B_Instruct_Fine_Tune_PEFT_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:08:28.456065Z",
          "iopub.execute_input": "2024-11-16T23:08:28.456368Z",
          "iopub.status.idle": "2024-11-16T23:09:33.387161Z",
          "shell.execute_reply.started": "2024-11-16T23:08:28.456334Z",
          "shell.execute_reply": "2024-11-16T23:09:33.385619Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-11-16T23:09:33.389236Z",
          "iopub.execute_input": "2024-11-16T23:09:33.389590Z",
          "iopub.status.idle": "2024-11-16T23:09:54.097425Z",
          "shell.execute_reply.started": "2024-11-16T23:09:33.389554Z",
          "shell.execute_reply": "2024-11-16T23:09:54.096419Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-11-16T23:09:54.098730Z",
          "iopub.execute_input": "2024-11-16T23:09:54.099065Z",
          "iopub.status.idle": "2024-11-16T23:09:54.106496Z",
          "shell.execute_reply.started": "2024-11-16T23:09:54.099000Z",
          "shell.execute_reply": "2024-11-16T23:09:54.105397Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n",
        "#model_name = url.split('.co/')[-1]\n",
        "\n",
        "model_name = 'HuggingFaceTB/SmolLM2-1.7B-Instruct'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:09:54.108551Z",
          "iopub.execute_input": "2024-11-16T23:09:54.108844Z",
          "iopub.status.idle": "2024-11-16T23:09:54.119616Z",
          "shell.execute_reply.started": "2024-11-16T23:09:54.108807Z",
          "shell.execute_reply": "2024-11-16T23:09:54.118802Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "  if base == True:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype = torch.float16,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "  else:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = True,\n",
        "      bnb_4bit_quant_type = 'nf4',\n",
        "      bnb_4bit_compute_dtype = torch.float16,\n",
        "      bnb_4bit_use_double_quant = True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config = bnb_config,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:09:54.120931Z",
          "iopub.execute_input": "2024-11-16T23:09:54.121304Z",
          "iopub.status.idle": "2024-11-16T23:09:54.130426Z",
          "shell.execute_reply.started": "2024-11-16T23:09:54.121265Z",
          "shell.execute_reply": "2024-11-16T23:09:54.129133Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:09:54.132974Z",
          "iopub.execute_input": "2024-11-16T23:09:54.133987Z",
          "iopub.status.idle": "2024-11-16T23:11:20.123020Z",
          "shell.execute_reply.started": "2024-11-16T23:09:54.133954Z",
          "shell.execute_reply": "2024-11-16T23:11:20.122127Z"
        },
        "outputId": "95c75f73-c619-4395-e5f5-fef91529c2d6",
        "colab": {
          "referenced_widgets": [
            "2bed0a8c640345b8b132cbeb5cfbb3ea",
            "889da7701359418497b6fcb74ad50997",
            "fecf570036ae4d28b6f90209332edb24"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bed0a8c640345b8b132cbeb5cfbb3ea"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/3.42G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "889da7701359418497b6fcb74ad50997"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fecf570036ae4d28b6f90209332edb24"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:20.124412Z",
          "iopub.execute_input": "2024-11-16T23:11:20.124807Z",
          "iopub.status.idle": "2024-11-16T23:11:20.134328Z",
          "shell.execute_reply.started": "2024-11-16T23:11:20.124762Z",
          "shell.execute_reply": "2024-11-16T23:11:20.133399Z"
        },
        "outputId": "5dfd0948-0847-4a7c-d4b8-02f3ead2e382"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 906070016\nTrainable parameters : 100763648\nTrainable percentage: 11.12%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:20.135559Z",
          "iopub.execute_input": "2024-11-16T23:11:20.136427Z",
          "iopub.status.idle": "2024-11-16T23:11:22.059427Z",
          "shell.execute_reply.started": "2024-11-16T23:11:20.136384Z",
          "shell.execute_reply": "2024-11-16T23:11:22.058243Z"
        },
        "outputId": "89329e3e-5961-40f8-c0f2-3503e38ba4a3",
        "colab": {
          "referenced_widgets": [
            "e52d68b808ff416fae935536631348fc",
            "f0e60d70d64b458fa7098a3c567fd112",
            "7f06d2252de4420c9f8273b134063a7a",
            "3980f1e807614613bfc69347c4684bda",
            "947b8151f2a64ba4af8309acea209971"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e52d68b808ff416fae935536631348fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0e60d70d64b458fa7098a3c567fd112"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f06d2252de4420c9f8273b134063a7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3980f1e807614613bfc69347c4684bda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "947b8151f2a64ba4af8309acea209971"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n",
        "#dataset_name = url.split('datasets/')[-1]\n",
        "\n",
        "dataset_name = 'mlabonne/FineTome-100k'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:22.060943Z",
          "iopub.execute_input": "2024-11-16T23:11:22.061411Z",
          "iopub.status.idle": "2024-11-16T23:11:22.066319Z",
          "shell.execute_reply.started": "2024-11-16T23:11:22.061356Z",
          "shell.execute_reply": "2024-11-16T23:11:22.065497Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1024"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:22.071821Z",
          "iopub.execute_input": "2024-11-16T23:11:22.072241Z",
          "iopub.status.idle": "2024-11-16T23:11:24.786113Z",
          "shell.execute_reply.started": "2024-11-16T23:11:22.072195Z",
          "shell.execute_reply": "2024-11-16T23:11:24.784992Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:24.787608Z",
          "iopub.execute_input": "2024-11-16T23:11:24.788040Z",
          "iopub.status.idle": "2024-11-16T23:11:30.487069Z",
          "shell.execute_reply.started": "2024-11-16T23:11:24.787979Z",
          "shell.execute_reply": "2024-11-16T23:11:30.486223Z"
        },
        "outputId": "b40a9f3c-dbc7-4266-c977-359443db88e3",
        "colab": {
          "referenced_widgets": [
            "b50806d43c7c45a59742a1dc43c73296",
            "8d52b9b685ca42769ce708c2f0e7a501",
            "7969743989ba4d18bad002462a8513b1"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/982 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b50806d43c7c45a59742a1dc43c73296"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d52b9b685ca42769ce708c2f0e7a501"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7969743989ba4d18bad002462a8513b1"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['conversations', 'source', 'score'],\n    num_rows: 100000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(10000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:30.488346Z",
          "iopub.execute_input": "2024-11-16T23:11:30.488980Z",
          "iopub.status.idle": "2024-11-16T23:11:30.495983Z",
          "shell.execute_reply.started": "2024-11-16T23:11:30.488935Z",
          "shell.execute_reply": "2024-11-16T23:11:30.495122Z"
        },
        "id": "HXy5_vITTKP2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:30.497216Z",
          "iopub.execute_input": "2024-11-16T23:11:30.497675Z",
          "iopub.status.idle": "2024-11-16T23:11:30.532768Z",
          "shell.execute_reply.started": "2024-11-16T23:11:30.497633Z",
          "shell.execute_reply": "2024-11-16T23:11:30.531910Z"
        },
        "outputId": "27d5ebcc-c6a6-48a3-f776-07b5a691d7ba"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                       conversations  \\\n0  [{'from': 'human', 'value': 'Explain what bool...   \n1  [{'from': 'human', 'value': 'Explain how recur...   \n2  [{'from': 'human', 'value': 'Explain what bool...   \n3  [{'from': 'human', 'value': 'Explain the conce...   \n4  [{'from': 'human', 'value': 'Print the reverse...   \n\n                     source     score  \n0  infini-instruct-top-500k  5.212621  \n1  infini-instruct-top-500k  5.157649  \n2  infini-instruct-top-500k  5.147540  \n3  infini-instruct-top-500k  5.053656  \n4  infini-instruct-top-500k  5.045648  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conversations</th>\n      <th>source</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.212621</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[{'from': 'human', 'value': 'Explain how recur...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.157649</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.147540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[{'from': 'human', 'value': 'Explain the conce...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.053656</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[{'from': 'human', 'value': 'Print the reverse...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.045648</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:30.533793Z",
          "iopub.execute_input": "2024-11-16T23:11:30.534080Z",
          "iopub.status.idle": "2024-11-16T23:11:30.542900Z",
          "shell.execute_reply.started": "2024-11-16T23:11:30.534044Z",
          "shell.execute_reply": "2024-11-16T23:11:30.541976Z"
        },
        "outputId": "4b0572ac-3b58-490a-a21b-d05e45f86621"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'conversations': [{'from': 'human',\n   'value': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.'},\n  {'from': 'gpt',\n   'value': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.'}],\n 'source': 'infini-instruct-top-500k',\n 'score': 5.212620735168457}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:30.544085Z",
          "iopub.execute_input": "2024-11-16T23:11:30.544373Z",
          "iopub.status.idle": "2024-11-16T23:11:30.551088Z",
          "shell.execute_reply.started": "2024-11-16T23:11:30.544343Z",
          "shell.execute_reply": "2024-11-16T23:11:30.550055Z"
        },
        "outputId": "40b15edd-4154-4ec0-edf2-31d29ec49da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['conversations', 'source', 'score']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_conversations(example):\n",
        "  role_map = {\n",
        "    'human' : 'user',\n",
        "    'gpt' : 'assistant'\n",
        "  }\n",
        "\n",
        "  transformed_conversations = [\n",
        "    {\n",
        "      'role' : role_map.get(turn['from'], turn['from']),\n",
        "      'content' : turn['value']\n",
        "    }\n",
        "    for turn in example['conversations']\n",
        "  ]\n",
        "  return {'conversations': transformed_conversations}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:30.552443Z",
          "iopub.execute_input": "2024-11-16T23:11:30.552744Z",
          "iopub.status.idle": "2024-11-16T23:11:30.561315Z",
          "shell.execute_reply.started": "2024-11-16T23:11:30.552711Z",
          "shell.execute_reply": "2024-11-16T23:11:30.560455Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(transform_conversations, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:30.562474Z",
          "iopub.execute_input": "2024-11-16T23:11:30.562789Z",
          "iopub.status.idle": "2024-11-16T23:11:31.645858Z",
          "shell.execute_reply.started": "2024-11-16T23:11:30.562758Z",
          "shell.execute_reply": "2024-11-16T23:11:31.644926Z"
        },
        "outputId": "bd76582a-910b-4257-9f5e-db13346455d0",
        "colab": {
          "referenced_widgets": [
            "2f68e60f2b084a989851fedfe90af99c"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f68e60f2b084a989851fedfe90af99c"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['conversations'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['conversations'])"
      ],
      "metadata": {
        "id": "cZya4tPEWUc9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:31.647054Z",
          "iopub.execute_input": "2024-11-16T23:11:31.647359Z",
          "iopub.status.idle": "2024-11-16T23:11:31.654439Z",
          "shell.execute_reply.started": "2024-11-16T23:11:31.647327Z",
          "shell.execute_reply": "2024-11-16T23:11:31.651666Z"
        },
        "outputId": "b0a267d2-246d-4f5c-8b79-5ccca356d3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[{'content': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.', 'role': 'user'}, {'content': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.', 'role': 'assistant'}]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_conversation(example):\n",
        "  for entry in example['conversations']:\n",
        "    role = entry['role']\n",
        "    content = entry['content']\n",
        "\n",
        "    if role == 'user':\n",
        "      formatted_text = f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n",
        "    elif role == 'assistant':\n",
        "      formatted_text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n",
        "\n",
        "  return {'prompt': formatted_text}"
      ],
      "metadata": {
        "id": "3bmXGueQWWzt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:31.655885Z",
          "iopub.execute_input": "2024-11-16T23:11:31.656183Z",
          "iopub.status.idle": "2024-11-16T23:11:31.695292Z",
          "shell.execute_reply.started": "2024-11-16T23:11:31.656153Z",
          "shell.execute_reply": "2024-11-16T23:11:31.694476Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = formatted_dataset.map(\n",
        "  format_conversation,\n",
        "  remove_columns = list(formatted_dataset.features.keys())\n",
        ")\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "Z6sSaCr5eaL7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:31.696304Z",
          "iopub.execute_input": "2024-11-16T23:11:31.696593Z",
          "iopub.status.idle": "2024-11-16T23:11:32.530139Z",
          "shell.execute_reply.started": "2024-11-16T23:11:31.696563Z",
          "shell.execute_reply": "2024-11-16T23:11:32.529003Z"
        },
        "outputId": "c2f8a89d-f16c-4233-f9b4-5e7dbf3635ea",
        "colab": {
          "referenced_widgets": [
            "37b2cd5336a2480287616d29d9960a68"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37b2cd5336a2480287616d29d9960a68"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:32.531524Z",
          "iopub.execute_input": "2024-11-16T23:11:32.532236Z",
          "iopub.status.idle": "2024-11-16T23:11:32.537486Z",
          "shell.execute_reply.started": "2024-11-16T23:11:32.532190Z",
          "shell.execute_reply": "2024-11-16T23:11:32.536562Z"
        },
        "outputId": "86f50e55-c2f1-4317-8e70-963c0b1df746"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:32.538704Z",
          "iopub.execute_input": "2024-11-16T23:11:32.538986Z",
          "iopub.status.idle": "2024-11-16T23:11:32.570640Z",
          "shell.execute_reply.started": "2024-11-16T23:11:32.538956Z",
          "shell.execute_reply": "2024-11-16T23:11:32.569930Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:32.571700Z",
          "iopub.execute_input": "2024-11-16T23:11:32.572028Z",
          "iopub.status.idle": "2024-11-16T23:11:42.475613Z",
          "shell.execute_reply.started": "2024-11-16T23:11:32.571984Z",
          "shell.execute_reply": "2024-11-16T23:11:42.474700Z"
        },
        "outputId": "77ad3aa1-3de1-4d91-db4e-6e488bd322f8",
        "colab": {
          "referenced_widgets": [
            "ea704cf329c94f65823793f83abbef67"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea704cf329c94f65823793f83abbef67"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.476769Z",
          "iopub.execute_input": "2024-11-16T23:11:42.477072Z",
          "iopub.status.idle": "2024-11-16T23:11:42.483728Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.477029Z",
          "shell.execute_reply": "2024-11-16T23:11:42.482710Z"
        },
        "outputId": "82e088a8-fb98-4870-b456-5d39fb47d491"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.484841Z",
          "iopub.execute_input": "2024-11-16T23:11:42.485151Z",
          "iopub.status.idle": "2024-11-16T23:11:42.712030Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.485119Z",
          "shell.execute_reply": "2024-11-16T23:11:42.711151Z"
        },
        "outputId": "28fac3bf-b149-47c7-9740-acbfcd332505"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.713124Z",
          "iopub.execute_input": "2024-11-16T23:11:42.713414Z",
          "iopub.status.idle": "2024-11-16T23:11:42.719295Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.713383Z",
          "shell.execute_reply": "2024-11-16T23:11:42.718388Z"
        },
        "outputId": "4ccc5aa5-3960-4e55-e13e-90b08983ef6f"
      },
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.720435Z",
          "iopub.execute_input": "2024-11-16T23:11:42.720716Z",
          "iopub.status.idle": "2024-11-16T23:11:42.749968Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.720686Z",
          "shell.execute_reply": "2024-11-16T23:11:42.749003Z"
        },
        "outputId": "4ca9451f-de7c-4383-87f5-7e288f63edfa"
      },
      "outputs": [
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n1  <|start_header_id|>user<|end_header_id|>\\n\\nWr...   \n2  <|start_header_id|>user<|end_header_id|>\\n\\nWh...   \n3  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n4  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n\n                                           input_ids  \\\n0  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n1  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n2  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n3  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n4  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWr...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.760619Z",
          "iopub.execute_input": "2024-11-16T23:11:42.760995Z",
          "iopub.status.idle": "2024-11-16T23:11:42.769003Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.760953Z",
          "shell.execute_reply": "2024-11-16T23:11:42.767873Z"
        },
        "outputId": "ba5619cd-016c-4b1e-91ff-14addb93bedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nHow do you graph the inequality #x > -4#?\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nTo graph the inequality #x > -4#, follow these steps:\n\n1. Start by plotting the boundary line: Draw a vertical line at #x = -4# on the X-axis. This line divides the X-axis into two halves.\n\n2. Identify the direction of the inequality: Since the inequality is #x > -4#, it indicates that we are looking for all values of #x# that are greater than -4.\n\n3. Shade the appropriate region: Shade the entire region to the right of the line at #x = -4#. This shaded area represents all the points whose X-coordinate is greater than -4.\n\n4. Include a dashed line: Since the inequality is \"greater than\" (#>,# not \"##\"), draw the line at #x = -4# as a dashed line, indicating that the points on the line itself are not included in the solution.\n\nThe graph consists of the entire half-plane to the right of the dashed vertical line at #x = -4#, excluding the line itself.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.770532Z",
          "iopub.execute_input": "2024-11-16T23:11:42.771060Z",
          "iopub.status.idle": "2024-11-16T23:11:42.782395Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.770958Z",
          "shell.execute_reply": "2024-11-16T23:11:42.779777Z"
        },
        "outputId": "071082ca-af84-4590-bbbe-7855f67b4a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4093, 44, 108, 486, 79, 12476, 79, 311, 108, 46, 198, 198, 2020, 536, 346, 3670, 260, 9724, 907, 104, 2986, 731, 36, 19, 47, 198, 44, 108, 85, 320, 79, 311, 108, 21198, 108, 3738, 79, 12476, 79, 311, 108, 46, 520, 9531, 44, 108, 486, 79, 12476, 79, 311, 108, 46, 198, 198, 2068, 3670, 260, 9724, 907, 104, 2986, 731, 36, 19, 28, 1066, 623, 3301, 42, 1116, 33, 30, 7734, 411, 27566, 260, 10651, 1761, 42, 10602, 253, 8681, 1761, 418, 907, 104, 446, 731, 36, 19, 335, 260, 2273, 29, 7814, 30, 669, 1761, 20751, 260, 2273, 29, 7814, 618, 827, 32386, 30, 1116, 34, 30, 12716, 260, 4376, 282, 260, 9724, 42, 4311, 260, 9724, 314, 907, 104, 2986, 731, 36, 19, 28, 357, 6773, 338, 392, 359, 3012, 327, 511, 2396, 282, 907, 104, 19, 338, 359, 2852, 670, 731, 36, 30, 1116, 35, 30, 1541, 903, 260, 3350, 2695, 42, 1541, 903, 260, 2415, 2695, 288, 260, 1048, 282, 260, 1761, 418, 907, 104, 446, 731, 36, 19, 30, 669, 32416, 1557, 4669, 511, 260, 2876, 3449, 2273, 29, 38263, 314, 2852, 670, 731, 36, 30, 1116, 36, 30, 26852, 253, 44613, 1761, 42, 4311, 260, 9724, 314, 476, 41773, 670, 18, 38078, 18128, 19, 441, 28096, 40533, 115, 19, 6872, 2634, 260, 1761, 418, 907, 104, 446, 731, 36, 19, 347, 253, 44613, 1761, 28, 10487, 338, 260, 2876, 335, 260, 1761, 2581, 359, 441, 3249, 281, 260, 3564, 30, 198, 198, 504, 3670, 5956, 282, 260, 2415, 2745, 29, 12758, 288, 260, 1048, 282, 260, 44613, 8681, 1761, 418, 907, 104, 446, 731, 36, 19, 28, 26827, 260, 1761, 2581, 30, 198, 44, 108, 85, 320, 79, 311, 108, 46, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.783811Z",
          "iopub.execute_input": "2024-11-16T23:11:42.784295Z",
          "iopub.status.idle": "2024-11-16T23:11:42.796450Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.784215Z",
          "shell.execute_reply": "2024-11-16T23:11:42.795336Z"
        },
        "outputId": "5acd1c83-fdc6-49e6-de65-cebc49b768b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.798199Z",
          "iopub.execute_input": "2024-11-16T23:11:42.798548Z",
          "iopub.status.idle": "2024-11-16T23:11:42.803311Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.798508Z",
          "shell.execute_reply": "2024-11-16T23:11:42.802059Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.804286Z",
          "iopub.execute_input": "2024-11-16T23:11:42.804632Z",
          "iopub.status.idle": "2024-11-16T23:11:42.811808Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.804592Z",
          "shell.execute_reply": "2024-11-16T23:11:42.810896Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.812742Z",
          "iopub.execute_input": "2024-11-16T23:11:42.813117Z",
          "iopub.status.idle": "2024-11-16T23:11:42.821143Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.813073Z",
          "shell.execute_reply": "2024-11-16T23:11:42.819857Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.822225Z",
          "iopub.execute_input": "2024-11-16T23:11:42.822814Z",
          "iopub.status.idle": "2024-11-16T23:11:42.831560Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.822771Z",
          "shell.execute_reply": "2024-11-16T23:11:42.830675Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:42.832695Z",
          "iopub.execute_input": "2024-11-16T23:11:42.833107Z",
          "iopub.status.idle": "2024-11-16T23:11:44.186807Z",
          "shell.execute_reply.started": "2024-11-16T23:11:42.833075Z",
          "shell.execute_reply": "2024-11-16T23:11:44.185834Z"
        },
        "outputId": "68f097e6-cde0-410b-a6ba-fe36da36ccce"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 72,351,744 || all params: 1,783,728,128 || trainable%: 4.0562\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ikF6Yfkz1myd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:44.188556Z",
          "iopub.execute_input": "2024-11-16T23:11:44.188961Z",
          "iopub.status.idle": "2024-11-16T23:11:44.207723Z",
          "shell.execute_reply.started": "2024-11-16T23:11:44.188917Z",
          "shell.execute_reply": "2024-11-16T23:11:44.206638Z"
        },
        "outputId": "4ec04428-958c-4a5a-cde7-4b3f91259a85"
      },
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=8192, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:44.208917Z",
          "iopub.execute_input": "2024-11-16T23:11:44.209319Z",
          "iopub.status.idle": "2024-11-16T23:11:44.232990Z",
          "shell.execute_reply.started": "2024-11-16T23:11:44.209275Z",
          "shell.execute_reply": "2024-11-16T23:11:44.232176Z"
        },
        "outputId": "9f8695d3-807d-4bda-91b1-3746aa874983"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 978421760\nTrainable parameters : 72351744\nTrainable percentage: 7.39%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:44.234064Z",
          "iopub.execute_input": "2024-11-16T23:11:44.234429Z",
          "iopub.status.idle": "2024-11-16T23:11:44.241732Z",
          "shell.execute_reply.started": "2024-11-16T23:11:44.234391Z",
          "shell.execute_reply": "2024-11-16T23:11:44.240794Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 2\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:44.242896Z",
          "iopub.execute_input": "2024-11-16T23:11:44.243229Z",
          "iopub.status.idle": "2024-11-16T23:11:44.282833Z",
          "shell.execute_reply.started": "2024-11-16T23:11:44.243194Z",
          "shell.execute_reply": "2024-11-16T23:11:44.281782Z"
        },
        "outputId": "e533c702-dbde-45fc-de3e-fda66aef2b31"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 39,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Nov16_23-11-44_84769eed1484,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:44.284409Z",
          "iopub.execute_input": "2024-11-16T23:11:44.284800Z",
          "iopub.status.idle": "2024-11-16T23:11:46.099927Z",
          "shell.execute_reply.started": "2024-11-16T23:11:44.284756Z",
          "shell.execute_reply": "2024-11-16T23:11:46.099090Z"
        },
        "outputId": "652ff1b8-b30c-4e12-ba86-84df0b606cd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 40,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7acbe80e32e0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T23:11:46.101911Z",
          "iopub.execute_input": "2024-11-16T23:11:46.102667Z",
          "iopub.status.idle": "2024-11-17T00:16:28.906451Z",
          "shell.execute_reply.started": "2024-11-16T23:11:46.102631Z",
          "shell.execute_reply": "2024-11-17T00:16:28.905571Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:16:28.907746Z",
          "iopub.execute_input": "2024-11-17T00:16:28.908444Z",
          "iopub.status.idle": "2024-11-17T00:18:35.923609Z",
          "shell.execute_reply.started": "2024-11-17T00:16:28.908398Z",
          "shell.execute_reply": "2024-11-17T00:18:35.922694Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:35.924738Z",
          "iopub.execute_input": "2024-11-17T00:18:35.925067Z",
          "iopub.status.idle": "2024-11-17T00:18:37.206147Z",
          "shell.execute_reply.started": "2024-11-17T00:18:35.925005Z",
          "shell.execute_reply": "2024-11-17T00:18:37.205318Z"
        },
        "outputId": "b532a729-17f4-43f6-daca-e534128d6591"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/d1bb90bcfbe0f211109880f4da18da66f229c4f6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 130000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/d1bb90bcfbe0f211109880f4da18da66f229c4f6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 130000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:37.207237Z",
          "iopub.execute_input": "2024-11-17T00:18:37.207521Z",
          "iopub.status.idle": "2024-11-17T00:18:37.662342Z",
          "shell.execute_reply.started": "2024-11-17T00:18:37.207483Z",
          "shell.execute_reply": "2024-11-17T00:18:37.661293Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:37.663675Z",
          "iopub.execute_input": "2024-11-17T00:18:37.664001Z",
          "iopub.status.idle": "2024-11-17T00:18:37.673873Z",
          "shell.execute_reply.started": "2024-11-17T00:18:37.663969Z",
          "shell.execute_reply": "2024-11-17T00:18:37.673052Z"
        },
        "id": "SW7c-1ymTKP7",
        "outputId": "5731a7c3-792c-45b6-97a0-e6446207d53e"
      },
      "outputs": [
        {
          "execution_count": 45,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:37.674948Z",
          "iopub.execute_input": "2024-11-17T00:18:37.675282Z",
          "iopub.status.idle": "2024-11-17T00:18:38.857493Z",
          "shell.execute_reply.started": "2024-11-17T00:18:37.675251Z",
          "shell.execute_reply": "2024-11-17T00:18:38.856726Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "bw6vA6dpTKP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:38.858738Z",
          "iopub.execute_input": "2024-11-17T00:18:38.859441Z",
          "iopub.status.idle": "2024-11-17T00:18:43.033024Z",
          "shell.execute_reply.started": "2024-11-17T00:18:38.859395Z",
          "shell.execute_reply": "2024-11-17T00:18:43.031889Z"
        },
        "id": "b0vZG3xBTKP7",
        "outputId": "3474bdae-1c42-4f74-e0a1-7c451471917e"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/d1bb90bcfbe0f211109880f4da18da66f229c4f6/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 130000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/d1bb90bcfbe0f211109880f4da18da66f229c4f6/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-1.7B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/d1bb90bcfbe0f211109880f4da18da66f229c4f6/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\n",
          "output_type": "stream"
        },
        {
          "execution_count": 47,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:43.034378Z",
          "iopub.execute_input": "2024-11-17T00:18:43.034727Z",
          "iopub.status.idle": "2024-11-17T00:18:43.047921Z",
          "shell.execute_reply.started": "2024-11-17T00:18:43.034689Z",
          "shell.execute_reply": "2024-11-17T00:18:43.046854Z"
        },
        "id": "TB8yij06TKP7",
        "outputId": "c2fc37ae-a0e9-4c80-f930-fb23834b9d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 906070016\nTrainable parameters : 100763648\nTrainable percentage: 11.12%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:43.049181Z",
          "iopub.execute_input": "2024-11-17T00:18:43.049473Z",
          "iopub.status.idle": "2024-11-17T00:18:43.075127Z",
          "shell.execute_reply.started": "2024-11-17T00:18:43.049442Z",
          "shell.execute_reply": "2024-11-17T00:18:43.073968Z"
        },
        "id": "48NL4aWaTKP7",
        "outputId": "cd0f9e23-23f7-42c2-b8f5-b366dcdc2e64"
      },
      "outputs": [
        {
          "execution_count": 49,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n        (layers): ModuleList(\n          (0-23): 24 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=8192, out_features=64, bias=False)\n                  (default): Linear(in_features=8192, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:43.076405Z",
          "iopub.execute_input": "2024-11-17T00:18:43.076744Z",
          "iopub.status.idle": "2024-11-17T00:18:43.103972Z",
          "shell.execute_reply.started": "2024-11-17T00:18:43.076710Z",
          "shell.execute_reply": "2024-11-17T00:18:43.103068Z"
        },
        "id": "tqx0NeMRTKP7",
        "outputId": "3c438fbf-8033-4590-9110-011462789092"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 1050773504\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt):\n",
        "  messages = [\n",
        "    {'role' : 'human', 'content' : prompt},\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = 'pt',\n",
        "    truncation = True,\n",
        "    padding = True,\n",
        "  ).to('cuda')\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    input_ids = inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:43.105491Z",
          "iopub.execute_input": "2024-11-17T00:18:43.106208Z",
          "iopub.status.idle": "2024-11-17T00:18:43.112998Z",
          "shell.execute_reply.started": "2024-11-17T00:18:43.106164Z",
          "shell.execute_reply": "2024-11-17T00:18:43.112126Z"
        },
        "id": "nVwEpiVETKP7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt):\n",
        "  messages = [\n",
        "    {'role' : 'human', 'content' : prompt},\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = 'pt',\n",
        "    truncation = True,\n",
        "    padding = True,\n",
        "  ).to('cuda')\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    input_ids = inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:43.114163Z",
          "iopub.execute_input": "2024-11-17T00:18:43.114490Z",
          "iopub.status.idle": "2024-11-17T00:18:43.122358Z",
          "shell.execute_reply.started": "2024-11-17T00:18:43.114459Z",
          "shell.execute_reply": "2024-11-17T00:18:43.121511Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:18:43.123476Z",
          "iopub.execute_input": "2024-11-17T00:18:43.125606Z",
          "iopub.status.idle": "2024-11-17T00:18:43.136582Z",
          "shell.execute_reply.started": "2024-11-17T00:18:43.125574Z",
          "shell.execute_reply": "2024-11-17T00:18:43.135660Z"
        },
        "id": "BxVz3eI1TKP7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:23:55.010702Z",
          "iopub.execute_input": "2024-11-17T00:23:55.011328Z",
          "iopub.status.idle": "2024-11-17T00:25:00.710320Z",
          "shell.execute_reply.started": "2024-11-17T00:23:55.011289Z",
          "shell.execute_reply": "2024-11-17T00:25:00.709433Z"
        },
        "outputId": "c9c1b38a-c491-4f45-fa42-2579dd511335"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human Explain how the order of          |  <|im_start|>human Explain how the order of        \noperations in BIDMAS ensures consistency in         |  operations in BIDMAS ensures consistency in       \nmathematical calculations.<|im_end|>                |  mathematical calculations.<|im_end|>              \n<|im_start|>assistant BIDMAS is an acronym that     |  <|im_start|>assistant BIDMAS is an acronym that   \nstands for Brackets, Indices, Division,             |  stands for Brackets, Indices, Division,           \nMultiplication, Addition, and Subtraction. It is a  |  Multiplication, Addition, and Subtraction. It is a\nset of rules that dictates the order in which       |  set of rules that dictates the order in which     \nmathematical operations should be performed. The    |  mathematical operations should be performed. The  \norder of operations ensures consistency in          |  order of operations ensures consistency in        \nmathematical calculations by providing a clear and  |  mathematical calculations by providing a clear and\npredictable sequence of steps.  Here's how BIDMAS   |  predictable sequence of steps.  Here's how BIDMAS \nworks:  1. Brackets (or parentheses): Any           |  works:  1. Brackets (or parentheses): Any         \ncalculations within brackets are performed first.   |  calculations within brackets are performed first. \nThis ensures that any calculations within the       |  This ensures that any calculations within the     \nbrackets are evaluated before moving on to the      |  brackets are evaluated before the rest of the     \nrest of the equation.  2. Indices (or powers): Any  |  calculation.  2. Indices (or powers): Any         \ncalculations involving indices (such as squaring    |  calculations involving indices (such as squaring  \nor cubing) are performed next. This ensures that    |  or cubing) are performed next. This ensures that  \nany calculations involving exponents are evaluated  |  any calculations involving powers are evaluated   \nbefore moving on to the rest of the equation.  3.   |  before the rest of the calculation.  3. Division  \nDivision and Multiplication: Any calculations       |  and Multiplication: Any calculations involving    \ninvolving division or multiplication are performed  |  division or multiplication are performed next.    \nnext. This ensures that any calculations involving  |  These operations are evaluated from left to right.\nmultiplication or division are evaluated before     |  This ensures that any calculations involving      \nmoving on to the rest of the equation.  4.          |  division or multiplication are evaluated in a     \nAddition and Subtraction: Any calculations          |  consistent order.  4. Addition and Subtraction:   \ninvolving addition or subtraction are performed     |  Any calculations involving addition or subtraction\nlast. This ensures that any calculations involving  |  are performed last. These operations are evaluated\naddition or subtraction are evaluated before        |  from left to right. This ensures that any         \nmoving on to the rest of the equation.  By          |  calculations involving addition or subtraction are\nfollowing the order of operations, BIDMAS ensures   |  evaluated in a consistent order.  By following the\nconsistency in mathematical calculations. It        |  order of operations, BIDMAS ensures consistency in\nprevents any ambiguity or confusion that could      |  mathematical calculations. It prevents any        \narise from performing calculations in a different   |  ambiguity or confusion that could arise from      \norder. For example, consider the equation 3 + 4 x   |  performing calculations in a different order. This\n2. If we follow the order of operations, we first   |  ensures that mathematical calculations are        \nperform the multiplication (4 x 2 = 8), then add 3  |  performed in a predictable and reliable manner.   \n(8 + 3 = 11), and finally subtract 2 (11 - 2 = 9).  |  For example, consider the calculation 3 + 4 x 2.  \nIf we perform the addition and subtraction in a     |  If we follow the order of operations, we first    \ndifferent order, we get a different result (3 + 4   |  perform the multiplication (4 x 2 = 8), then we   \nx 2 = 11).  In summary, BIDMAS ensures consistency  |  perform the addition (3 + 8 = 11). If we follow   \nin mathematical calculations by providing a clear   |  the order of operations in a different order, we  \nand predictable sequence of steps. It prevents any  |  might perform the addition first (3 + 4 = 7), then\nambiguity or confusion that could arise from        |  we perform the multiplication (7 x 2 = 14). This  \nperforming calculations in a different order, and   |  would give a different result, which is not what  \nit helps to ensure that mathematical equations are  |  we intended.  In summary, BIDMAS ensures          \nevaluated correctly.<|im_end|>                      |  consistency in mathematical calculations by       \n                                                    |  providing a clear and predictable sequence of     \n                                                    |  steps. It prevents any ambiguity or confusion that\n                                                    |  could arise from performing calculations in a     \n                                                    |  different order. This ensures that mathematical   \n                                                    |  calculations are performed in a predictable and   \n                                                    |  reliable manner.<|im_end|>                        \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:28:16.234484Z",
          "iopub.execute_input": "2024-11-17T00:28:16.234877Z",
          "iopub.status.idle": "2024-11-17T00:29:27.407041Z",
          "shell.execute_reply.started": "2024-11-17T00:28:16.234841Z",
          "shell.execute_reply": "2024-11-17T00:29:27.406070Z"
        },
        "outputId": "d9cf79c5-3072-4e83-e658-ef0eed3ed00c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human Provide a comprehensive list of   |  <|im_start|>human Provide a comprehensive list of \nbenefits of integrating a focus on student          |  benefits of integrating a focus on student        \nquestioning into the school curriculum with a       |  questioning into the school curriculum with a     \nclear emphasis on the correlation between student   |  clear emphasis on the correlation between student \nquestion formulation and academic                   |  question formulation and academic                 \nsuccess.<|im_end|> <|im_start|>assistant 1.         |  success.<|im_end|> <|im_start|>assistant 1.       \nImproved Academic Performance: When students are    |  Improved Academic Performance: When students are  \nencouraged to ask questions, they are more likely   |  encouraged to ask questions, they are more likely \nto engage with the material and understand the      |  to engage with the material and understand the    \nconcepts being taught. This leads to better         |  concepts being taught. This leads to better       \nacademic performance and a deeper understanding of  |  academic performance and a deeper understanding of\nthe subject matter.  2. Enhanced Critical Thinking  |  the subject matter.  2. Enhanced Critical Thinking\nSkills: The ability to formulate questions is a     |  Skills: The ability to formulate questions is a   \nkey component of critical thinking. By encouraging  |  key component of critical thinking. By encouraging\nstudents to ask questions, you're helping them      |  students to ask questions, you're helping them    \ndevelop this skill, which is essential for success  |  develop this skill, which is essential for success\nin all areas of life.  3. Increased Student         |  in all areas of life.  3. Increased Student       \nEngagement: When students are encouraged to ask     |  Engagement: When students are encouraged to ask   \nquestions, they become more engaged in the          |  questions, they become more engaged in the        \nlearning process. This leads to a more positive     |  learning process. This leads to a more positive   \nand interactive classroom environment.  4. Better   |  and interactive classroom environment.  4. Better \nUnderstanding of Complex Concepts: By asking        |  Understanding of Complex Concepts: By asking      \nquestions, students are forced to think critically  |  questions, students are forced to think critically\nabout complex concepts and ideas. This can lead to  |  about complex concepts and ideas. This can lead to\na deeper understanding of the material and a more   |  a deeper understanding of the material and a more \ncomprehensive grasp of the subject.  5. Improved    |  comprehensive grasp of the subject.  5. Improved  \nCommunication Skills: The ability to articulate     |  Communication Skills: The ability to articulate   \nquestions and thoughts is a valuable skill that     |  questions and thoughts is a valuable skill that   \ncan benefit students throughout their lives. By     |  can benefit students throughout their lives. By   \nencouraging students to ask questions, you're       |  encouraging students to ask questions, you're     \nhelping them develop this skill.  6. Increased      |  helping them develop this skill.  6. Increased    \nConfidence: When students are encouraged to ask     |  Confidence: When students are encouraged to ask   \nquestions, they feel more comfortable and           |  questions, they feel more comfortable and         \nconfident in their ability to learn. This can lead  |  confident in their ability to learn. This can lead\nto a more positive and supportive learning          |  to a more positive and supportive learning        \nenvironment.  7. Better Preparation for the Real    |  environment.  7. Better Preparation for the Real  \nWorld: In the real world, people are often faced    |  World: In the real world, people are often faced  \nwith complex and unfamiliar situations. By          |  with complex and unfamiliar situations. By        \nlearning to ask questions, students are preparing   |  learning to ask questions, students are preparing \nthemselves for the challenges they will face in     |  themselves for the challenges they will face in   \nthe future.  8. Improved Relationships: When        |  the future.  8. Improved Collaboration: When      \nstudents are encouraged to ask questions, they are  |  students are encouraged to ask questions, they are\nmore likely to build positive relationships with    |  more likely to collaborate with their peers. This \ntheir teachers and peers. This can lead to a more   |  can lead to a more positive and supportive        \nsupportive and collaborative learning environment.  |  learning environment.  9. Enhanced Problem-Solving\n9. Development of a Growth Mindset: The ability to  |  Skills: The ability to formulate questions is a   \nask questions is closely tied to a growth mindset.  |  key component of problem-solving. By encouraging  \nBy encouraging students to ask questions, you're    |  students to ask questions, you're helping them    \nhelping them develop a growth mindset, which is     |  develop this skill, which is essential for success\nessential for success in all areas of life.  10.    |  in all areas of life.  10. Improved Time          \nImproved Learning Outcomes: When students are       |  Management: When students are encouraged to ask   \nencouraged to ask questions, they are more likely   |  questions, they are more likely to manage their   \nto achieve better learning outcomes. This is        |  time effectively. This can lead to a more positive\nbecause they are more engaged, more confident, and  |  and productive learning environment.  In          \nmore prepared to tackle complex concepts.  In       |  conclusion, integrating a focus on student        \nconclusion, integrating a focus on student          |  questioning into the school curriculum can have a \nquestioning into the school curriculum can have a   |  significant impact on academic success. By        \nsignificant impact on academic success. By          |  emphasizing the correlation between student       \nemphasizing the importance of questioning, you can  |  question formulation and academic success, you can\nhelp students develop critical thinking skills,     |  help students develop valuable skills that will   \nenhance their understanding of complex concepts,    |  benefit them throughout their lives.<|im_end|>    \nand improve their overall learning                  |                                                    \noutcomes.<|im_end|>                                 |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:20:06.314719Z",
          "iopub.execute_input": "2024-11-17T00:20:06.315033Z",
          "iopub.status.idle": "2024-11-17T00:20:44.448722Z",
          "shell.execute_reply.started": "2024-11-17T00:20:06.314982Z",
          "shell.execute_reply": "2024-11-17T00:20:44.447789Z"
        },
        "outputId": "b328d06d-1af6-4991-ce68-934f983ef38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human Solve the following math problem  |  <|im_start|>human Solve the following math problem\nstep-by-step. Simplify your answer as much as       |  step-by-step. Simplify your answer as much as     \npossible. Present your final answer as \\boxed{Your  |  possible. Present your final answer as \\boxed{Your\nAnswer}. if the height of a cone is increased by    |  Answer}. if the height of a cone is increased by  \n160 % then its volume is increased by ?<|im_end|>   |  160 % then its volume is increased by ?<|im_end|> \n<|im_start|>assistant Step 1: Understand the        |  <|im_start|>assistant Step 1: Understand the      \nproblem The problem states that the height of a     |  problem The problem states that the height of a   \ncone is increased by 160%, and we need to find the  |  cone is increased by 160%, and we need to find the\npercentage increase in the volume of the cone.      |  percentage increase in the volume of the cone.    \nStep 2: Use the formula for the volume of a cone    |  Step 2: Use the formula for the volume of a cone  \nThe volume of a cone is given by V = (1/3)rh,     |  The volume of a cone is given by V = (1/3)rh,   \nwhere r is the radius and h is the height.  Step    |  where r is the radius and h is the height.  Step  \n3: Calculate the percentage increase in volume      |  3: Calculate the increase in volume Let's assume  \nLet's assume the original height of the cone is h.  |  the original height of the cone is h. If the      \nIf the height is increased by 160%, the new height  |  height is increased by 160%, the new height       \nbecomes 1.6h.  Step 4: Calculate the percentage     |  becomes 1.6h.  Step 4: Calculate the increase in  \nincrease in volume The percentage increase in       |  volume The increase in volume is (1.6h - h) *     \nvolume is given by ((1.6h - h) / h) * 100%.  Step   |  (1/3)r.  Step 5: Simplify the expression The    \n5: Simplify the expression ((1.6h - h) / h) * 100%  |  increase in volume is (0.6h/3)r.  Step 6: Find  \n= (0.6h / h) * 100% = 0.6 * 100% = 60%.  So, the    |  the percentage increase The percentage increase in\nvolume of the cone is increased by 60%.<|im_end|>   |  volume is ((0.6h/3)r / (1/3)r) * 100% = 600%. \n                                                    |  So, the volume of the cone is increased by        \n                                                    |  600%.<|im_end|>                                   \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:20:44.450255Z",
          "iopub.execute_input": "2024-11-17T00:20:44.450914Z",
          "iopub.status.idle": "2024-11-17T00:21:52.295092Z",
          "shell.execute_reply.started": "2024-11-17T00:20:44.450868Z",
          "shell.execute_reply": "2024-11-17T00:21:52.294205Z"
        },
        "outputId": "decd2af2-6fd7-48e6-e2fc-ae36490a93f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human What were the social and          |  <|im_start|>human What were the social and        \neconomic impacts of the Industrial Revolution in    |  economic impacts of the Industrial Revolution in  \n19th century England?<|im_end|>                     |  19th century England?<|im_end|>                   \n<|im_start|>assistant The Industrial Revolution in  |  <|im_start|>assistant The Industrial Revolution in\n19th century England had significant social and     |  19th century England had significant social and   \neconomic impacts. Here are some of the key          |  economic impacts. Here are some of the key        \neffects:  1. Social Impacts:    - Urbanization:     |  effects:  1. Social Impacts:    - Urbanization:   \nThe Industrial Revolution led to rapid              |  The Industrial Revolution led to rapid            \nurbanization. Cities grew rapidly as people moved   |  urbanization. Cities grew rapidly as people moved \nfrom rural areas to work in factories. This led to  |  from rural areas to work in factories. This led to\novercrowding, poor living conditions, and the rise  |  overcrowding, poor living conditions, and the rise\nof slums.    - Social Class Changes: The            |  of slums.    - Social Class Changes: The          \nIndustrial Revolution saw a shift in the social     |  Industrial Revolution saw a shift in the social   \nhierarchy. Factory owners and managers became       |  hierarchy. Factory owners and managers became     \nwealthy and influential, while the working class    |  wealthy and influential, while the working class  \nbecame poorer and more marginalized. The middle     |  became poorer and more marginalized. The middle   \nclass, which included artisans and small business   |  class, which included artisans and small business \nowners, also grew.    - Changes in Family           |  owners, also grew.    - Changes in Family         \nStructure: The Industrial Revolution led to         |  Structure: The Industrial Revolution led to       \nchanges in family structure. With the rise of       |  changes in family structure. With the rise of     \nfactories, women and children were employed in      |  factories, women and children were employed in    \nfactories, changing the traditional family roles.   |  factories, changing the traditional family roles. \n- Rise of Unemployment: The Industrial Revolution   |  - Rise of Unemployment: The Industrial Revolution \nled to an increase in unemployment due to the       |  led to an increase in unemployment due to the     \nmechanization of jobs.  2. Economic Impacts:    -   |  mechanization of jobs.  2. Economic Impacts:    - \nEconomic Growth: The Industrial Revolution led to   |  Economic Growth: The Industrial Revolution led to \nrapid economic growth in England. New industries    |  rapid economic growth in England. New industries  \nwere created, and the country became a global       |  were created, and the country became a global     \neconomic power.    - Technological Advancements:    |  economic power.    - Technological Advancements:  \nThe Industrial Revolution saw significant           |  The Industrial Revolution saw significant         \ntechnological advancements, such as the steam       |  technological advancements, such as the steam     \nengine, spinning jenny, and power loom. These       |  engine, spinning jenny, and power loom. These     \ninventions increased productivity and efficiency.   |  inventions increased productivity and efficiency. \n- Rise of Capitalism: The Industrial Revolution     |  - Rise of Capitalism: The Industrial Revolution   \nsaw the rise of capitalism. New forms of business   |  saw the rise of capitalism. New forms of business \nand finance emerged, and the concept of private     |  and finance emerged, and the concept of private   \nownership became more prevalent.    - Changes in    |  ownership became more prevalent.    - Changes in  \nTrade: The Industrial Revolution led to changes in  |  Trade and Commerce: The Industrial Revolution led \ntrade. New markets were created, and international  |  to changes in trade and commerce. New markets were\ntrade increased.    - Emergence of New Industries:  |  created, and international trade increased.    -  \nThe Industrial Revolution saw the emergence of new  |  Emergence of New Industries: The Industrial       \nindustries, such as textiles, steel, and coal       |  Revolution saw the emergence of new industries,   \nmining.  These impacts were not uniform across all  |  such as textiles, steel, and coal mining.  These  \nregions of England. The Industrial Revolution had   |  impacts were not uniform across all regions of    \ndifferent effects in the North and South of         |  England. The Industrial Revolution had different  \nEngland, with the North experiencing more rapid     |  effects in the North and South of England, with   \nindustrialization and the South experiencing more   |  the North experiencing more rapid                 \nresistance to change.  The Industrial Revolution    |  industrialization and the South experiencing more \nalso had significant impacts on the environment.    |  resistance to change.  The Industrial Revolution  \nThe rapid industrialization led to pollution,       |  also had significant impacts on the environment.  \ndeforestation, and the depletion of natural         |  The rapid industrialization led to pollution,     \nresources.  The Industrial Revolution was a period  |  deforestation, and the depletion of natural       \nof great change and upheaval, and its social and    |  resources.  The Industrial Revolution was a period\neconomic impacts continue to be studied and         |  of significant change in England, leading to both \ndebated today.<|im_end|>                            |  positive and negative impacts. It marked the      \n                                                    |  beginning of a new era of industrialization and   \n                                                    |  urbanization, but it also led to significant      \n                                                    |  social and economic challenges.<|im_end|>         \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:34:34.254502Z",
          "iopub.execute_input": "2024-11-17T00:34:34.255695Z",
          "iopub.status.idle": "2024-11-17T00:34:50.961348Z",
          "shell.execute_reply.started": "2024-11-17T00:34:34.255642Z",
          "shell.execute_reply": "2024-11-17T00:34:50.960424Z"
        },
        "outputId": "60534907-59d4-44e9-ef8f-7eb324de2e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human How can I find the division       |  <|im_start|>human How can I find the division     \nremainder of a number in Python? For instance, if   |  remainder of a number in Python? For instance, if \nI have a number like 26 and it is divided by 7,     |  I have a number like 26 and it is divided by 7,   \nthe division remainder should be 5, as 7+7+7        |  the division remainder should be 5, as 7+7+7      \nequals 21 and 26 minus 21 equals 5. How can I       |  equals 21 and 26 minus 21 equals 5. How can I     \ncompute this?<|im_end|> <|im_start|>assistant You   |  compute this?<|im_end|> <|im_start|>assistant You \ncan use the modulo operator (%) in Python to find   |  can use the modulo operator (%) in Python to find \nthe division remainder. The modulo operator         |  the division remainder. The modulo operator       \nreturns the remainder of the division of the        |  returns the remainder of the division of the      \nnumber before the operator by the number after the  |  number before it by the number after it.  Here's  \noperator.  Here's how you can do it:  ```python     |  how you can do it:  ```python number = 26 divisor \nnumber = 26 divisor = 7 remainder = number %        |  = 7 remainder = number % divisor print(remainder) \ndivisor print(remainder) ```  In this case, the     |  ```  In this case, the output will be 5, which is \noutput will be 5, which is the division remainder   |  the division remainder of 26 divided by           \nof 26 divided by 7.<|im_end|>                       |  7.<|im_end|>                                      \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}