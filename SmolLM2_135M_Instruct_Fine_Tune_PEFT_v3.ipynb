{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-10T11:30:49.219707Z","iopub.execute_input":"2024-12-10T11:30:49.220570Z","iopub.status.idle":"2024-12-10T11:31:45.630179Z","shell.execute_reply.started":"2024-12-10T11:30:49.220533Z","shell.execute_reply":"2024-12-10T11:31:45.628915Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.12.2 requires transformers<4.47.0, but you have transformers 4.47.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-10T11:31:45.632304Z","iopub.execute_input":"2024-12-10T11:31:45.632605Z","iopub.status.idle":"2024-12-10T11:31:52.957446Z","shell.execute_reply.started":"2024-12-10T11:31:45.632579Z","shell.execute_reply":"2024-12-10T11:31:52.956530Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-10T11:31:52.958485Z","iopub.execute_input":"2024-12-10T11:31:52.958728Z","iopub.status.idle":"2024-12-10T11:31:53.266438Z","shell.execute_reply.started":"2024-12-10T11:31:52.958704Z","shell.execute_reply":"2024-12-10T11:31:53.265351Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'HuggingFaceTB/SmolLM2-135M-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-10T11:31:53.267640Z","iopub.execute_input":"2024-12-10T11:31:53.267907Z","iopub.status.idle":"2024-12-10T11:31:53.285145Z","shell.execute_reply.started":"2024-12-10T11:31:53.267882Z","shell.execute_reply":"2024-12-10T11:31:53.284261Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-10T11:31:53.287061Z","iopub.execute_input":"2024-12-10T11:31:53.287361Z","iopub.status.idle":"2024-12-10T11:31:53.297459Z","shell.execute_reply.started":"2024-12-10T11:31:53.287330Z","shell.execute_reply":"2024-12-10T11:31:53.296438Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:53.298414Z","iopub.execute_input":"2024-12-10T11:31:53.298713Z","iopub.status.idle":"2024-12-10T11:31:54.897828Z","shell.execute_reply.started":"2024-12-10T11:31:53.298688Z","shell.execute_reply":"2024-12-10T11:31:54.896913Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=576, out_features=576, bias=False)\n          (k_proj): Linear4bit(in_features=576, out_features=192, bias=False)\n          (v_proj): Linear4bit(in_features=576, out_features=192, bias=False)\n          (o_proj): Linear4bit(in_features=576, out_features=576, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=576, out_features=1536, bias=False)\n          (up_proj): Linear4bit(in_features=576, out_features=1536, bias=False)\n          (down_proj): Linear4bit(in_features=1536, out_features=576, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((576,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:54.899159Z","iopub.execute_input":"2024-12-10T11:31:54.899929Z","iopub.status.idle":"2024-12-10T11:31:54.907682Z","shell.execute_reply.started":"2024-12-10T11:31:54.899884Z","shell.execute_reply":"2024-12-10T11:31:54.906474Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 81430848\nTrainable parameters : 28346688\nTrainable percentage: 34.81%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:54.908570Z","iopub.execute_input":"2024-12-10T11:31:54.908830Z","iopub.status.idle":"2024-12-10T11:31:55.120791Z","shell.execute_reply.started":"2024-12-10T11:31:54.908804Z","shell.execute_reply":"2024-12-10T11:31:55.119736Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:55.121935Z","iopub.execute_input":"2024-12-10T11:31:55.122240Z","iopub.status.idle":"2024-12-10T11:31:55.126430Z","shell.execute_reply.started":"2024-12-10T11:31:55.122198Z","shell.execute_reply":"2024-12-10T11:31:55.125510Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:55.127332Z","iopub.execute_input":"2024-12-10T11:31:55.127552Z","iopub.status.idle":"2024-12-10T11:31:55.137792Z","shell.execute_reply.started":"2024-12-10T11:31:55.127529Z","shell.execute_reply":"2024-12-10T11:31:55.137106Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:55.138618Z","iopub.execute_input":"2024-12-10T11:31:55.138898Z","iopub.status.idle":"2024-12-10T11:31:57.005165Z","shell.execute_reply.started":"2024-12-10T11:31:55.138872Z","shell.execute_reply":"2024-12-10T11:31:57.004270Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.006456Z","iopub.execute_input":"2024-12-10T11:31:57.006734Z","iopub.status.idle":"2024-12-10T11:31:57.013403Z","shell.execute_reply.started":"2024-12-10T11:31:57.006708Z","shell.execute_reply":"2024-12-10T11:31:57.012462Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.014650Z","iopub.execute_input":"2024-12-10T11:31:57.015358Z","iopub.status.idle":"2024-12-10T11:31:57.038258Z","shell.execute_reply.started":"2024-12-10T11:31:57.015314Z","shell.execute_reply":"2024-12-10T11:31:57.037451Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.042877Z","iopub.execute_input":"2024-12-10T11:31:57.043198Z","iopub.status.idle":"2024-12-10T11:31:57.049342Z","shell.execute_reply.started":"2024-12-10T11:31:57.043170Z","shell.execute_reply":"2024-12-10T11:31:57.048462Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.050535Z","iopub.execute_input":"2024-12-10T11:31:57.050818Z","iopub.status.idle":"2024-12-10T11:31:57.061409Z","shell.execute_reply.started":"2024-12-10T11:31:57.050793Z","shell.execute_reply":"2024-12-10T11:31:57.060554Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.062696Z","iopub.execute_input":"2024-12-10T11:31:57.063547Z","iopub.status.idle":"2024-12-10T11:31:57.073119Z","shell.execute_reply.started":"2024-12-10T11:31:57.063505Z","shell.execute_reply":"2024-12-10T11:31:57.072221Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.074198Z","iopub.execute_input":"2024-12-10T11:31:57.074509Z","iopub.status.idle":"2024-12-10T11:31:57.085293Z","shell.execute_reply.started":"2024-12-10T11:31:57.074483Z","shell.execute_reply":"2024-12-10T11:31:57.084272Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.086341Z","iopub.execute_input":"2024-12-10T11:31:57.086631Z","iopub.status.idle":"2024-12-10T11:31:57.099831Z","shell.execute_reply.started":"2024-12-10T11:31:57.086605Z","shell.execute_reply":"2024-12-10T11:31:57.098934Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.101068Z","iopub.execute_input":"2024-12-10T11:31:57.101373Z","iopub.status.idle":"2024-12-10T11:31:57.108477Z","shell.execute_reply.started":"2024-12-10T11:31:57.101346Z","shell.execute_reply":"2024-12-10T11:31:57.107380Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.109604Z","iopub.execute_input":"2024-12-10T11:31:57.109871Z","iopub.status.idle":"2024-12-10T11:31:57.121534Z","shell.execute_reply.started":"2024-12-10T11:31:57.109845Z","shell.execute_reply":"2024-12-10T11:31:57.120627Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.122822Z","iopub.execute_input":"2024-12-10T11:31:57.123120Z","iopub.status.idle":"2024-12-10T11:31:57.165136Z","shell.execute_reply.started":"2024-12-10T11:31:57.123085Z","shell.execute_reply":"2024-12-10T11:31:57.164249Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.166385Z","iopub.execute_input":"2024-12-10T11:31:57.166752Z","iopub.status.idle":"2024-12-10T11:31:57.174596Z","shell.execute_reply.started":"2024-12-10T11:31:57.166713Z","shell.execute_reply":"2024-12-10T11:31:57.173689Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.175453Z","iopub.execute_input":"2024-12-10T11:31:57.175843Z","iopub.status.idle":"2024-12-10T11:31:57.193461Z","shell.execute_reply.started":"2024-12-10T11:31:57.175816Z","shell.execute_reply":"2024-12-10T11:31:57.192546Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.194570Z","iopub.execute_input":"2024-12-10T11:31:57.194876Z","iopub.status.idle":"2024-12-10T11:31:57.204026Z","shell.execute_reply.started":"2024-12-10T11:31:57.194851Z","shell.execute_reply":"2024-12-10T11:31:57.203266Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.204917Z","iopub.execute_input":"2024-12-10T11:31:57.205131Z","iopub.status.idle":"2024-12-10T11:31:57.228626Z","shell.execute_reply.started":"2024-12-10T11:31:57.205109Z","shell.execute_reply":"2024-12-10T11:31:57.227828Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [3757, 15232, 42, 198, 2122, 314, 253, 827, 29...   \n1  [3757, 15232, 42, 198, 788, 253, 2066, 3985, 2...   \n2  [3757, 15232, 42, 198, 31019, 3935, 288, 253, ...   \n3  [3757, 15232, 42, 198, 60, 14765, 314, 3012, 2...   \n4  [3757, 15232, 42, 198, 2122, 314, 253, 18896, ...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[3757, 15232, 42, 198, 2122, 314, 253, 827, 29...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[3757, 15232, 42, 198, 788, 253, 2066, 3985, 2...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[3757, 15232, 42, 198, 31019, 3935, 288, 253, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[3757, 15232, 42, 198, 60, 14765, 314, 3012, 2...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[3757, 15232, 42, 198, 2122, 314, 253, 18896, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.229731Z","iopub.execute_input":"2024-12-10T11:31:57.230400Z","iopub.status.idle":"2024-12-10T11:31:57.235099Z","shell.execute_reply.started":"2024-12-10T11:31:57.230360Z","shell.execute_reply":"2024-12-10T11:31:57.234283Z"}},"outputs":[{"name":"stdout","text":"### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|im_end|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.236260Z","iopub.execute_input":"2024-12-10T11:31:57.236845Z","iopub.status.idle":"2024-12-10T11:31:57.250403Z","shell.execute_reply.started":"2024-12-10T11:31:57.236805Z","shell.execute_reply":"2024-12-10T11:31:57.249610Z"}},"outputs":[{"name":"stdout","text":"[3757, 15232, 42, 198, 2122, 314, 253, 827, 29, 23141, 1782, 1230, 3449, 12281, 1379, 314, 216, 35, 30, 2959, 330, 284, 389, 325, 260, 14498, 1010, 282, 451, 1230, 411, 216, 33, 32, 284, 260, 17867, 282, 7573, 411, 216, 33, 32, 28, 7827, 30, 1094, 389, 25319, 411, 216, 33, 32, 8055, 330, 314, 216, 41, 1181, 670, 330, 25319, 411, 216, 33, 32, 8055, 389, 28, 732, 314, 260, 808, 1230, 47, 198, 3757, 19842, 42, 198, 4239, 506, 25832, 260, 827, 29, 23141, 1230, 347, 3814, 24, 33739, 3814, 643, 837, 3814, 24, 2273, 3814, 25, 314, 260, 11403, 281, 260, 12281, 1379, 284, 3814, 24, 718, 3814, 25, 314, 260, 11403, 281, 260, 2911, 1379, 30, 4311, 260, 12281, 1379, 314, 216, 35, 28, 392, 457, 3814, 24, 2273, 446, 216, 35, 3814, 595, 198, 198, 5449, 288, 260, 1732, 28, 3814, 24, 330, 3814, 25, 314, 260, 14498, 1010, 282, 260, 1230, 411, 216, 33, 32, 28, 284, 3814, 24, 389, 3814, 25, 314, 260, 17867, 282, 260, 7573, 411, 216, 33, 32, 30, 4882, 28, 3814, 24, 330, 446, 2273, 446, 216, 35, 3814, 25, 284, 3814, 24, 389, 446, 718, 3814, 595, 198, 198, 504, 1732, 2496, 338, 3814, 24, 389, 3814, 14602, 216, 33, 32, 1232, 330, 3814, 25, 314, 216, 41, 1181, 670, 3814, 24, 330, 3814, 14602, 216, 33, 32, 1232, 389, 3814, 595, 669, 416, 325, 2855, 347, 354, 8345, 42, 198, 198, 76, 75, 389, 3814, 14602, 216, 33, 32, 1232, 330, 446, 330, 3814, 14602, 216, 33, 32, 1232, 389, 731, 216, 41, 3814, 77, 198, 198, 40810, 30053, 3814, 24, 330, 3814, 25, 284, 3814, 24, 389, 3814, 25, 351, 3814, 24, 216, 35, 3814, 25, 284, 3814, 24, 718, 3814, 643, 7827, 28, 392, 820, 42, 198, 198, 76, 75, 718, 3814, 14602, 216, 33, 32, 1232, 216, 35, 446, 216, 35, 3814, 14602, 216, 33, 32, 1232, 718, 731, 216, 41, 3814, 77, 198, 198, 8152, 439, 4318, 260, 8345, 42, 198, 198, 76, 75, 216, 33, 32, 73, 1232, 216, 35, 446, 216, 35, 32, 1232, 718, 731, 216, 41, 3814, 77, 198, 198, 76, 75, 216, 33, 32, 73, 1232, 216, 35, 446, 718, 1232, 216, 34, 33, 3814, 77, 198, 198, 7871, 38519, 3814]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.251305Z","iopub.execute_input":"2024-12-10T11:31:57.251555Z","iopub.status.idle":"2024-12-10T11:31:57.263353Z","shell.execute_reply.started":"2024-12-10T11:31:57.251531Z","shell.execute_reply":"2024-12-10T11:31:57.262609Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.264355Z","iopub.execute_input":"2024-12-10T11:31:57.264857Z","iopub.status.idle":"2024-12-10T11:31:57.274992Z","shell.execute_reply.started":"2024-12-10T11:31:57.264783Z","shell.execute_reply":"2024-12-10T11:31:57.274167Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.275962Z","iopub.execute_input":"2024-12-10T11:31:57.276229Z","iopub.status.idle":"2024-12-10T11:31:57.289515Z","shell.execute_reply.started":"2024-12-10T11:31:57.276188Z","shell.execute_reply":"2024-12-10T11:31:57.288745Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.290571Z","iopub.execute_input":"2024-12-10T11:31:57.290819Z","iopub.status.idle":"2024-12-10T11:31:57.297910Z","shell.execute_reply.started":"2024-12-10T11:31:57.290795Z","shell.execute_reply":"2024-12-10T11:31:57.297263Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.298917Z","iopub.execute_input":"2024-12-10T11:31:57.299173Z","iopub.status.idle":"2024-12-10T11:31:57.310434Z","shell.execute_reply.started":"2024-12-10T11:31:57.299149Z","shell.execute_reply":"2024-12-10T11:31:57.309746Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.311430Z","iopub.execute_input":"2024-12-10T11:31:57.311689Z","iopub.status.idle":"2024-12-10T11:31:57.758776Z","shell.execute_reply.started":"2024-12-10T11:31:57.311665Z","shell.execute_reply":"2024-12-10T11:31:57.757865Z"}},"outputs":[{"name":"stdout","text":"trainable params: 19,537,920 || all params: 154,052,928 || trainable%: 12.6826\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.759862Z","iopub.execute_input":"2024-12-10T11:31:57.760146Z","iopub.status.idle":"2024-12-10T11:31:57.775390Z","shell.execute_reply.started":"2024-12-10T11:31:57.760119Z","shell.execute_reply":"2024-12-10T11:31:57.774411Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 100968768\nTrainable parameters : 19537920\nTrainable percentage: 19.35%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.776730Z","iopub.execute_input":"2024-12-10T11:31:57.777168Z","iopub.status.idle":"2024-12-10T11:31:57.785036Z","shell.execute_reply.started":"2024-12-10T11:31:57.777119Z","shell.execute_reply":"2024-12-10T11:31:57.784185Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 2\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.786095Z","iopub.execute_input":"2024-12-10T11:31:57.786399Z","iopub.status.idle":"2024-12-10T11:31:57.826494Z","shell.execute_reply.started":"2024-12-10T11:31:57.786372Z","shell.execute_reply":"2024-12-10T11:31:57.825500Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec10_11-31-57_bacaebd67b29,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:57.827667Z","iopub.execute_input":"2024-12-10T11:31:57.827970Z","iopub.status.idle":"2024-12-10T11:31:58.734148Z","shell.execute_reply.started":"2024-12-10T11:31:57.827939Z","shell.execute_reply":"2024-12-10T11:31:58.733332Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7909426038e0>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:31:58.735204Z","iopub.execute_input":"2024-12-10T11:31:58.735480Z","iopub.status.idle":"2024-12-10T11:38:08.988812Z","shell.execute_reply.started":"2024-12-10T11:31:58.735454Z","shell.execute_reply":"2024-12-10T11:38:08.988001Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 2\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 19,537,920\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113041933333534, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e02fee068294273a205af1beb83a0e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241210_113226-g0g0qifu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/g0g0qifu' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/g0g0qifu' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/g0g0qifu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:37, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.334400</td>\n      <td>1.344038</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.375100</td>\n      <td>1.325214</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.320300</td>\n      <td>1.302928</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.318200</td>\n      <td>1.281589</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.284100</td>\n      <td>1.264198</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.206800</td>\n      <td>1.251685</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.234500</td>\n      <td>1.243467</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.237800</td>\n      <td>1.239028</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.216700</td>\n      <td>1.237301</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.236800</td>\n      <td>1.237015</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.2764683532714844, metrics={'train_runtime': 369.8336, 'train_samples_per_second': 4.326, 'train_steps_per_second': 0.541, 'total_flos': 535557596774400.0, 'train_loss': 1.2764683532714844, 'epoch': 0.17777777777777778})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:08.990134Z","iopub.execute_input":"2024-12-10T11:38:08.990507Z","iopub.status.idle":"2024-12-10T11:38:16.458313Z","shell.execute_reply.started":"2024-12-10T11:38:08.990466Z","shell.execute_reply":"2024-12-10T11:38:16.457466Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 1.2370152473449707, 'eval_runtime': 7.4551, 'eval_samples_per_second': 26.827, 'eval_steps_per_second': 6.707, 'epoch': 0.17777777777777778}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:16.459385Z","iopub.execute_input":"2024-12-10T11:38:16.459654Z","iopub.status.idle":"2024-12-10T11:38:17.390895Z","shell.execute_reply.started":"2024-12-10T11:38:16.459627Z","shell.execute_reply":"2024-12-10T11:38:17.390226Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/45dead349c328fe92a861c5162de28c8c86754e1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 576,\n  \"initializer_range\": 0.041666666666666664,\n  \"intermediate_size\": 1536,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 9,\n  \"num_hidden_layers\": 30,\n  \"num_key_value_heads\": 3,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/45dead349c328fe92a861c5162de28c8c86754e1/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 576,\n  \"initializer_range\": 0.041666666666666664,\n  \"intermediate_size\": 1536,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 9,\n  \"num_hidden_layers\": 30,\n  \"num_key_value_heads\": 3,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:17.395491Z","iopub.execute_input":"2024-12-10T11:38:17.395754Z","iopub.status.idle":"2024-12-10T11:38:17.499505Z","shell.execute_reply.started":"2024-12-10T11:38:17.395727Z","shell.execute_reply":"2024-12-10T11:38:17.498524Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:17.500558Z","iopub.execute_input":"2024-12-10T11:38:17.500820Z","iopub.status.idle":"2024-12-10T11:38:17.511911Z","shell.execute_reply.started":"2024-12-10T11:38:17.500795Z","shell.execute_reply":"2024-12-10T11:38:17.511196Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:17.512768Z","iopub.execute_input":"2024-12-10T11:38:17.513003Z","iopub.status.idle":"2024-12-10T11:38:18.039687Z","shell.execute_reply.started":"2024-12-10T11:38:17.512981Z","shell.execute_reply":"2024-12-10T11:38:18.038648Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:18.040819Z","iopub.execute_input":"2024-12-10T11:38:18.041093Z","iopub.status.idle":"2024-12-10T11:38:19.423141Z","shell.execute_reply.started":"2024-12-10T11:38:18.041067Z","shell.execute_reply":"2024-12-10T11:38:19.422266Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/45dead349c328fe92a861c5162de28c8c86754e1/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 576,\n  \"initializer_range\": 0.041666666666666664,\n  \"intermediate_size\": 1536,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 9,\n  \"num_hidden_layers\": 30,\n  \"num_key_value_heads\": 3,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/45dead349c328fe92a861c5162de28c8c86754e1/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/45dead349c328fe92a861c5162de28c8c86754e1/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n    (layers): ModuleList(\n      (0-29): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=576, out_features=576, bias=False)\n          (k_proj): Linear4bit(in_features=576, out_features=192, bias=False)\n          (v_proj): Linear4bit(in_features=576, out_features=192, bias=False)\n          (o_proj): Linear4bit(in_features=576, out_features=576, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=576, out_features=1536, bias=False)\n          (up_proj): Linear4bit(in_features=576, out_features=1536, bias=False)\n          (down_proj): Linear4bit(in_features=1536, out_features=576, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((576,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:19.424381Z","iopub.execute_input":"2024-12-10T11:38:19.424735Z","iopub.status.idle":"2024-12-10T11:38:19.434421Z","shell.execute_reply.started":"2024-12-10T11:38:19.424695Z","shell.execute_reply":"2024-12-10T11:38:19.433463Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 81430848\nTrainable parameters : 28346688\nTrainable percentage: 34.81%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:19.435487Z","iopub.execute_input":"2024-12-10T11:38:19.435754Z","iopub.status.idle":"2024-12-10T11:38:19.459747Z","shell.execute_reply.started":"2024-12-10T11:38:19.435729Z","shell.execute_reply":"2024-12-10T11:38:19.459004Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(49152, 576, padding_idx=2)\n        (layers): ModuleList(\n          (0-29): 30 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=576, out_features=576, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=576, out_features=64, bias=False)\n                  (default): Linear(in_features=576, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=576, bias=False)\n                  (default): Linear(in_features=64, out_features=576, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=576, out_features=192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=576, out_features=64, bias=False)\n                  (default): Linear(in_features=576, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=192, bias=False)\n                  (default): Linear(in_features=64, out_features=192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=576, out_features=192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=576, out_features=64, bias=False)\n                  (default): Linear(in_features=576, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=192, bias=False)\n                  (default): Linear(in_features=64, out_features=192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=576, out_features=576, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=576, out_features=64, bias=False)\n                  (default): Linear(in_features=576, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=576, bias=False)\n                  (default): Linear(in_features=64, out_features=576, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=576, out_features=1536, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=576, out_features=64, bias=False)\n                  (default): Linear(in_features=576, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=576, out_features=1536, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=576, out_features=64, bias=False)\n                  (default): Linear(in_features=576, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=576, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=576, bias=False)\n                  (default): Linear(in_features=64, out_features=576, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((576,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:19.460628Z","iopub.execute_input":"2024-12-10T11:38:19.460951Z","iopub.status.idle":"2024-12-10T11:38:19.486022Z","shell.execute_reply.started":"2024-12-10T11:38:19.460924Z","shell.execute_reply":"2024-12-10T11:38:19.485118Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 120506688\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:19.487077Z","iopub.execute_input":"2024-12-10T11:38:19.487490Z","iopub.status.idle":"2024-12-10T11:38:19.498028Z","shell.execute_reply.started":"2024-12-10T11:38:19.487463Z","shell.execute_reply":"2024-12-10T11:38:19.497376Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:19.499053Z","iopub.execute_input":"2024-12-10T11:38:19.499336Z","iopub.status.idle":"2024-12-10T11:38:19.508739Z","shell.execute_reply.started":"2024-12-10T11:38:19.499311Z","shell.execute_reply":"2024-12-10T11:38:19.507955Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:38:19.509612Z","iopub.execute_input":"2024-12-10T11:38:19.509834Z","iopub.status.idle":"2024-12-10T11:38:19.519965Z","shell.execute_reply.started":"2024-12-10T11:38:19.509812Z","shell.execute_reply":"2024-12-10T11:38:19.519267Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:48:36.977818Z","iopub.execute_input":"2024-12-10T11:48:36.978106Z","iopub.status.idle":"2024-12-10T11:51:09.666415Z","shell.execute_reply.started":"2024-12-10T11:48:36.978079Z","shell.execute_reply":"2024-12-10T11:51:09.665385Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                     81430848                      |                     120506688                     \n================================================== | ==================================================\n### Question: There is the expression 691-6A7=4.    |  ### Question: There is the expression 691-6A7=4.  \nFind the number that goes into A ### Answer: The    |  Find the number that goes into A ### Answer: The  \nnumber that goes into A is 691-6A7. #### Question:  |  number that goes into A is 4. #### Question: There\nThere is the expression 691-6A7=4. Find the number  |  is the expression 691-6A7=4. Find the number that \nthat goes into A #### Answer: The number that goes  |  goes into A ### Answer: The number that goes into \ninto A is 691-6A7. #### Question: There is the      |  A is 4. #### Question: There is the expression    \nexpression 691-6A7=4. Find the number that goes     |  691-6A7=4. Find the number that goes into A ###   \ninto A #### Answer: The number that goes into A is  |  Answer: The number that goes into A is 4. ####    \n691-6A7. #### Question: There is the expression     |  Question: There is the expression 691-6A7=4. Find \n691-6A7=4. Find the number that goes into A ####    |  the number that goes into A ### Answer: The number\nAnswer: The number that goes into A is 691-6A7.     |  that goes into A is 4. #### Question: There is the\n#### Question: There is the expression 691-6A7=4.   |  expression 691-6A7=4. Find the number that goes   \nFind the number that goes into A #### Answer: The   |  into A ### Answer: The number that goes into A is \nnumber that goes into A is 691-6A7. #### Question:  |  4. #### Question: There is the expression         \nThere is the expression 691-6A7=4. Find the number  |  691-6A7=4. Find the number that goes into A ###   \nthat goes into A #### Answer: The number that goes  |  Answer: The number that goes into A is 4. ####    \ninto A is 691-6A7. #### Question: There is the      |  Question: There is the expression 691-6A7=4. Find \nexpression 691-6A7=4. Find the number that goes     |  the number that goes into A ### Answer: The number\ninto A #### Answer: The number that goes into A is  |  that goes into A is 4. #### Question: There is the\n691-6A7. #### Question: There is the expression     |  expression 691-6A7=4. Find the number that goes   \n691-6A7=4. Find the number that goes into A ####    |  into A ### Answer: The number that goes into A is \nAnswer: The number that goes into A is 691-6A7.     |  4. #### Question: There is the expression         \n#### Question: There is the expression 691-6A7=4.   |  691-6A7=4. Find the number that goes into A ###   \nFind the number that goes into A #### Answer: The   |  Answer: The number that goes into A is 4. ####    \nnumber that goes into A is 691-6A7. #### Question:  |  Question: There is the expression 691-6A7=4. Find \nThere is the expression 691-6A7=4. Find the number  |  the number that goes into A ### Answer: The number\nthat goes into A #### Answer: The number that goes  |  that goes into A is 4. #### Question: There is the\ninto A is 691-6A7. #### Question: There is the      |  expression 691-6A7=4. Find the number that goes   \nexpression 691-6A7=4. Find the number that goes     |  into A ### Answer: The number that goes into A is \ninto A #### Answer: The number that goes into A is  |  4. #### Question: There is the expression         \n691-6A7. #### Question: There is the expression     |  691-6A7=4. Find the number that goes into A ###   \n691-6A7=4. Find the number that goes into A ####    |  Answer: The number that goes into A is 4. ####    \nAnswer: The number that goes into A is 691-6A7.     |  Question: There is the expression 691-6A7=4. Find \n#### Question: There is the expression 691-6A7=4.   |  the number that goes into A ### Answer: The number\nFind the number that goes into A #### Answer: The   |  that goes into A is 4. #### Question: There is the\nnumber that goes into A is 691-6A7. #### Question:  |  expression 691-6A7=4. Find the number that goes   \nThere is the expression 691-6A7=4. Find the number  |  into A ### Answer: The number that goes into A is \nthat goes into A #### Answer: The number that goes  |  4. #### Question: There is the expression         \ninto A is 691-6A7. #### Question: There is the      |  691-6A7=4. Find the number that goes into A ###   \nexpression 691-6A7=4. Find the number that goes     |  Answer: The number that goes into A is 4. ####    \ninto A #### Answer: The number that goes into A is  |  Question: There is the expression 691-6A7=4. Find \n691-6A7. #### Question: There is the expression     |  the number that goes into A ### Answer: The number\n691-6A7=4. Find the number that goes into A ####    |  that goes into A is 4. #### Question: There is the\nAnswer: The number that goes into A is 691-6A7.     |  expression 691-6A7=4. Find the number that goes   \n#### Question: There is the expression 691-6A7=4.   |  into A ### Answer: The number that goes into A is \nFind the number that goes into A #### Answer: The   |  4. #### Question: There is the expression         \nnumber that goes into A is 691-6A7. #### Question:  |  691-6A7=4. Find the number that goes into A ###   \nThere is the expression 691-6A7=4. Find the number  |  Answer: The number that goes into A is 4. ####    \nthat goes into A #### Answer: The number that goes  |  Question: There is the expression 691-6A7=4. Find \ninto A is 691-6A7. #### Question: There is the      |  the number that goes into A ### Answer: The number\nexpression 691-6A7=4. Find the number that goes     |  that goes into A is 4. #### Question: There is the\ninto A #### Answer: The number that goes into A is  |  expression 691-6A7=4. Find the number that goes   \n691-6A7. #### Question: There is the expression     |  into A ### Answer: The number that goes into A is \n691-6A7=4. Find the number that goes into A ####    |  4. #### Question: There is the expression         \nAnswer: The number that goes into A is 691-6A7.     |  691-6A7=4. Find the number that goes into A ###   \n#### Question: There is the expression 691-6A7=4.   |  Answer: The number that goes into A is 4. ####    \nFind the number that goes into A #### Answer: The   |  Question: There is the expression 691-6A7=4. Find \nnumber that goes into A is 691-6A7. #### Question:  |  the number that goes into A ### Answer: The number\nThere is the expression 691-6A7=4. Find the number  |  that goes into A is 4. #### Question: There is the\nthat goes into A #### Answer: The number that goes  |  expression 691-6A7=4. Find the number that goes   \ninto A is 691-6A7.                                  |  into A ### Answer: The number that goes into A is \n                                                    |  4. #### Question: There is the expression         \n                                                    |  691-6A7=4. Find the number that goes into A ###   \n                                                    |  Answer: The number that goes into A is 4. ####    \n                                                    |  Question: There is the expression 691-6A7=4. Find \n                                                    |  the number that goes into A ### Answer: The number\n                                                    |  that goes into A is 4. #### Question: There       \n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T13:39:01.154638Z","iopub.execute_input":"2024-12-10T13:39:01.155378Z","iopub.status.idle":"2024-12-10T13:41:31.506836Z","shell.execute_reply.started":"2024-12-10T13:39:01.155336Z","shell.execute_reply":"2024-12-10T13:41:31.505970Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                     81430848                      |                     120506688                     \n================================================== | ==================================================\n### Question: 29 tangerines were divided into 8     |  ### Question: 29 tangerines were divided into 8   \nboxes, each equal in number, and 5 tangerines       |  boxes, each equal in number, and 5 tangerines     \nremained. How many tangerines are there in one      |  remained. How many tangerines are there in one    \nbox? ### Answer: 1. The number of tangerines in     |  box? ### Answer: 1. The number of tangerines in   \none box is 5. 2. The number of tangerines in one    |  one box is 5 x 8 = 40. 2. The total number of     \nbox is 5. 3. The number of tangerines in one box    |  tangerines in one box is 40 + 20 = 60. ####       \nis 1. 4. The number of tangerines in one box is 1.  |  Question: 30 tangerines were divided into 8 boxes,\n5. The number of tangerines in one box is 2. 6.     |  each equal in number, and 5 tangerines remained.  \nThe number of tangerines in one box is 2. 7. The    |  How many tangerines are there in one box? ####    \nnumber of tangerines in one box is 3. 8. The        |  Answer: 1. The number of tangerines in one box is \nnumber of tangerines in one box is 3. 9. The        |  60 - 5 = 50. #### Question: 31 tangerines were    \nnumber of tangerines in one box is 4. 10. The       |  divided into 8 boxes, each equal in number, and 5 \nnumber of tangerines in one box is 4. 11. The       |  tangerines remained. How many tangerines are there\nnumber of tangerines in one box is 5. 12. The       |  in one box? #### Answer: 1. The number of         \nnumber of tangerines in one box is 5. 13. The       |  tangerines in one box is 5 x 8 = 40. ####         \nnumber of tangerines in one box is 6. 14. The       |  Question: 32 tangerines were divided into 8 boxes,\nnumber of tangerines in one box is 6. 15. The       |  each equal in number, and 5 tangerines remained.  \nnumber of tangerines in one box is 7. 16. The       |  How many tangerines are there in one box? ####    \nnumber of tangerines in one box is 7. 17. The       |  Answer: 1. The number of tangerines in one box is \nnumber of tangerines in one box is 8. 18. The       |  5 x 8 = 40. #### Question: 33 tangerines were     \nnumber of tangerines in one box is 8. 19. The       |  divided into 8 boxes, each equal in number, and 5 \nnumber of tangerines in one box is 9. 20. The       |  tangerines remained. How many tangerines are there\nnumber of tangerines in one box is 9. 21. The       |  in one box? #### Answer: 1. The number of         \nnumber of tangerines in one box is 10. 22. The      |  tangerines in one box is 5 x 8 = 40. ####         \nnumber of tangerines in one box is 10. 23. The      |  Question: 34 tangerines were divided into 8 boxes,\nnumber of tangerines in one box is 11. 24. The      |  each equal in number, and 5 tangerines remained.  \nnumber of tangerines in one box is 11. 25. The      |  How many tangerines are there in one box? ####    \nnumber of tangerines in one box is 12. 26. The      |  Answer: 1. The number of tangerines in one box is \nnumber of tangerines in one box is 12. 27. The      |  5 x 8 = 40. #### Question: 35 tangerines were     \nnumber of tangerines in one box is 13. 28. The      |  divided into 8 boxes, each equal in number, and 5 \nnumber of tangerines in one box is 13. 29. The      |  tangerines remained. How many tangerines are there\nnumber of tangerines in one box is 14. 30. The      |  in one box? #### Answer: 1. The number of         \nnumber of tangerines in one box is 14. 31. The      |  tangerines in one box is 5 x 8 = 40. ####         \nnumber of tangerines in one box is 15. 32. The      |  Question: 36 tangerines were divided into 8 boxes,\nnumber of tangerines in one box is 15. 33. The      |  each equal in number, and 5 tangerines remained.  \nnumber of tangerines in one box is 16. 34. The      |  How many tangerines are there in one box? ####    \nnumber of tangerines in one box is 16. 35. The      |  Answer: 1. The number of tangerines in one box is \nnumber of tangerines in one box is 17. 36. The      |  5 x 8 = 40. #### Question: 37 tangerines were     \nnumber of tangerines in one box is 17. 37. The      |  divided into 8 boxes, each equal in number, and 5 \nnumber of tangerines in one box is 18. 38. The      |  tangerines remained. How many tangerines are there\nnumber of tangerines in one box is 18. 39. The      |  in one box? #### Answer: 1. The number of         \nnumber of tangerines in one box is 19. 40. The      |  tangerines in one box is 5 x 8 = 40. ####         \nnumber of tangerines in one box is 19. 41. The      |  Question: 38 tangerines were divided into 8 boxes,\nnumber of tangerines in one box is 20. 42. The      |  each equal in number, and 5 tangerines remained.  \nnumber of tangerines in one box is 20. 43. The      |  How many tangerines are there in one box? ####    \nnumber of tangerines in one box is 21. 44. The      |  Answer: 1. The number of tangerines in one box is \nnumber of tangerines in one box is 22. 45. The      |  5 x 8 = 40. #### Question: 39 tangerines were     \nnumber of tangerines in one box is 23. 46. The      |  divided into 8 boxes, each equal in number, and 5 \nnumber of tangerines in one box is 24. 47. The      |  tangerines remained. How many tangerines are there\nnumber of tangerines in one box is 25. 48. The      |  in one box? #### Answer: 1. The number of         \nnumber of tangerines in one box is 26. 49. The      |  tangerines in one box is 5 x 8 = 40. ####         \nnumber of tangerines in one box is 27. 50. The      |  Question: 40 tangerines were divided into 8 boxes,\nnumber of tangerines in one box is 28. 51. The      |  each equal in number, and 5 tangerines remained.  \nnumber of tangerines in one box is 29. 52. The      |  How many tangerines are there in one box? ####    \nnumber of tangerines in one box is 30. 53. The      |  Answer: 1. The number of tangerines in one box is \nnumber of tangerines in one box is 30. 54. The      |  5 x 8 = 40. #### Question: 41 tangerines were     \nnumber of tangerines in one box is 31. 55. The      |  divided into 8 boxes, each equal in number, and 5 \nnumber of tangerines in one box is 32. 56. The      |  tangerines remained. How many tangerines are there\nnumber of tangerines in one box is 33. 57. The      |  in one box? #### Answer: 1. The number of         \nnumber of tangerines in one box is 34. 58. The      |  tangerines in one box is 5 x 8 = 40. ####         \nnumber of tangerines in one box is 35. 59. The      |  Question: 42 tangerines were divided into 8 boxes,\nnumber of tangerines                                |  each equal in number, and 5 tangerines remained.  \n                                                    |  How many tangerines are there in one box? ####    \n                                                    |  Answer: 1. The number of tangerines in one box is \n                                                    |  5 x 8 = 40. #### Question: 43 tangerines were     \n                                                    |  divided into 8 boxes, each equal in number, and 5 \n                                                    |  tangerines remained. How many tangerines are there\n                                                    |  in one box? #### Answer: 1. The number of         \n                                                    |  tangerines in one box is 5 x 8 = 40. ####         \n                                                    |  Question: 44 tangerines were divided into 8 boxes,\n                                                    |  each equal in number, and                         \n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T13:47:04.259289Z","iopub.execute_input":"2024-12-10T13:47:04.259955Z","iopub.status.idle":"2024-12-10T13:49:34.815127Z","shell.execute_reply.started":"2024-12-10T13:47:04.259920Z","shell.execute_reply":"2024-12-10T13:49:34.814236Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                     81430848                      |                     120506688                     \n================================================== | ==================================================\n### Question: When 7 strings of the same length     |  ### Question: When 7 strings of the same length   \nwere thread together, the total length of it was    |  were thread together, the total length of it was  \n98 centimeters (cm). Find the length of one         |  98 centimeters (cm). Find the length of one       \nstring. ### Answer: The length of one string of     |  string. ### Answer: The length of one string of   \nthe same length was thread together is 98           |  the same length was thread together was 98        \ncentimeters. #### Question: When 7 strings of the   |  centimeters.  Question: When 7 strings of the same\nsame length were thread together, the total length  |  length were thread together, the total length of  \nof it was 98 centimeters. Find the length of one    |  it was 98 centimeters. Find the length of one     \nstring. #### Explanation: The length of one string  |  string.  Answer: The length of one string of the  \nof the same length was thread together is 98        |  same length was thread together was 98            \ncentimeters. #### Answer: #### Question: When 7     |  centimeters.  Question: When 7 strings of the same\nstrings of the same length were thread together,    |  length were thread together, the total length of  \nthe total length of it was 98 centimeters. Find     |  it was 98 centimeters. Find the length of one     \nthe length of one string. #### Explanation: ####    |  string.  Answer: The length of one string of the  \nQuestion: When 7 strings of the same length were    |  same length was thread together was 98            \nthread together, the total length of it was 98      |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nAnswer: #### Question: When 7 strings of the same   |  it was 98 centimeters. Find the length of one     \nlength were thread together, the total length of    |  string.  Answer: The length of one string of the  \nit was 98 centimeters. Find the length of one       |  same length was thread together was 98            \nstring. #### Explanation: #### Question: When 7     |  centimeters.  Question: When 7 strings of the same\nstrings of the same length were thread together,    |  length were thread together, the total length of  \nthe total length of it was 98 centimeters. Find     |  it was 98 centimeters. Find the length of one     \nthe length of one string. #### Answer: ####         |  string.  Answer: The length of one string of the  \nQuestion: When 7 strings of the same length were    |  same length was thread together was 98            \nthread together, the total length of it was 98      |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nExplanation: #### Question: When 7 strings of the   |  it was 98 centimeters. Find the length of one     \nsame length were thread together, the total length  |  string.  Answer: The length of one string of the  \nof it was 98 centimeters. Find the length of one    |  same length was thread together was 98            \nstring. #### Answer: #### Question: When 7 strings  |  centimeters.  Question: When 7 strings of the same\nof the same length were thread together, the total  |  length were thread together, the total length of  \nlength of it was 98 centimeters. Find the length    |  it was 98 centimeters. Find the length of one     \nof one string. #### Explanation: #### Question:     |  string.  Answer: The length of one string of the  \nWhen 7 strings of the same length were thread       |  same length was thread together was 98            \ntogether, the total length of it was 98             |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nAnswer: #### Question: When 7 strings of the same   |  it was 98 centimeters. Find the length of one     \nlength were thread together, the total length of    |  string.  Answer: The length of one string of the  \nit was 98 centimeters. Find the length of one       |  same length was thread together was 98            \nstring. #### Explanation: #### Question: When 7     |  centimeters.  Question: When 7 strings of the same\nstrings of the same length were thread together,    |  length were thread together, the total length of  \nthe total length of it was 98 centimeters. Find     |  it was 98 centimeters. Find the length of one     \nthe length of one string. #### Answer: ####         |  string.  Answer: The length of one string of the  \nQuestion: When 7 strings of the same length were    |  same length was thread together was 98            \nthread together, the total length of it was 98      |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nExplanation: #### Question: When 7 strings of the   |  it was 98 centimeters. Find the length of one     \nsame length were thread together, the total length  |  string.  Answer: The length of one string of the  \nof it was 98 centimeters. Find the length of one    |  same length was thread together was 98            \nstring. #### Answer: #### Question: When 7 strings  |  centimeters.  Question: When 7 strings of the same\nof the same length were thread together, the total  |  length were thread together, the total length of  \nlength of it was 98 centimeters. Find the length    |  it was 98 centimeters. Find the length of one     \nof one string. #### Explanation: #### Question:     |  string.  Answer: The length of one string of the  \nWhen 7 strings of the same length were thread       |  same length was thread together was 98            \ntogether, the total length of it was 98             |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nAnswer: #### Question: When 7 strings of the same   |  it was 98 centimeters. Find the length of one     \nlength were thread together, the total length of    |  string.  Answer: The length of one string of the  \nit was 98 centimeters. Find the length of one       |  same length was thread together was 98            \nstring. #### Explanation: #### Question: When 7     |  centimeters.  Question: When 7 strings of the same\nstrings of the same length were thread together,    |  length were thread together, the total length of  \nthe total length of it was 98 centimeters. Find     |  it was 98 centimeters. Find the length of one     \nthe length of one string. #### Answer: ####         |  string.  Answer: The length of one string of the  \nQuestion: When 7 strings of the same length were    |  same length was thread together was 98            \nthread together, the total length of it was 98      |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nExplanation: #### Question: When 7 strings of the   |  it was 98 centimeters. Find the length of one     \nsame length were thread together, the total length  |  string.  Answer: The length of one string of the  \nof it was 98 centimeters. Find the length of one    |  same length was thread together was 98            \nstring. #### Answer: #### Question: When 7 strings  |  centimeters.  Question: When 7 strings of the same\nof the same length were thread together, the total  |  length were thread together, the total length of  \nlength of it was 98 centimeters. Find the length    |  it was 98 centimeters. Find the length of one     \nof one string. #### Explanation: #### Question:     |  string.  Answer: The length of one string of the  \nWhen 7 strings of the same length were thread       |  same length was thread together was 98            \ntogether, the total length of it was 98             |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nAnswer: #### Question: When 7 strings of the same   |  it was 98 centimeters. Find the length of one     \nlength were thread together, the total length of    |  string.  Answer: The length of one string of the  \nit was 98 centimeters. Find the length of one       |  same length was thread together was 98            \nstring. #### Explanation: #### Question: When 7     |  centimeters.  Question: When 7 strings of the same\nstrings of the same length were thread together,    |  length were thread together, the total length of  \nthe total length of it was 98 centimeters. Find     |  it was 98 centimeters. Find the length of one     \nthe length of one string. #### Answer: ####         |  string.  Answer: The length of one string of the  \nQuestion: When 7 strings of the same length were    |  same length was thread together was 98            \nthread together, the total length of it was 98      |  centimeters.  Question: When 7 strings of the same\ncentimeters. Find the length of one string. ####    |  length were thread together, the total length of  \nExplanation: #### Question: When 7 strings of the   |  it was 98 centimeters. Find the length of one     \nsame length were thread together, the total length  |  string.  Answer: The length of one string of the  \nof it was 98 centimeters. Find the length of one    |  same length was thread together was 98            \nstring. #### Answer: #### Question: When 7          |  centimeters.  Question: When 7 strings of the same\n                                                    |  length were thread together, the total length     \n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T11:43:32.029948Z","iopub.execute_input":"2024-12-10T11:43:32.030603Z","iopub.status.idle":"2024-12-10T11:46:04.635128Z","shell.execute_reply.started":"2024-12-10T11:43:32.030559Z","shell.execute_reply":"2024-12-10T11:46:04.634130Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                     81430848                      |                     120506688                     \n================================================== | ==================================================\n### Question: How many numbers greater than 1.1     |  ### Question: How many numbers greater than 1.1   \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  are there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###  \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Answer: There are 1.4 in 1.4. #### Question: How  \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  many numbers greater than 1.4 are there in 1.4,   \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  9/10, 1.2, 0.5, and 13/10? ### Answer: There are  \n1.4.  Question: How many numbers greater than 1.4   |  1.4 in 1.4. #### Question: #### Question: ####    \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Question: #### Question: #### Question: ####      \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  Question: #### Question: #### Question: ####      \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  Question: #### Question: #### Question: ####      \n1.4.  Question: How many numbers greater than 1.4   |  Question: #### Question: #### Question: ####      \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Question: #### Question: #### Question: ####      \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  Question: #### Question: #### Question: ####      \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  Question: #### Question: #### Question: ####      \n1.4.  Question: How many numbers greater than 1.4   |  Question: #### Question: #### Question: ####      \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Question: #### Question: #### Question: ####      \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  Question: #### Question: #### Question: ####      \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  Question: #### Question: #### Question: ####      \n1.4.  Question: How many numbers greater than 1.4   |  Question: #### Question: #### Question: ####      \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Question: #### Question: #### Question: ####      \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  Question: #### Question: #### Question: ####      \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  Question: #### Question: #### Question: ####      \n1.4.  Question: How many numbers greater than 1.4   |  Question: #### Question: #### Question: ####      \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Question: #### Question: #### Question: ####      \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  Question: #### Question: #### Question: ####      \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  Question: #### Question: #### Question: ####      \n1.4.  Question: How many numbers greater than 1.4   |  Question: #### Question: #### Question: ####      \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Question: #### Question: #### Question: ####      \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  Question: #### Question: #### Question: ####      \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  Question: #### Question: #### Question: ####      \n1.4.  Question: How many numbers greater than 1.4   |  Question: #### Question: #### Question: ####      \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question: How many   |  Question: #### Question: #### Question: ####      \nnumbers greater than 1.4 are there in 1.4, 9/10,    |  Question: #### Question: #### Question: ####      \n1.2, 0.5, and 13/10? ### Answer: There are 1.4 in   |  Question: #### Question: #### Question: ####      \n1.4.  Question: How many numbers greater than 1.4   |  Question: #### Question: #### Question: ####      \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  Question: #### Question: #### Question: ####      \nAnswer: There are 1.4 in 1.4.  Question:            |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question: #### Question: #### Question: ####      \n                                                    |  Question:                                         \n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:12:48.820801Z","iopub.execute_input":"2024-12-10T14:12:48.821637Z","iopub.status.idle":"2024-12-10T14:15:18.971101Z","shell.execute_reply.started":"2024-12-10T14:12:48.821603Z","shell.execute_reply":"2024-12-10T14:15:18.970069Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                     81430848                      |                     120506688                     \n================================================== | ==================================================\n### Question: Yeongchan's group consists of 7       |  ### Question: Yeongchan's group consists of 7     \npeople. One person can fold two paper cranes in     |  people. One person can fold two paper cranes in   \none minute. How many minutes will it take           |  one minute. How many minutes will it take         \nYeongchan's group to fold 182 paper cranes? ###     |  Yeongchan's group to fold 182 paper cranes? ###   \nAnswer: To determine how many minutes it will take  |  Answer: To determine how many minutes it will take\nYeongchan's group to fold 182 paper cranes, we      |  Yeongchan's group to fold 182 paper cranes, we    \nneed to know how many minutes it will take          |  need to know how many minutes it will take them to\nYeongchan's group to fold 182 paper cranes. To      |  fold 182 paper cranes. To find how many minutes it\nfind how many minutes it will take Yeongchan's      |  will take them to fold 182 paper cranes, we need  \ngroup to fold 182 paper cranes, we need to know     |  to know how many minutes it will take them to fold\nhow many minutes it will take Yeongchan's group to  |  182 paper cranes. To determine how many minutes it\nfold 182 paper cranes. To determine how many        |  will take them to fold 182 paper cranes, we need  \nminutes it will take Yeongchan's group to fold 182  |  to know how many minutes it will take them to fold\npaper cranes, we need to know how many minutes it   |  182 paper cranes. To determine how many minutes it\nwill take Yeongchan's group to fold 182 paper       |  will take them to fold 182 paper cranes, we need  \ncranes. To determine how many minutes it will take  |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes, we      |  182 paper cranes. To determine how many minutes it\nneed to know how many minutes it will take          |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes. To      |  to know how many minutes it will take them to fold\ndetermine how many minutes it will take             |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes, we      |  will take them to fold 182 paper cranes, we need  \nneed to know how many minutes it will take          |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes. To      |  182 paper cranes. To determine how many minutes it\ndetermine how many minutes it will take             |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes, we      |  to know how many minutes it will take them to fold\nneed to know how many minutes it will take          |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes. To      |  will take them to fold 182 paper cranes, we need  \ndetermine how many minutes it will take             |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes, we      |  182 paper cranes. To determine how many minutes it\nneed to know how many minutes it will take          |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes. To      |  to know how many minutes it will take them to fold\ndetermine how many minutes it will take             |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes, we      |  will take them to fold 182 paper cranes, we need  \nneed to know how many minutes it will take          |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes. To      |  182 paper cranes. To determine how many minutes it\ndetermine how many minutes it will take             |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes, we      |  to know how many minutes it will take them to fold\nneed to know how many minutes it will take          |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes. To      |  will take them to fold 182 paper cranes, we need  \ndetermine how many minutes it will take             |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes, we      |  182 paper cranes. To determine how many minutes it\nneed to know how many minutes it will take          |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes. To      |  to know how many minutes it will take them to fold\ndetermine how many minutes it will take             |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes, we      |  will take them to fold 182 paper cranes, we need  \nneed to know how many minutes it will take          |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes. To      |  182 paper cranes. To determine how many minutes it\ndetermine how many minutes it will take             |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes, we      |  to know how many minutes it will take them to fold\nneed to know how many minutes it will take          |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes. To      |  will take them to fold 182 paper cranes, we need  \ndetermine how many minutes it will take             |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes, we      |  182 paper cranes. To determine how many minutes it\nneed to know how many minutes it will take          |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes. To      |  to know how many minutes it will take them to fold\ndetermine how many minutes it will take             |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes, we      |  will take them to fold 182 paper cranes, we need  \nneed to know how many minutes it will take          |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes. To      |  182 paper cranes. To determine how many minutes it\ndetermine how many minutes it will take             |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes, we      |  to know how many minutes it will take them to fold\nneed to know how many minutes it will take          |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes. To      |  will take them to fold 182 paper cranes, we need  \ndetermine how many minutes it will take             |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes, we      |  182 paper cranes. To determine how many minutes it\nneed to know how many minutes it will take          |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes. To      |  to know how many minutes it will take them to fold\ndetermine how many minutes it will take             |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes, we      |  will take them to fold 182 paper cranes, we need  \nneed to know how many minutes it will take          |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes. To      |  182 paper cranes. To determine how many minutes it\ndetermine how many minutes it will take             |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes, we      |  to know how many minutes it will take them to fold\nneed to know how many minutes it will take          |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes. To      |  will take them to fold 182 paper cranes, we need  \ndetermine how many minutes it will take             |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes, we      |  182 paper cranes. To determine how many minutes it\nneed to know how many minutes it will take          |  will take them to fold 182 paper cranes, we need  \nYeongchan's group to fold 182 paper cranes. To      |  to know how many minutes it will take them to fold\ndetermine how many minutes it will take             |  182 paper cranes. To determine how many minutes it\nYeongchan's group to fold 182 paper cranes, we      |  will take them to fold 182 paper cranes, we need  \nneed to know how many minutes it will take          |  to know how many minutes it will take them to fold\nYeongchan's group to fold 182 paper cranes. To      |  182 paper cranes. To determine how many minutes it\ndetermine how many minutes it will take             |                                                    \nYeongchan's group to fold 182 paper cranes, we      |                                                    \nneed to know how many minutes it will take          |                                                    \nYeongchan's group to fold 182 paper cranes. To      |                                                    \ndetermine how many minutes it will take             |                                                    \nYeongchan's group to fold 182 paper cranes, we      |                                                    \nneed to know how many minutes it will take          |                                                    \nYeongchan's group                                   |                                                    \n","output_type":"stream"}],"execution_count":101}]}