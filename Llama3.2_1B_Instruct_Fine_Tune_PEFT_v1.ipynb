{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/Llama3.2_1B_Instruct_Fine_Tune_PEFT_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:44:26.661377Z",
          "iopub.execute_input": "2024-11-16T07:44:26.661838Z",
          "iopub.status.idle": "2024-11-16T07:45:14.216082Z",
          "shell.execute_reply.started": "2024-11-16T07:44:26.661788Z",
          "shell.execute_reply": "2024-11-16T07:45:14.215005Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:14.217634Z",
          "iopub.execute_input": "2024-11-16T07:45:14.218054Z",
          "iopub.status.idle": "2024-11-16T07:45:22.499602Z",
          "shell.execute_reply.started": "2024-11-16T07:45:14.218010Z",
          "shell.execute_reply": "2024-11-16T07:45:22.498648Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:22.500982Z",
          "iopub.execute_input": "2024-11-16T07:45:22.501318Z",
          "iopub.status.idle": "2024-11-16T07:45:22.508956Z",
          "shell.execute_reply.started": "2024-11-16T07:45:22.501284Z",
          "shell.execute_reply": "2024-11-16T07:45:22.507949Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n",
        "#model_name = url.split('.co/')[-1]\n",
        "\n",
        "model_name = 'unsloth/Llama-3.2-1B-Instruct'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:22.511368Z",
          "iopub.execute_input": "2024-11-16T07:45:22.511654Z",
          "iopub.status.idle": "2024-11-16T07:45:22.522039Z",
          "shell.execute_reply.started": "2024-11-16T07:45:22.511624Z",
          "shell.execute_reply": "2024-11-16T07:45:22.521126Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "    if base == True:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype = torch.float16,\n",
        "            trust_remote_code = True\n",
        "        ).to(device)\n",
        "\n",
        "        return model\n",
        "\n",
        "    else:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = 'nf4',\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config = bnb_config,\n",
        "            trust_remote_code = True\n",
        "        ).to(device)\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:22.523082Z",
          "iopub.execute_input": "2024-11-16T07:45:22.523396Z",
          "iopub.status.idle": "2024-11-16T07:45:22.531807Z",
          "shell.execute_reply.started": "2024-11-16T07:45:22.523349Z",
          "shell.execute_reply": "2024-11-16T07:45:22.531037Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:22.535010Z",
          "iopub.execute_input": "2024-11-16T07:45:22.535339Z",
          "iopub.status.idle": "2024-11-16T07:45:26.368305Z",
          "shell.execute_reply.started": "2024-11-16T07:45:22.535308Z",
          "shell.execute_reply": "2024-11-16T07:45:26.367423Z"
        },
        "outputId": "a0d37fcc-9dac-4a73-9a4d-4eb86a923123"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:26.369493Z",
          "iopub.execute_input": "2024-11-16T07:45:26.369798Z",
          "iopub.status.idle": "2024-11-16T07:45:26.378028Z",
          "shell.execute_reply.started": "2024-11-16T07:45:26.369764Z",
          "shell.execute_reply": "2024-11-16T07:45:26.377094Z"
        },
        "outputId": "0297f079-8fcb-4982-a35e-ce63b082f01e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 749275136\nTrainable parameters : 262735872\nTrainable percentage: 35.07%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:26.379458Z",
          "iopub.execute_input": "2024-11-16T07:45:26.379780Z",
          "iopub.status.idle": "2024-11-16T07:45:27.100890Z",
          "shell.execute_reply.started": "2024-11-16T07:45:26.379728Z",
          "shell.execute_reply": "2024-11-16T07:45:27.099872Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n",
        "#dataset_name = url.split('datasets/')[-1]\n",
        "\n",
        "dataset_name = 'mlabonne/FineTome-100k'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:27.102274Z",
          "iopub.execute_input": "2024-11-16T07:45:27.102613Z",
          "iopub.status.idle": "2024-11-16T07:45:27.106604Z",
          "shell.execute_reply.started": "2024-11-16T07:45:27.102577Z",
          "shell.execute_reply": "2024-11-16T07:45:27.105604Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1024"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:27.108004Z",
          "iopub.execute_input": "2024-11-16T07:45:27.108721Z",
          "iopub.status.idle": "2024-11-16T07:45:27.115837Z",
          "shell.execute_reply.started": "2024-11-16T07:45:27.108644Z",
          "shell.execute_reply": "2024-11-16T07:45:27.115013Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:27.116975Z",
          "iopub.execute_input": "2024-11-16T07:45:27.117251Z",
          "iopub.status.idle": "2024-11-16T07:45:31.277647Z",
          "shell.execute_reply.started": "2024-11-16T07:45:27.117206Z",
          "shell.execute_reply": "2024-11-16T07:45:31.276750Z"
        },
        "outputId": "54949400-012a-4669-c3f0-10c699287fd4"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['conversations', 'source', 'score'],\n    num_rows: 100000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(10000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.279070Z",
          "iopub.execute_input": "2024-11-16T07:45:31.279455Z",
          "iopub.status.idle": "2024-11-16T07:45:31.286444Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.279422Z",
          "shell.execute_reply": "2024-11-16T07:45:31.285693Z"
        },
        "id": "bshmzr25YPD2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.291015Z",
          "iopub.execute_input": "2024-11-16T07:45:31.291370Z",
          "iopub.status.idle": "2024-11-16T07:45:31.317765Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.291338Z",
          "shell.execute_reply": "2024-11-16T07:45:31.316917Z"
        },
        "outputId": "478c26b6-9e8d-46c1-a8be-df7d9c514fb1"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                       conversations  \\\n0  [{'from': 'human', 'value': 'Explain what bool...   \n1  [{'from': 'human', 'value': 'Explain how recur...   \n2  [{'from': 'human', 'value': 'Explain what bool...   \n3  [{'from': 'human', 'value': 'Explain the conce...   \n4  [{'from': 'human', 'value': 'Print the reverse...   \n\n                     source     score  \n0  infini-instruct-top-500k  5.212621  \n1  infini-instruct-top-500k  5.157649  \n2  infini-instruct-top-500k  5.147540  \n3  infini-instruct-top-500k  5.053656  \n4  infini-instruct-top-500k  5.045648  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conversations</th>\n      <th>source</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.212621</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[{'from': 'human', 'value': 'Explain how recur...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.157649</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.147540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[{'from': 'human', 'value': 'Explain the conce...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.053656</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[{'from': 'human', 'value': 'Print the reverse...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.045648</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.318785Z",
          "iopub.execute_input": "2024-11-16T07:45:31.319162Z",
          "iopub.status.idle": "2024-11-16T07:45:31.326867Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.319129Z",
          "shell.execute_reply": "2024-11-16T07:45:31.325965Z"
        },
        "outputId": "66514ec2-9701-48a6-acfb-829db32b1d58"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'conversations': [{'from': 'human',\n   'value': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.'},\n  {'from': 'gpt',\n   'value': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.'}],\n 'source': 'infini-instruct-top-500k',\n 'score': 5.212620735168457}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.328252Z",
          "iopub.execute_input": "2024-11-16T07:45:31.328640Z",
          "iopub.status.idle": "2024-11-16T07:45:31.334638Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.328604Z",
          "shell.execute_reply": "2024-11-16T07:45:31.333747Z"
        },
        "outputId": "1e4efabc-09c2-4d78-d47c-c463c11bc465"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['conversations', 'source', 'score']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_conversations(example):\n",
        "  role_map = {\n",
        "    'human' : 'user',\n",
        "    'gpt' : 'assistant'\n",
        "  }\n",
        "\n",
        "  transformed_conversations = [\n",
        "    {\n",
        "      'role' : role_map.get(turn['from'], turn['from']),\n",
        "      'content' : turn['value']\n",
        "    }\n",
        "    for turn in example['conversations']\n",
        "  ]\n",
        "  return {'conversations': transformed_conversations}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.335877Z",
          "iopub.execute_input": "2024-11-16T07:45:31.336568Z",
          "iopub.status.idle": "2024-11-16T07:45:31.343565Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.336524Z",
          "shell.execute_reply": "2024-11-16T07:45:31.342558Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(transform_conversations, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.344796Z",
          "iopub.execute_input": "2024-11-16T07:45:31.345722Z",
          "iopub.status.idle": "2024-11-16T07:45:31.361233Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.345688Z",
          "shell.execute_reply": "2024-11-16T07:45:31.360414Z"
        },
        "outputId": "638ace08-131e-493f-d1a4-6b48deaf1e81"
      },
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['conversations'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['conversations'])"
      ],
      "metadata": {
        "id": "cZya4tPEWUc9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.362237Z",
          "iopub.execute_input": "2024-11-16T07:45:31.362596Z",
          "iopub.status.idle": "2024-11-16T07:45:31.368417Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.362564Z",
          "shell.execute_reply": "2024-11-16T07:45:31.367440Z"
        },
        "outputId": "17e905c2-76bb-4ddc-ff97-ba7ad192126b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[{'content': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.', 'role': 'user'}, {'content': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.', 'role': 'assistant'}]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_conversation(example):\n",
        "  for entry in example['conversations']:\n",
        "    role = entry['role']\n",
        "    content = entry['content']\n",
        "\n",
        "    if role == 'user':\n",
        "      formatted_text = f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n",
        "    elif role == 'assistant':\n",
        "      formatted_text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n",
        "\n",
        "  return {'prompt': formatted_text}"
      ],
      "metadata": {
        "id": "3bmXGueQWWzt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.369806Z",
          "iopub.execute_input": "2024-11-16T07:45:31.370179Z",
          "iopub.status.idle": "2024-11-16T07:45:31.378637Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.370147Z",
          "shell.execute_reply": "2024-11-16T07:45:31.377806Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = formatted_dataset.map(\n",
        "  format_conversation,\n",
        "  remove_columns = list(formatted_dataset.features.keys())\n",
        ")\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "Z6sSaCr5eaL7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.379798Z",
          "iopub.execute_input": "2024-11-16T07:45:31.380176Z",
          "iopub.status.idle": "2024-11-16T07:45:31.392235Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.380144Z",
          "shell.execute_reply": "2024-11-16T07:45:31.391232Z"
        },
        "outputId": "cd3a7c43-3213-4ea6-88c0-1c13bb696313"
      },
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.393325Z",
          "iopub.execute_input": "2024-11-16T07:45:31.393690Z",
          "iopub.status.idle": "2024-11-16T07:45:31.398936Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.393658Z",
          "shell.execute_reply": "2024-11-16T07:45:31.397966Z"
        },
        "outputId": "118796cb-f756-45d8-9ae1-204d9b5d3f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.400064Z",
          "iopub.execute_input": "2024-11-16T07:45:31.400418Z",
          "iopub.status.idle": "2024-11-16T07:45:31.407460Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.400373Z",
          "shell.execute_reply": "2024-11-16T07:45:31.406675Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.408613Z",
          "iopub.execute_input": "2024-11-16T07:45:31.408890Z",
          "iopub.status.idle": "2024-11-16T07:45:31.626459Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.408860Z",
          "shell.execute_reply": "2024-11-16T07:45:31.625417Z"
        },
        "outputId": "c1d6b1cc-19fd-4603-f0f2-abdcd29f3207"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.627884Z",
          "iopub.execute_input": "2024-11-16T07:45:31.628559Z",
          "iopub.status.idle": "2024-11-16T07:45:31.635136Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.628511Z",
          "shell.execute_reply": "2024-11-16T07:45:31.634175Z"
        },
        "outputId": "69acf46d-524b-4f1d-d5bb-8733a639db82"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.636421Z",
          "iopub.execute_input": "2024-11-16T07:45:31.636988Z",
          "iopub.status.idle": "2024-11-16T07:45:31.650785Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.636938Z",
          "shell.execute_reply": "2024-11-16T07:45:31.649908Z"
        },
        "outputId": "9b87b1f6-2187-41f9-8453-3f72650365cb"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.651806Z",
          "iopub.execute_input": "2024-11-16T07:45:31.652159Z",
          "iopub.status.idle": "2024-11-16T07:45:31.659244Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.652127Z",
          "shell.execute_reply": "2024-11-16T07:45:31.658299Z"
        },
        "outputId": "fa88851b-cb05-4d14-ed29-b2968fa6a8a0"
      },
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.660416Z",
          "iopub.execute_input": "2024-11-16T07:45:31.660706Z",
          "iopub.status.idle": "2024-11-16T07:45:31.688878Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.660675Z",
          "shell.execute_reply": "2024-11-16T07:45:31.688033Z"
        },
        "outputId": "d981c2c9-8c13-49fb-9c5f-6dea9e9fd138"
      },
      "outputs": [
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n1  <|start_header_id|>user<|end_header_id|>\\n\\nWr...   \n2  <|start_header_id|>user<|end_header_id|>\\n\\nWh...   \n3  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n4  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n\n                                           input_ids  \\\n0  [128004, 128004, 128004, 128004, 128004, 12800...   \n1  [128004, 128004, 128004, 128004, 128004, 12800...   \n2  [128004, 128004, 128004, 128004, 128004, 12800...   \n3  [128004, 128004, 128004, 128004, 128004, 12800...   \n4  [128004, 128004, 128004, 128004, 128004, 12800...   \n\n                                      attention_mask  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[128004, 128004, 128004, 128004, 128004, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWr...</td>\n      <td>[128004, 128004, 128004, 128004, 128004, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n      <td>[128004, 128004, 128004, 128004, 128004, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[128004, 128004, 128004, 128004, 128004, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[128004, 128004, 128004, 128004, 128004, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.689974Z",
          "iopub.execute_input": "2024-11-16T07:45:31.690266Z",
          "iopub.status.idle": "2024-11-16T07:45:31.696586Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.690234Z",
          "shell.execute_reply": "2024-11-16T07:45:31.695629Z"
        },
        "outputId": "1c818403-b832-49c8-d65d-4310fa837ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nHow do you graph the inequality #x > -4#?\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nTo graph the inequality #x > -4#, follow these steps:\n\n1. Start by plotting the boundary line: Draw a vertical line at #x = -4# on the X-axis. This line divides the X-axis into two halves.\n\n2. Identify the direction of the inequality: Since the inequality is #x > -4#, it indicates that we are looking for all values of #x# that are greater than -4.\n\n3. Shade the appropriate region: Shade the entire region to the right of the line at #x = -4#. This shaded area represents all the points whose X-coordinate is greater than -4.\n\n4. Include a dashed line: Since the inequality is \"greater than\" (#>,# not \"#≥#\"), draw the line at #x = -4# as a dashed line, indicating that the points on the line itself are not included in the solution.\n\nThe graph consists of the entire half-plane to the right of the dashed vertical line at #x = -4#, excluding the line itself.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.697853Z",
          "iopub.execute_input": "2024-11-16T07:45:31.698249Z",
          "iopub.status.idle": "2024-11-16T07:45:31.708112Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.698191Z",
          "shell.execute_reply": "2024-11-16T07:45:31.707223Z"
        },
        "outputId": "93fbe91c-11a8-4c0c-a44c-520db60323e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128000, 128006, 882, 128007, 271, 4438, 656, 499, 4876, 279, 32305, 674, 87, 871, 482, 19, 2, 5380, 128009, 128006, 78191, 128007, 271, 1271, 4876, 279, 32305, 674, 87, 871, 482, 19, 61878, 1833, 1521, 7504, 1473, 16, 13, 5256, 555, 45002, 279, 19254, 1584, 25, 12261, 264, 12414, 1584, 520, 674, 87, 284, 482, 19, 2, 389, 279, 1630, 36421, 13, 1115, 1584, 65928, 279, 1630, 36421, 1139, 1403, 75212, 382, 17, 13, 65647, 279, 5216, 315, 279, 32305, 25, 8876, 279, 32305, 374, 674, 87, 871, 482, 19, 61878, 433, 15151, 430, 584, 527, 3411, 369, 682, 2819, 315, 674, 87, 2, 430, 527, 7191, 1109, 482, 19, 382, 18, 13, 80918, 279, 8475, 5654, 25, 80918, 279, 4553, 5654, 311, 279, 1314, 315, 279, 1584, 520, 674, 87, 284, 482, 19, 88946, 1115, 92866, 3158, 11105, 682, 279, 3585, 6832, 1630, 81797, 374, 7191, 1109, 482, 19, 382, 19, 13, 30834, 264, 67822, 1584, 25, 8876, 279, 32305, 374, 330, 66335, 1109, 1, 30183, 8226, 2, 539, 5993, 120156, 2, 4063, 4128, 279, 1584, 520, 674, 87, 284, 482, 19, 2, 439, 264, 67822, 1584, 11, 19392, 430, 279, 3585, 389, 279, 1584, 5196, 527, 539, 5343, 304, 279, 6425, 382, 791, 4876, 17610, 315, 279, 4553, 4376, 90649, 311, 279, 1314, 315, 279, 67822, 12414, 1584, 520, 674, 87, 284, 482, 19, 61878, 44878, 279, 1584, 5196, 627, 128009]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.709101Z",
          "iopub.execute_input": "2024-11-16T07:45:31.709399Z",
          "iopub.status.idle": "2024-11-16T07:45:31.718051Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.709360Z",
          "shell.execute_reply": "2024-11-16T07:45:31.717172Z"
        },
        "outputId": "d8f3f887-68f1-45bf-e59b-5b6eedba495d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.719123Z",
          "iopub.execute_input": "2024-11-16T07:45:31.719428Z",
          "iopub.status.idle": "2024-11-16T07:45:31.726655Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.719398Z",
          "shell.execute_reply": "2024-11-16T07:45:31.725845Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.727781Z",
          "iopub.execute_input": "2024-11-16T07:45:31.728136Z",
          "iopub.status.idle": "2024-11-16T07:45:31.737521Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.728103Z",
          "shell.execute_reply": "2024-11-16T07:45:31.736839Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.738453Z",
          "iopub.execute_input": "2024-11-16T07:45:31.738740Z",
          "iopub.status.idle": "2024-11-16T07:45:31.745263Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.738710Z",
          "shell.execute_reply": "2024-11-16T07:45:31.744399Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.746556Z",
          "iopub.execute_input": "2024-11-16T07:45:31.747278Z",
          "iopub.status.idle": "2024-11-16T07:45:31.754574Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.747233Z",
          "shell.execute_reply": "2024-11-16T07:45:31.753784Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:31.755725Z",
          "iopub.execute_input": "2024-11-16T07:45:31.756093Z",
          "iopub.status.idle": "2024-11-16T07:45:32.409651Z",
          "shell.execute_reply.started": "2024-11-16T07:45:31.756060Z",
          "shell.execute_reply": "2024-11-16T07:45:32.408576Z"
        },
        "outputId": "87837181-c310-46ba-9431-f719741416c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 45,088,768 || all params: 1,280,903,168 || trainable%: 3.5201\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ikF6Yfkz1myd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:32.410843Z",
          "iopub.execute_input": "2024-11-16T07:45:32.411191Z",
          "iopub.status.idle": "2024-11-16T07:45:32.428756Z",
          "shell.execute_reply.started": "2024-11-16T07:45:32.411154Z",
          "shell.execute_reply": "2024-11-16T07:45:32.427873Z"
        },
        "outputId": "52dae9a8-277d-47b5-9c67-ca909744bc74"
      },
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=512, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=512, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=8192, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:32.429994Z",
          "iopub.execute_input": "2024-11-16T07:45:32.430321Z",
          "iopub.status.idle": "2024-11-16T07:45:32.447859Z",
          "shell.execute_reply.started": "2024-11-16T07:45:32.430285Z",
          "shell.execute_reply": "2024-11-16T07:45:32.446811Z"
        },
        "outputId": "8e492bb1-fcb7-4509-9d99-0fccd08123f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 794363904\nTrainable parameters : 45088768\nTrainable percentage: 5.68%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:32.449210Z",
          "iopub.execute_input": "2024-11-16T07:45:32.450171Z",
          "iopub.status.idle": "2024-11-16T07:45:32.455378Z",
          "shell.execute_reply.started": "2024-11-16T07:45:32.450124Z",
          "shell.execute_reply": "2024-11-16T07:45:32.454419Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 2\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:32.456774Z",
          "iopub.execute_input": "2024-11-16T07:45:32.457333Z",
          "iopub.status.idle": "2024-11-16T07:45:32.494294Z",
          "shell.execute_reply.started": "2024-11-16T07:45:32.457288Z",
          "shell.execute_reply": "2024-11-16T07:45:32.493342Z"
        },
        "outputId": "cf2b35fd-5dd9-45b0-e6d5-63cc19b26c69"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 39,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Nov16_07-45-32_13d0209803f8,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:32.501831Z",
          "iopub.execute_input": "2024-11-16T07:45:32.502168Z",
          "iopub.status.idle": "2024-11-16T07:45:33.789880Z",
          "shell.execute_reply.started": "2024-11-16T07:45:32.502135Z",
          "shell.execute_reply": "2024-11-16T07:45:33.788974Z"
        },
        "outputId": "93921255-5761-4d69-8ab4-a990c47c7f99"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 40,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7fce6059ba30>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T07:45:33.791050Z",
          "iopub.execute_input": "2024-11-16T07:45:33.791366Z",
          "iopub.status.idle": "2024-11-16T08:30:23.322946Z",
          "shell.execute_reply.started": "2024-11-16T07:45:33.791331Z",
          "shell.execute_reply": "2024-11-16T08:30:23.322115Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:30:23.324237Z",
          "iopub.execute_input": "2024-11-16T08:30:23.324555Z",
          "iopub.status.idle": "2024-11-16T08:31:53.803513Z",
          "shell.execute_reply.started": "2024-11-16T08:30:23.324522Z",
          "shell.execute_reply": "2024-11-16T08:31:53.802417Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:31:53.804881Z",
          "iopub.execute_input": "2024-11-16T08:31:53.805248Z",
          "iopub.status.idle": "2024-11-16T08:31:55.789064Z",
          "shell.execute_reply.started": "2024-11-16T08:31:53.805214Z",
          "shell.execute_reply": "2024-11-16T08:31:55.788068Z"
        },
        "outputId": "fa42afb9-9a18-4ef8-d56c-db122ba82618"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/50ea995812f20bf680a17a02cfbc4f90ff4d9c0e/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/50ea995812f20bf680a17a02cfbc4f90ff4d9c0e/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:31:55.790434Z",
          "iopub.execute_input": "2024-11-16T08:31:55.790823Z",
          "iopub.status.idle": "2024-11-16T08:31:56.118857Z",
          "shell.execute_reply.started": "2024-11-16T08:31:55.790777Z",
          "shell.execute_reply": "2024-11-16T08:31:56.117961Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:31:56.120208Z",
          "iopub.execute_input": "2024-11-16T08:31:56.120608Z",
          "iopub.status.idle": "2024-11-16T08:31:56.129671Z",
          "shell.execute_reply.started": "2024-11-16T08:31:56.120560Z",
          "shell.execute_reply": "2024-11-16T08:31:56.128724Z"
        },
        "id": "0LCkaOuFYPD7",
        "outputId": "8214f07b-8172-4c2b-d0fc-92d6248c0353"
      },
      "outputs": [
        {
          "execution_count": 45,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:31:56.130833Z",
          "iopub.execute_input": "2024-11-16T08:31:56.131144Z",
          "iopub.status.idle": "2024-11-16T08:31:56.913578Z",
          "shell.execute_reply.started": "2024-11-16T08:31:56.131107Z",
          "shell.execute_reply": "2024-11-16T08:31:56.912537Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "O57aBH1VYPD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:31:56.914885Z",
          "iopub.execute_input": "2024-11-16T08:31:56.915242Z",
          "iopub.status.idle": "2024-11-16T08:32:00.354820Z",
          "shell.execute_reply.started": "2024-11-16T08:31:56.915207Z",
          "shell.execute_reply": "2024-11-16T08:32:00.353788Z"
        },
        "id": "aRCTzNTtYPD7",
        "outputId": "13071e9f-37ab-48b5-915e-d4d8af1007b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/50ea995812f20bf680a17a02cfbc4f90ff4d9c0e/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"unsloth/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/50ea995812f20bf680a17a02cfbc4f90ff4d9c0e/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ]\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Llama-3.2-1B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/50ea995812f20bf680a17a02cfbc4f90ff4d9c0e/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n",
          "output_type": "stream"
        },
        {
          "execution_count": 47,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:32:00.356255Z",
          "iopub.execute_input": "2024-11-16T08:32:00.356977Z",
          "iopub.status.idle": "2024-11-16T08:32:00.368188Z",
          "shell.execute_reply.started": "2024-11-16T08:32:00.356929Z",
          "shell.execute_reply": "2024-11-16T08:32:00.367220Z"
        },
        "id": "kdk_GgfvYPD7",
        "outputId": "9dcea9e7-72d4-4887-e70d-79dbdd94b987"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 749275136\nTrainable parameters : 262735872\nTrainable percentage: 35.07%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:32:00.369889Z",
          "iopub.execute_input": "2024-11-16T08:32:00.370292Z",
          "iopub.status.idle": "2024-11-16T08:32:00.804907Z",
          "shell.execute_reply.started": "2024-11-16T08:32:00.370247Z",
          "shell.execute_reply": "2024-11-16T08:32:00.803847Z"
        },
        "id": "rb1yhWPUYPD8",
        "outputId": "10594323-4b70-4b17-a6bd-830fc1aa1b73"
      },
      "outputs": [
        {
          "execution_count": 49,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 2048)\n        (layers): ModuleList(\n          (0-15): 16 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=512, bias=False)\n                  (default): Linear(in_features=64, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=512, bias=False)\n                  (default): Linear(in_features=64, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=8192, out_features=64, bias=False)\n                  (default): Linear(in_features=8192, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:32:00.806173Z",
          "iopub.execute_input": "2024-11-16T08:32:00.806520Z",
          "iopub.status.idle": "2024-11-16T08:32:00.827543Z",
          "shell.execute_reply.started": "2024-11-16T08:32:00.806487Z",
          "shell.execute_reply": "2024-11-16T08:32:00.826493Z"
        },
        "id": "933a8Tc1YPD8",
        "outputId": "2245a885-329e-482f-eb47-cf0bd95df0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 839452672\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt):\n",
        "  messages = [\n",
        "    {'role' : 'human', 'content' : prompt},\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = 'pt',\n",
        "    truncation = True,\n",
        "    padding = True,\n",
        "  ).to('cuda')\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    input_ids = inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:42:31.110030Z",
          "iopub.execute_input": "2024-11-16T08:42:31.110443Z",
          "iopub.status.idle": "2024-11-16T08:42:31.117981Z",
          "shell.execute_reply.started": "2024-11-16T08:42:31.110403Z",
          "shell.execute_reply": "2024-11-16T08:42:31.116961Z"
        },
        "id": "GPadt5MYYPD8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt):\n",
        "  messages = [\n",
        "    {'role' : 'human', 'content' : prompt},\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = 'pt',\n",
        "    truncation = True,\n",
        "    padding = True,\n",
        "  ).to('cuda')\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    input_ids = inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:42:33.577024Z",
          "iopub.execute_input": "2024-11-16T08:42:33.577710Z",
          "iopub.status.idle": "2024-11-16T08:42:33.585123Z",
          "shell.execute_reply.started": "2024-11-16T08:42:33.577667Z",
          "shell.execute_reply": "2024-11-16T08:42:33.584063Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:42:36.191972Z",
          "iopub.execute_input": "2024-11-16T08:42:36.192370Z",
          "iopub.status.idle": "2024-11-16T08:42:36.200680Z",
          "shell.execute_reply.started": "2024-11-16T08:42:36.192331Z",
          "shell.execute_reply": "2024-11-16T08:42:36.199840Z"
        },
        "id": "SuLf0SO8YPD8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:42:38.232084Z",
          "iopub.execute_input": "2024-11-16T08:42:38.232489Z",
          "iopub.status.idle": "2024-11-16T08:43:46.087546Z",
          "shell.execute_reply.started": "2024-11-16T08:42:38.232442Z",
          "shell.execute_reply": "2024-11-16T08:43:46.086592Z"
        },
        "outputId": "91eb39bf-cfca-49f0-bde2-b07d8dbe6a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n<|begin_of_text|><|start_header_id|>system<|end_he  |  <|begin_of_text|><|start_header_id|>system<|end_he\nader_id|>  Cutting Knowledge Date: December 2023    |  ader_id|>  Cutting Knowledge Date: December 2023  \nToday Date: 16 Nov 2024  <|eot_id|><|start_header_  |  Today Date: 16 Nov 2024  <|eot_id|><|start_header_\nid|>human<|end_header_id|>  What is the observable  |  id|>human<|end_header_id|>  What is the observable\nuniverse, and why does its size not correspond to   |  universe, and why does its size not correspond to \nthe age of the universe multiplied by the speed of  |  the age of the universe multiplied by the speed of\nlight? How does the expansion of the universe       |  light? How does the expansion of the universe     \naffect our perception of its size?<|eot_id|><|star  |  affect our perception of its size?<|eot_id|><|star\nt_header_id|>assistant<|end_header_id|>  The        |  t_header_id|>assistant<|end_header_id|>  The      \nobservable universe is the part of the universe     |  observable universe is the part of the universe   \nthat we can see and observe from Earth. It is the   |  that we can see and observe from Earth. It is the \npart of the universe that is within our cosmic      |  part of the universe that is within our cosmic    \nhorizon, which is the distance light has traveled   |  horizon, which is the distance light has traveled \nsince the Big Bang.  The observable universe is     |  since the Big Bang.  The observable universe is   \nestimated to be around 93 billion light-years in    |  estimated to be around 93 billion light-years in  \ndiameter. However, its size does not correspond to  |  diameter. However, its size does not correspond to\nthe age of the universe multiplied by the speed of  |  the age of the universe multiplied by the speed of\nlight. This is because the universe is expanding,   |  light. This is because the universe is expanding, \nand the expansion is accelerating.  The reason for  |  and the expansion is accelerating.  The reason for\nthis discrepancy is the following:  1. **The speed  |  this discrepancy is the following:  1. **The speed\nof light is constant**: The speed of light is a     |  of light is constant**: The speed of light (c) is \nfundamental constant of the universe, and it        |  a fundamental constant of the universe, and it    \nremains constant over time. However, the expansion  |  remains constant over time. However, the expansion\nof the universe is accelerating, which means that   |  of the universe is accelerating, which means that \nthe distance between galaxies is increasing over    |  the distance between galaxies is increasing over  \ntime. 2. **The universe is expanding**: The         |  time. 2. **The expansion of the universe is not   \nexpansion of the universe is accelerating, which    |  uniform**: The expansion of the universe is not   \nmeans that the distance between galaxies is         |  uniform throughout the universe. Some regions are \nincreasing over time. This is why the observable    |  expanding faster than others, which means that the\nuniverse is not expanding at the same rate as the   |  distance between galaxies is increasing at        \nage of the universe multiplied by the speed of      |  different rates. 3. **The universe is expanding at\nlight. 3. **The universe is not expanding           |  a rate of 67 kilometers per second per            \nuniformly**: The expansion of the universe is not   |  megaparsec**: This is the rate at which the       \nuniform throughout the universe. Some regions are   |  universe is expanding, and it is measured by the  \nexpanding faster than others, which means that the  |  Hubble constant (H0). The Hubble constant is a    \ndistance between galaxies is increasing at          |  measure of the rate at which the universe is      \ndifferent rates.  To illustrate this, consider the  |  expanding, and it is used to calculate the size of\nfollowing example:  Imagine a balloon with a        |  the universe.  To illustrate this, consider the   \nradius of 1 inch. If you were to inflate the        |  following example:  Imagine a balloon with a      \nballoon at a constant rate, the distance between    |  radius of 1 meter. If you were to inflate the     \nthe points where the balloon touches the ground     |  balloon at a rate of 1 meter per second, the      \nwould increase linearly with time. However, if you  |  balloon would expand to a radius of 2 meters in 1 \nwere to inflate the balloon at an accelerating      |  second. However, if you were to inflate the       \nrate, the distance between the points where the     |  balloon at a rate of 2 meters per second, the     \nballoon touches the ground would increase at a      |  balloon would expand to a radius of 4 meters in 1 \nconstant rate, but at a different rate than the     |  second. This is because the expansion of the      \nlinear increase.  This is similar to what is        |  balloon is not uniform, and the distance between  \nhappening with the universe. The expansion of the   |  the points of expansion is increasing.  Similarly,\nuniverse is accelerating, which means that the      |  the observable universe is expanding at a rate of \ndistance between galaxies is increasing at a        |  67 kilometers per second per megaparsec. This     \nconstant rate, but at a different rate than the     |  means that the distance between galaxies is       \nlinear increase.  The expansion of the universe     |  increasing at a rate of 67 kilometers per second  \naffects our perception of its size in several       |  per megaparsec. To calculate the size of the      \nways:  1. **The universe appears to be              |  universe, we need to use the Hubble constant to   \nexpanding**: The expansion of the universe makes    |  convert this rate into a distance per megaparsec. \nthe universe appear to be expanding, which is why   |  Using the Hubble constant, we can calculate the   \nwe can see distant galaxies and other objects. 2.   |  size of the universe as follows:  1 megaparsec =  \n**The universe appears to be getting smaller**:     |  3.26 million light-years 1 light-year = 9.46      \nHowever, the expansion of the universe is also      |  trillion kilometers 1 megaparsec = 3.26 million   \ncausing the universe to appear to be getting        |  light-years 1 light-year = 9.46 trillion          \nsmaller, which is why the cosmic horizon is         |  kilometers  So, the size of the observable        \nexpanding. 3. **The universe appears to be getting  |  universe is approximately 93 billion light-years  \nsmaller at a faster rate**: The expansion of the    |  in diameter. However, its size does not correspond\nuniverse is accelerating, which means that the      |  to the age of the universe multiplied by the speed\nuniverse is getting smaller at a faster rate than   |  of light, because the expansion of the universe is\nthe linear increase would suggest.  In summary,     |  accelerating, and the distance between galaxies is\nthe observable universe is not expanding at the     |  increasing at a rate of 67 kilometers per second  \nsame rate as the age of the universe multiplied by  |  per megaparsec.  In summary, the observable       \nthe speed of light. The expansion of the universe   |  universe is the part of the universe that we can  \nis accelerating, which means that the universe is   |  see and observe from Earth, and its size does not \ngetting smaller at a faster rate than the linear    |  correspond to the age of the universe multiplied  \nincrease would suggest.<|eot_id|>                   |  by the speed of light due to the acceleration of  \n                                                    |  the universe's expansion.<|eot_id|>               \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:49:30.100664Z",
          "iopub.execute_input": "2024-11-16T08:49:30.101477Z",
          "iopub.status.idle": "2024-11-16T08:49:59.414679Z",
          "shell.execute_reply.started": "2024-11-16T08:49:30.101433Z",
          "shell.execute_reply": "2024-11-16T08:49:59.413712Z"
        },
        "outputId": "b2d82e60-0117-495d-e00b-1e50e0137214"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n<|begin_of_text|><|start_header_id|>system<|end_he  |  <|begin_of_text|><|start_header_id|>system<|end_he\nader_id|>  Cutting Knowledge Date: December 2023    |  ader_id|>  Cutting Knowledge Date: December 2023  \nToday Date: 16 Nov 2024  <|eot_id|><|start_header_  |  Today Date: 16 Nov 2024  <|eot_id|><|start_header_\nid|>human<|end_header_id|>  How can I find the      |  id|>human<|end_header_id|>  How can I find the    \ndivision remainder of a number in Python? For       |  division remainder of a number in Python? For     \ninstance, if I have a number like 26 and it is      |  instance, if I have a number like 26 and it is    \ndivided by 7, the division remainder should be 5,   |  divided by 7, the division remainder should be 5, \nas 7+7+7 equals 21 and 26 minus 21 equals 5. How    |  as 7+7+7 equals 21 and 26 minus 21 equals 5. How  \ncan I compute this?<|eot_id|><|start_header_id|>as  |  can I compute this?<|eot_id|><|start_header_id|>as\nsistant<|end_header_id|>  You can use the `divmod`  |  sistant<|end_header_id|>  You can use the `divmod`\nfunction in Python to find the division remainder.  |  function in Python to find the division remainder.\nHere's an example:  ```python def                   |  Here's an example:  ```python def                 \nfind_division_remainder(dividend, divisor):         |  find_division_remainder(dividend, divisor):       \n\"\"\"     Find the division remainder of a number.    |  \"\"\"     Find the division remainder of a number.  \nArgs:         dividend (int): The number being      |  Args:         dividend (int): The number being    \ndivided.         divisor (int): The number by       |  divided.         divisor (int): The number by     \nwhich we are dividing.      Returns:         int:   |  which we are dividing.      Returns:         int: \nThe division remainder.     \"\"\"     return          |  The division remainder.     \"\"\"     return        \ndivmod(dividend, divisor)  # Example usage:         |  divmod(dividend, divisor)  # Example usage:       \ndividend = 26 divisor = 7 remainder =               |  dividend = 26 divisor = 7 remainder =             \nfind_division_remainder(dividend, divisor)          |  find_division_remainder(dividend, divisor)        \nprint(f\"The division remainder of {dividend}        |  print(f\"The division remainder of {dividend}      \ndivided by {divisor} is {remainder}\") ```  In this  |  divided by {divisor} is {remainder}\") ```  In this\ncode, `divmod` is a function that returns a tuple   |  code, `divmod` is a function that returns a tuple \ncontaining the quotient and the remainder. We can   |  containing the quotient and the remainder. We can \nthen use the quotient as the division remainder.    |  then access the remainder using the second element\nAlternatively, you can use the `//` operator to     |  of the tuple.  Alternatively, you can use the `//`\nfind the integer quotient and the `%` operator to   |  operator to find the integer quotient and the `%` \nfind the remainder:  ```python dividend = 26        |  operator to find the remainder:  ```python        \ndivisor = 7 quotient = dividend // divisor          |  dividend = 26 divisor = 7 quotient, remainder =   \nremainder = dividend % divisor print(f\"The          |  divmod(dividend, divisor) print(f\"The division    \ndivision remainder of {dividend} divided by         |  remainder of {dividend} divided by {divisor} is   \n{divisor} is {quotient}\") ```  This approach is     |  {remainder}\") ```  This will give you the same    \nmore efficient and easier to read, especially for   |  result as the previous example.<|eot_id|>         \nlarger numbers.<|eot_id|>                           |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:58:39.768285Z",
          "iopub.execute_input": "2024-11-16T08:58:39.769117Z",
          "iopub.status.idle": "2024-11-16T08:59:45.140092Z",
          "shell.execute_reply.started": "2024-11-16T08:58:39.769068Z",
          "shell.execute_reply": "2024-11-16T08:59:45.139101Z"
        },
        "outputId": "e3510a92-51e2-4dd9-abbf-532665efeb94"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n<|begin_of_text|><|start_header_id|>system<|end_he  |  <|begin_of_text|><|start_header_id|>system<|end_he\nader_id|>  Cutting Knowledge Date: December 2023    |  ader_id|>  Cutting Knowledge Date: December 2023  \nToday Date: 16 Nov 2024  <|eot_id|><|start_header_  |  Today Date: 16 Nov 2024  <|eot_id|><|start_header_\nid|>human<|end_header_id|>  How can we              |  id|>human<|end_header_id|>  How can we            \nscientifically prove that the universe is expandin  |  scientifically prove that the universe is expandin\ng?<|eot_id|><|start_header_id|>assistant<|end_head  |  g?<|eot_id|><|start_header_id|>assistant<|end_head\ner_id|>  To scientifically prove that the universe  |  er_id|>  To scientifically prove that the universe\nis expanding, we can use a combination of           |  is expanding, we can use a combination of         \nobservations, experiments, and theoretical models.  |  observations, experiments, and theoretical models.\nHere's a step-by-step approach:  1. **Redshift of   |  Here's a step-by-step approach:  1. **Redshift of \nlight from distant galaxies**: In the 1920s, Edwin  |  light from distant galaxies**: In the 1920s, Edwin\nHubble discovered that the light emitted by         |  Hubble discovered that the light emitted by       \ndistant galaxies is shifted towards the red end of  |  distant galaxies is shifted towards the red end of\nthe spectrum, a phenomenon known as redshift. This  |  the spectrum, a phenomenon known as redshift. This\nis due to the expansion of the universe. 2.         |  is due to the expansion of the universe. 2.       \n**Cosmic Microwave Background (CMB) radiation**:    |  **Cosmic Microwave Background (CMB) radiation**:  \nThe CMB is the residual radiation from the Big      |  The CMB is the residual radiation from the Big    \nBang. Its uniformity and homogeneity provide        |  Bang. Its uniformity and homogeneity provide      \nstrong evidence for the universe's expansion. 3.    |  strong evidence for the universe's expansion. 3.  \n**Baryon Acoustic Oscillation (BAO)**: BAO is a     |  **Baryon Acoustic Oscillation (BAO)**: BAO is a   \nfeature of the large-scale structure of the         |  feature of the large-scale structure of the       \nuniverse. It is a characteristic of the             |  universe. It is a characteristic of the           \ndistribution of galaxies and galaxy clusters on     |  distribution of galaxies and galaxy clusters on   \nlarge scales. The BAO scale is a measure of the     |  large scales. The BAO scale is a measure of the   \ndistance between galaxies, which is consistent      |  distance between galaxies, which is consistent    \nwith the universe's expansion. 4. **Supernovae      |  with the universe's expansion. 4. **Supernovae    \nobservations**: Supernovae are explosive events     |  observations**: Supernovae are explosive events   \nthat occur when a star runs out of fuel. By         |  that occur when a star runs out of fuel. By       \nobserving the light curves of supernovae,           |  observing the light curves of supernovae,         \nscientists can measure the expansion history of     |  scientists can measure the expansion history of   \nthe universe. 5. **Gravitational lensing**: The     |  the universe. 5. **Gravitational lensing**: The   \nbending of light around massive objects, such as    |  bending of light around massive objects, such as  \ngalaxies and galaxy clusters, can be used to        |  galaxies and galaxy clusters, can be used to      \nmeasure the expansion of the universe. 6. **Hubble  |  measure the expansion of the universe. 6. **Hubble\nSpace Telescope observations**: The HST has         |  Space Telescope observations**: The HST has       \nprovided a wealth of data on the universe's         |  provided a wealth of data on the universe's       \nexpansion, including the redshift of light from     |  expansion, including the redshift of light from   \ndistant galaxies, the BAO scale, and the CMB        |  distant galaxies, the BAO scale, and the CMB      \nradiation. 7. **Simulations and models**:           |  radiation. 7. **Simulations and models**:         \nScientists use computer simulations and models to   |  Scientists use computer simulations and models to \npredict the expansion history of the universe.      |  predict the expansion history of the universe.    \nThese models are based on the laws of physics,      |  These models are based on the laws of physics,    \nsuch as general relativity and the theory of        |  such as general relativity and the theory of      \ngravity.  To prove the expansion of the universe,   |  gravity.  By combining these lines of evidence,   \nscientists use a combination of observations and    |  scientists have established that the universe is  \nexperiments, including:  * **Redshift               |  expanding. The evidence is based on observations  \nmeasurements**: By measuring the redshift of light  |  of the universe's expansion, including:  * The    \nfrom distant galaxies, scientists can determine     |  redshift of light from distant galaxies * The BAO \nthe expansion history of the universe. * **Baryon   |  scale * The CMB radiation * Supernovae            \nAcoustic Oscillation (BAO) measurements**: By       |  observations * Gravitational lensing * Hubble     \nmeasuring the BAO scale, scientists can determine   |  Space Telescope observations  The evidence for the\nthe expansion history of the universe. *            |  universe's expansion is strong, and it is widely  \n**Supernovae observations**: By observing the       |  accepted by the scientific community.  **Timeline \nlight curves of supernovae, scientists can measure  |  of evidence**:  * 1920s: Edwin Hubble discovers   \nthe expansion history of the universe. *            |  the redshift of light from distant galaxies *     \n**Gravitational lensing**: By measuring the         |  1960s: Baryon Acoustic Oscillation (BAO) is       \nbending of light around massive objects,            |  discovered * 1990s: Supernovae observations       \nscientists can measure the expansion of the         |  provide evidence for the expansion history of the \nuniverse. * **Simulations and models**: By using    |  universe * 2000s: Gravitational lensing and Hubble\ncomputer simulations and models, scientists can     |  Space Telescope observations provide further      \npredict the expansion history of the universe.  By  |  evidence for the expansion of the universe *      \ncombining these observations and experiments,       |  2010s: Simulations and models predict the         \nscientists have strong evidence for the expansion   |  expansion history of the universe  **Conclusion**:\nof the universe.<|eot_id|>                          |  The evidence for the universe's expansion is based\n                                                    |  on observations of the universe's expansion,      \n                                                    |  including the redshift of light from distant      \n                                                    |  galaxies, the BAO scale, the CMB radiation,       \n                                                    |  supernovae observations, gravitational lensing,   \n                                                    |  and Hubble Space Telescope observations. The      \n                                                    |  evidence is strong, and it is widely accepted by  \n                                                    |  the scientific community.<|eot_id|>               \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:55:09.525101Z",
          "iopub.execute_input": "2024-11-16T08:55:09.525405Z",
          "iopub.status.idle": "2024-11-16T08:55:50.449442Z",
          "shell.execute_reply.started": "2024-11-16T08:55:09.525372Z",
          "shell.execute_reply": "2024-11-16T08:55:50.448414Z"
        },
        "outputId": "3138135f-97bc-468f-a5e9-e37b6bc28f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n<|begin_of_text|><|start_header_id|>system<|end_he  |  <|begin_of_text|><|start_header_id|>system<|end_he\nader_id|>  Cutting Knowledge Date: December 2023    |  ader_id|>  Cutting Knowledge Date: December 2023  \nToday Date: 16 Nov 2024  <|eot_id|><|start_header_  |  Today Date: 16 Nov 2024  <|eot_id|><|start_header_\nid|>human<|end_header_id|>  Describe the structure  |  id|>human<|end_header_id|>  Describe the structure\nof an atom.<|eot_id|><|start_header_id|>assistant<  |  of an atom.<|eot_id|><|start_header_id|>assistant<\n|end_header_id|>  An atom is the basic building     |  |end_header_id|>  An atom is the basic building   \nblock of matter. It consists of three main parts:   |  block of matter. It consists of three main parts: \nprotons, neutrons, and electrons.  1. **Protons**:  |  protons, neutrons, and electrons.  1. **Protons**:\nThe number of protons in an atom determines the     |  The number of protons in an atom determines the   \nelement of an atom. Protons are positively charged  |  element of an atom. Protons are positively charged\nparticles that reside in the nucleus of the atom.   |  particles that reside in the nucleus of the atom. \n2. **Neutrons**: Neutrons are neutral particles     |  2. **Neutrons**: Neutrons are neutral particles   \nthat reside in the nucleus of the atom. They have   |  that reside in the nucleus of the atom. They have \nno charge and contribute to the mass of the atom.   |  no charge and contribute to the mass of the atom. \n3. **Electrons**: Electrons are negatively charged  |  3. **Electrons**: Electrons are negatively charged\nparticles that orbit the nucleus of the atom. They  |  particles that orbit the nucleus of the atom. They\nare attracted to the protons in the nucleus and     |  are attracted to the protons in the nucleus and   \nare arranged in energy levels or shells around the  |  are arranged in energy levels or shells around the\nnucleus.  The structure of an atom can be           |  nucleus.  The structure of an atom can be         \nrepresented as follows:  * **Nucleus**: The center  |  represented as follows:  * **Nucleus**: The center\nof the atom, where the protons and neutrons         |  of the atom, where the protons and neutrons       \nreside. * **Proton-Neutron Cloud**: The region      |  reside. * **Proton-Neutron Cloud**: The region    \naround the nucleus where the protons and neutrons   |  around the nucleus where the protons and neutrons \nreside. * **Electron Cloud**: The region around     |  reside. * **Electron Cloud**: The region around   \nthe nucleus where the electrons reside.  The        |  the nucleus where the electrons reside.  The      \nnumber of protons in an atom determines the         |  number of protons in an atom determines the       \nelement of the atom. For example:  * **Hydrogen     |  element of the atom. For example:  * **Hydrogen   \n(H)**: 1 proton, 0 neutrons, 1 electron * **Helium  |  (H)**: 1 proton, 0 neutrons, 1 electron * **Helium\n(He)**: 2 protons, 2 neutrons, 2 electrons *        |  (He)**: 2 protons, 2 neutrons, 2 electrons *      \n**Lithium (Li)**: 3 protons, 0 neutrons, 3          |  **Lithium (Li)**: 3 protons, 0 neutrons, 3        \nelectrons  The number of neutrons in an atom can    |  electrons  The number of neutrons in an atom can  \nvary, and the number of electrons can also vary     |  vary, and the number of electrons can also vary   \ndepending on the element. The number of protons     |  depending on the element. The number of protons   \ndetermines the identity of the element.  In         |  determines the identity of the element.  In       \nsummary, the structure of an atom consists of a     |  summary, the structure of an atom consists of a   \nnucleus, a proton-neutron cloud, and an electron    |  nucleus, a proton-neutron cloud, and an electron  \ncloud. The number of protons, neutrons, and         |  cloud. The number of protons, neutrons, and       \nelectrons determines the identity of the            |  electrons determines the identity of the          \nelement.<|eot_id|>                                  |  element.<|eot_id|>                                \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-16T08:46:16.544086Z",
          "iopub.execute_input": "2024-11-16T08:46:16.544755Z",
          "iopub.status.idle": "2024-11-16T08:47:11.770118Z",
          "shell.execute_reply.started": "2024-11-16T08:46:16.544707Z",
          "shell.execute_reply": "2024-11-16T08:47:11.769195Z"
        },
        "outputId": "24911926-69fe-4079-d353-72c3269e513e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n<|begin_of_text|><|start_header_id|>system<|end_he  |  <|begin_of_text|><|start_header_id|>system<|end_he\nader_id|>  Cutting Knowledge Date: December 2023    |  ader_id|>  Cutting Knowledge Date: December 2023  \nToday Date: 16 Nov 2024  <|eot_id|><|start_header_  |  Today Date: 16 Nov 2024  <|eot_id|><|start_header_\nid|>human<|end_header_id|>  How can chemistry       |  id|>human<|end_header_id|>  How can chemistry     \nstudents effectively learn and apply experimental   |  students effectively learn and apply experimental \ntechniques when faced with limited opportunities    |  techniques when faced with limited opportunities  \nfor hands-on experimentation in the classroom sett  |  for hands-on experimentation in the classroom sett\ning?<|eot_id|><|start_header_id|>assistant<|end_he  |  ing?<|eot_id|><|start_header_id|>assistant<|end_he\nader_id|>  Chemistry students can still             |  ader_id|>  Chemistry students can still           \neffectively learn and apply experimental            |  effectively learn and apply experimental          \ntechniques even with limited opportunities for      |  techniques even with limited opportunities for    \nhands-on experimentation in the classroom setting.  |  hands-on experimentation in the classroom setting.\nHere are some strategies to help:  1. **Simulate    |  Here are some strategies to help:  1. **Virtual   \nexperiments**: Use theoretical models, computer     |  labs**: Utilize online platforms, such as LabPad, \nsimulations, or laboratory simulations to           |  LabVIEW, or online chemistry labs, to conduct     \ndemonstrate experimental techniques. This can help  |  experiments remotely. These platforms often       \nstudents understand the underlying principles and   |  provide pre-recorded experiments, simulations, and\nconcepts without the need for actual experiments.   |  virtual labs that can be used to learn and apply  \n2. **Use online resources**: Utilize online         |  experimental techniques. 2. **Simulation          \nresources, such as video lectures, tutorials, and   |  software**: Leverage simulation software, like    \ninteractive simulations, to learn about             |  ChemDraw, Chem3D, or Molecular Simulator, to model\nexperimental techniques. These resources can        |  and simulate chemical reactions, molecular        \nprovide a comprehensive overview of the subject     |  interactions, and other phenomena. These tools can\nwithout requiring hands-on experimentation. 3.      |  help students understand complex concepts and     \n**Focus on theoretical foundations**: Emphasize     |  apply theoretical knowledge to real-world         \nthe theoretical foundations of experimental         |  scenarios. 3. **Pre-recorded experiments**: Use   \ntechniques, such as the laws of thermodynamics,     |  pre-recorded experiments, such as those found on  \nkinetics, and chemical reactions. This can help     |  YouTube, Khan Academy, or online chemistry        \nstudents understand the underlying principles and   |  courses, to learn and apply experimental          \nconcepts without needing to perform experiments.    |  techniques. These resources can provide a wealth  \n4. **Practice with simulations and models**: Use    |  of information on various topics, including       \nlaboratory simulations, such as the LabVIEW or      |  experimental design, data analysis, and           \nLabVIEW-like software, to practice experimental     |  experimental safety. 4. **Peer-to-peer learning**:\ntechniques. These simulations can provide a safe    |  Encourage students to share their experiences, ask\nand controlled environment for students to          |  questions, and learn from each other. This can    \npractice and experiment. 5. **Collaborate with      |  help students develop problem-solving skills,     \npeers**: Encourage students to work in groups to    |  critical thinking, and collaboration. 5. **Theory-\ndesign and conduct experiments. This can help them  |  based learning**: Focus on theoretical concepts   \nlearn from each other and apply experimental        |  and principles, which can be learned through      \ntechniques in a more practical setting. 6. **Use    |  lectures, readings, and online resources. This can\nvirtual labs**: Utilize virtual labs, such as the   |  help students understand the underlying mechanisms\nVirtual Lab or Labster, which offer interactive     |  and relationships between different chemical      \nsimulations and experiments. These virtual labs     |  concepts. 6. **Hands-on simulations**: Use hands- \ncan provide a safe and controlled environment for   |  on simulations, such as virtual labs or           \nstudents to practice and experiment. 7. **Focus on  |  interactive simulations, to learn and apply       \ntheoretical applications**: Emphasize the           |  experimental techniques. These simulations can    \ntheoretical applications of experimental            |  provide a safe and controlled environment for     \ntechniques, such as the design of experiments,      |  students to experiment and learn. 7.              \ndata analysis, and interpretation of results. This  |  **Collaborative projects**: Encourage students to \ncan help students understand the practical aspects  |  work in groups on collaborative projects, which   \nof experimental techniques without needing to       |  can help them apply experimental techniques to    \nperform experiments. 8. **Provide opportunities     |  real-world scenarios and develop problem-solving  \nfor hands-on experimentation**: While it may not    |  skills. 8. **Guest lectures and workshops**:      \nbe possible to conduct hands-on experiments in the  |  Invite guest lecturers or participate in workshops\nclassroom setting, provide opportunities for        |  to learn from experts in the field. These can     \nstudents to participate in hands-on experiments or  |  provide valuable insights and hands-on experience \nlab simulations through guest lectures, workshops,  |  with experimental techniques. 9. **Online         \nor field trips. 9. **Encourage critical thinking    |  communities**: Foster online communities, such as \nand problem-solving**: Encourage students to think  |  forums or social media groups, where students can \ncritically and solve problems using experimental    |  discuss their experiments, ask questions, and     \ntechniques. This can help them develop essential    |  share their experiences. 10. **Assessment and     \nskills in experimental design, data analysis, and   |  feedback**: Regularly assess and provide feedback \ninterpretation of results. 10. **Use technology to  |  on students' experiments and projects. This can   \nfacilitate learning**: Leverage technology, such    |  help students identify areas for improvement and  \nas online platforms, mobile apps, or educational    |  develop their experimental skills.  By using these\nsoftware, to facilitate learning and                |  strategies, chemistry students can effectively    \nexperimentation. These tools can provide a range    |  learn and apply experimental techniques, even with\nof interactive simulations, experiments, and        |  limited opportunities for hands-on experimentation\nresources to support student learning.  By using    |  in the classroom setting.<|eot_id|>               \nthese strategies, chemistry students can still      |                                                    \neffectively learn and apply experimental            |                                                    \ntechniques, even with limited opportunities for     |                                                    \nhands-on experimentation in the classroom           |                                                    \nsetting.<|eot_id|>                                  |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}