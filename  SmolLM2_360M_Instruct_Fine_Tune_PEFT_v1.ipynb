{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/%20SmolLM2_360M_Instruct_Fine_Tune_PEFT_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:37:07.113722Z",
          "iopub.execute_input": "2024-11-17T00:37:07.114598Z",
          "iopub.status.idle": "2024-11-17T00:37:53.913383Z",
          "shell.execute_reply.started": "2024-11-17T00:37:07.114555Z",
          "shell.execute_reply": "2024-11-17T00:37:53.912117Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-11-17T00:37:53.916109Z",
          "iopub.execute_input": "2024-11-17T00:37:53.916850Z",
          "iopub.status.idle": "2024-11-17T00:38:02.338827Z",
          "shell.execute_reply.started": "2024-11-17T00:37:53.916802Z",
          "shell.execute_reply": "2024-11-17T00:38:02.337839Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:02.340256Z",
          "iopub.execute_input": "2024-11-17T00:38:02.340573Z",
          "iopub.status.idle": "2024-11-17T00:38:02.348129Z",
          "shell.execute_reply.started": "2024-11-17T00:38:02.340536Z",
          "shell.execute_reply": "2024-11-17T00:38:02.346944Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n",
        "#model_name = url.split('.co/')[-1]\n",
        "\n",
        "model_name = 'HuggingFaceTB/SmolLM2-360M-Instruct'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:02.349579Z",
          "iopub.execute_input": "2024-11-17T00:38:02.350002Z",
          "iopub.status.idle": "2024-11-17T00:38:02.362677Z",
          "shell.execute_reply.started": "2024-11-17T00:38:02.349937Z",
          "shell.execute_reply": "2024-11-17T00:38:02.361929Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "  if base == True:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype = torch.float16,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "  else:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = True,\n",
        "      bnb_4bit_quant_type = 'nf4',\n",
        "      bnb_4bit_compute_dtype = torch.float16,\n",
        "      bnb_4bit_use_double_quant = True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config = bnb_config,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:02.365084Z",
          "iopub.execute_input": "2024-11-17T00:38:02.365367Z",
          "iopub.status.idle": "2024-11-17T00:38:02.373488Z",
          "shell.execute_reply.started": "2024-11-17T00:38:02.365336Z",
          "shell.execute_reply": "2024-11-17T00:38:02.372653Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:02.374434Z",
          "iopub.execute_input": "2024-11-17T00:38:02.374704Z",
          "iopub.status.idle": "2024-11-17T00:38:22.402990Z",
          "shell.execute_reply.started": "2024-11-17T00:38:02.374674Z",
          "shell.execute_reply": "2024-11-17T00:38:22.402063Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:22.404144Z",
          "iopub.execute_input": "2024-11-17T00:38:22.404459Z",
          "iopub.status.idle": "2024-11-17T00:38:22.414937Z",
          "shell.execute_reply.started": "2024-11-17T00:38:22.404424Z",
          "shell.execute_reply": "2024-11-17T00:38:22.414113Z"
        },
        "outputId": "53167b1f-26e9-46d2-d3f9-b3eecf20368b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 204534720\nTrainable parameters : 47248320\nTrainable percentage: 23.10%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:22.416119Z",
          "iopub.execute_input": "2024-11-17T00:38:22.416442Z",
          "iopub.status.idle": "2024-11-17T00:38:24.608610Z",
          "shell.execute_reply.started": "2024-11-17T00:38:22.416409Z",
          "shell.execute_reply": "2024-11-17T00:38:24.607814Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n",
        "#dataset_name = url.split('datasets/')[-1]\n",
        "\n",
        "dataset_name = 'mlabonne/FineTome-100k'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:24.609723Z",
          "iopub.execute_input": "2024-11-17T00:38:24.610068Z",
          "iopub.status.idle": "2024-11-17T00:38:24.614448Z",
          "shell.execute_reply.started": "2024-11-17T00:38:24.610033Z",
          "shell.execute_reply": "2024-11-17T00:38:24.613516Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1024"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:24.615558Z",
          "iopub.execute_input": "2024-11-17T00:38:24.615854Z",
          "iopub.status.idle": "2024-11-17T00:38:24.624067Z",
          "shell.execute_reply.started": "2024-11-17T00:38:24.615822Z",
          "shell.execute_reply": "2024-11-17T00:38:24.623164Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:24.625093Z",
          "iopub.execute_input": "2024-11-17T00:38:24.625796Z",
          "iopub.status.idle": "2024-11-17T00:38:26.557805Z",
          "shell.execute_reply.started": "2024-11-17T00:38:24.625750Z",
          "shell.execute_reply": "2024-11-17T00:38:26.556838Z"
        },
        "outputId": "60afe21b-0e17-4efb-b7a0-87596141348d"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['conversations', 'source', 'score'],\n    num_rows: 100000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(10000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.559026Z",
          "iopub.execute_input": "2024-11-17T00:38:26.559355Z",
          "iopub.status.idle": "2024-11-17T00:38:26.566372Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.559319Z",
          "shell.execute_reply": "2024-11-17T00:38:26.565646Z"
        },
        "id": "MNRvqTeCUk9U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.567478Z",
          "iopub.execute_input": "2024-11-17T00:38:26.567814Z",
          "iopub.status.idle": "2024-11-17T00:38:26.595744Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.567766Z",
          "shell.execute_reply": "2024-11-17T00:38:26.594974Z"
        },
        "outputId": "9486e9fb-7d51-43ec-91c4-a263f0575980"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                       conversations  \\\n0  [{'from': 'human', 'value': 'Explain what bool...   \n1  [{'from': 'human', 'value': 'Explain how recur...   \n2  [{'from': 'human', 'value': 'Explain what bool...   \n3  [{'from': 'human', 'value': 'Explain the conce...   \n4  [{'from': 'human', 'value': 'Print the reverse...   \n\n                     source     score  \n0  infini-instruct-top-500k  5.212621  \n1  infini-instruct-top-500k  5.157649  \n2  infini-instruct-top-500k  5.147540  \n3  infini-instruct-top-500k  5.053656  \n4  infini-instruct-top-500k  5.045648  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conversations</th>\n      <th>source</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.212621</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[{'from': 'human', 'value': 'Explain how recur...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.157649</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.147540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[{'from': 'human', 'value': 'Explain the conce...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.053656</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[{'from': 'human', 'value': 'Print the reverse...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.045648</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.601070Z",
          "iopub.execute_input": "2024-11-17T00:38:26.601353Z",
          "iopub.status.idle": "2024-11-17T00:38:26.608369Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.601321Z",
          "shell.execute_reply": "2024-11-17T00:38:26.607425Z"
        },
        "outputId": "3edba36d-6a7f-4977-98fd-507e6112ac9f"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'conversations': [{'from': 'human',\n   'value': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.'},\n  {'from': 'gpt',\n   'value': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.'}],\n 'source': 'infini-instruct-top-500k',\n 'score': 5.212620735168457}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.609367Z",
          "iopub.execute_input": "2024-11-17T00:38:26.609653Z",
          "iopub.status.idle": "2024-11-17T00:38:26.617514Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.609621Z",
          "shell.execute_reply": "2024-11-17T00:38:26.616677Z"
        },
        "outputId": "279f9c0a-a830-4e37-945d-b7052e86919c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['conversations', 'source', 'score']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_conversations(example):\n",
        "  role_map = {\n",
        "    'human' : 'user',\n",
        "    'gpt' : 'assistant'\n",
        "  }\n",
        "\n",
        "  transformed_conversations = [\n",
        "    {\n",
        "      'role' : role_map.get(turn['from'], turn['from']),\n",
        "      'content' : turn['value']\n",
        "    }\n",
        "    for turn in example['conversations']\n",
        "  ]\n",
        "  return {'conversations': transformed_conversations}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.618782Z",
          "iopub.execute_input": "2024-11-17T00:38:26.619207Z",
          "iopub.status.idle": "2024-11-17T00:38:26.627242Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.619159Z",
          "shell.execute_reply": "2024-11-17T00:38:26.626450Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(transform_conversations, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.628484Z",
          "iopub.execute_input": "2024-11-17T00:38:26.629208Z",
          "iopub.status.idle": "2024-11-17T00:38:26.641496Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.629157Z",
          "shell.execute_reply": "2024-11-17T00:38:26.640611Z"
        },
        "outputId": "34a68cf6-6ff3-48dd-ebdb-d023553b57c2"
      },
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['conversations'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['conversations'])"
      ],
      "metadata": {
        "id": "cZya4tPEWUc9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.642805Z",
          "iopub.execute_input": "2024-11-17T00:38:26.643276Z",
          "iopub.status.idle": "2024-11-17T00:38:26.648696Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.643230Z",
          "shell.execute_reply": "2024-11-17T00:38:26.647863Z"
        },
        "outputId": "0a31efb2-5697-41bf-b091-d4670b748834"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[{'content': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.', 'role': 'user'}, {'content': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.', 'role': 'assistant'}]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_conversation(example):\n",
        "  for entry in example['conversations']:\n",
        "    role = entry['role']\n",
        "    content = entry['content']\n",
        "\n",
        "    if role == 'user':\n",
        "      formatted_text = f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n",
        "    elif role == 'assistant':\n",
        "      formatted_text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n",
        "\n",
        "  return {'prompt': formatted_text}"
      ],
      "metadata": {
        "id": "3bmXGueQWWzt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.649879Z",
          "iopub.execute_input": "2024-11-17T00:38:26.650297Z",
          "iopub.status.idle": "2024-11-17T00:38:26.657976Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.650264Z",
          "shell.execute_reply": "2024-11-17T00:38:26.657132Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = formatted_dataset.map(\n",
        "  format_conversation,\n",
        "  remove_columns = list(formatted_dataset.features.keys())\n",
        ")\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "Z6sSaCr5eaL7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.659031Z",
          "iopub.execute_input": "2024-11-17T00:38:26.659322Z",
          "iopub.status.idle": "2024-11-17T00:38:26.674151Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.659290Z",
          "shell.execute_reply": "2024-11-17T00:38:26.673255Z"
        },
        "outputId": "7a32b2c7-6294-40e0-bfd0-fd76baf601e5"
      },
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.675203Z",
          "iopub.execute_input": "2024-11-17T00:38:26.675494Z",
          "iopub.status.idle": "2024-11-17T00:38:26.683278Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.675462Z",
          "shell.execute_reply": "2024-11-17T00:38:26.682360Z"
        },
        "outputId": "80747c1b-77c5-4592-cf4e-a5d34f1e0953"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.684436Z",
          "iopub.execute_input": "2024-11-17T00:38:26.684808Z",
          "iopub.status.idle": "2024-11-17T00:38:26.691285Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.684765Z",
          "shell.execute_reply": "2024-11-17T00:38:26.690372Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:26.692503Z",
          "iopub.execute_input": "2024-11-17T00:38:26.693371Z",
          "iopub.status.idle": "2024-11-17T00:38:37.208683Z",
          "shell.execute_reply.started": "2024-11-17T00:38:26.693337Z",
          "shell.execute_reply": "2024-11-17T00:38:37.207702Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.209832Z",
          "iopub.execute_input": "2024-11-17T00:38:37.210184Z",
          "iopub.status.idle": "2024-11-17T00:38:37.216757Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.210148Z",
          "shell.execute_reply": "2024-11-17T00:38:37.215768Z"
        },
        "outputId": "5b6d5d8e-283c-497f-942b-ff90538d82fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.218111Z",
          "iopub.execute_input": "2024-11-17T00:38:37.218883Z",
          "iopub.status.idle": "2024-11-17T00:38:37.305480Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.218837Z",
          "shell.execute_reply": "2024-11-17T00:38:37.304412Z"
        },
        "outputId": "ad51c781-f11e-4710-8141-973486f793be"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.306820Z",
          "iopub.execute_input": "2024-11-17T00:38:37.307219Z",
          "iopub.status.idle": "2024-11-17T00:38:37.313426Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.307175Z",
          "shell.execute_reply": "2024-11-17T00:38:37.312477Z"
        },
        "outputId": "e266b83d-d801-4e35-a1a5-c4f57f762311"
      },
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.314588Z",
          "iopub.execute_input": "2024-11-17T00:38:37.314883Z",
          "iopub.status.idle": "2024-11-17T00:38:37.342212Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.314851Z",
          "shell.execute_reply": "2024-11-17T00:38:37.341341Z"
        },
        "outputId": "ff173245-7ed4-4be0-f96a-d1b18b1d162d"
      },
      "outputs": [
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n1  <|start_header_id|>user<|end_header_id|>\\n\\nWr...   \n2  <|start_header_id|>user<|end_header_id|>\\n\\nWh...   \n3  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n4  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n\n                                           input_ids  \\\n0  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n1  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n2  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n3  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n4  [44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWr...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.343267Z",
          "iopub.execute_input": "2024-11-17T00:38:37.343553Z",
          "iopub.status.idle": "2024-11-17T00:38:37.349754Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.343521Z",
          "shell.execute_reply": "2024-11-17T00:38:37.348785Z"
        },
        "outputId": "022ffb94-00db-423d-ab41-a83bb9f12bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|start_header_id|>user<|end_header_id|>\n\nHow do you graph the inequality #x > -4#?\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nTo graph the inequality #x > -4#, follow these steps:\n\n1. Start by plotting the boundary line: Draw a vertical line at #x = -4# on the X-axis. This line divides the X-axis into two halves.\n\n2. Identify the direction of the inequality: Since the inequality is #x > -4#, it indicates that we are looking for all values of #x# that are greater than -4.\n\n3. Shade the appropriate region: Shade the entire region to the right of the line at #x = -4#. This shaded area represents all the points whose X-coordinate is greater than -4.\n\n4. Include a dashed line: Since the inequality is \"greater than\" (#>,# not \"##\"), draw the line at #x = -4# as a dashed line, indicating that the points on the line itself are not included in the solution.\n\nThe graph consists of the entire half-plane to the right of the dashed vertical line at #x = -4#, excluding the line itself.\n<|eot_id|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.351020Z",
          "iopub.execute_input": "2024-11-17T00:38:37.351335Z",
          "iopub.status.idle": "2024-11-17T00:38:37.360645Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.351304Z",
          "shell.execute_reply": "2024-11-17T00:38:37.359775Z"
        },
        "outputId": "db283d13-1584-4d18-ae78-6ca451726732"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[44, 108, 3738, 79, 12476, 79, 311, 108, 46, 4093, 44, 108, 486, 79, 12476, 79, 311, 108, 46, 198, 198, 2020, 536, 346, 3670, 260, 9724, 907, 104, 2986, 731, 36, 19, 47, 198, 44, 108, 85, 320, 79, 311, 108, 21198, 108, 3738, 79, 12476, 79, 311, 108, 46, 520, 9531, 44, 108, 486, 79, 12476, 79, 311, 108, 46, 198, 198, 2068, 3670, 260, 9724, 907, 104, 2986, 731, 36, 19, 28, 1066, 623, 3301, 42, 1116, 33, 30, 7734, 411, 27566, 260, 10651, 1761, 42, 10602, 253, 8681, 1761, 418, 907, 104, 446, 731, 36, 19, 335, 260, 2273, 29, 7814, 30, 669, 1761, 20751, 260, 2273, 29, 7814, 618, 827, 32386, 30, 1116, 34, 30, 12716, 260, 4376, 282, 260, 9724, 42, 4311, 260, 9724, 314, 907, 104, 2986, 731, 36, 19, 28, 357, 6773, 338, 392, 359, 3012, 327, 511, 2396, 282, 907, 104, 19, 338, 359, 2852, 670, 731, 36, 30, 1116, 35, 30, 1541, 903, 260, 3350, 2695, 42, 1541, 903, 260, 2415, 2695, 288, 260, 1048, 282, 260, 1761, 418, 907, 104, 446, 731, 36, 19, 30, 669, 32416, 1557, 4669, 511, 260, 2876, 3449, 2273, 29, 38263, 314, 2852, 670, 731, 36, 30, 1116, 36, 30, 26852, 253, 44613, 1761, 42, 4311, 260, 9724, 314, 476, 41773, 670, 18, 38078, 18128, 19, 441, 28096, 40533, 115, 19, 6872, 2634, 260, 1761, 418, 907, 104, 446, 731, 36, 19, 347, 253, 44613, 1761, 28, 10487, 338, 260, 2876, 335, 260, 1761, 2581, 359, 441, 3249, 281, 260, 3564, 30, 198, 198, 504, 3670, 5956, 282, 260, 2415, 2745, 29, 12758, 288, 260, 1048, 282, 260, 44613, 8681, 1761, 418, 907, 104, 446, 731, 36, 19, 28, 26827, 260, 1761, 2581, 30, 198, 44, 108, 85, 320, 79, 311, 108, 46, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.361830Z",
          "iopub.execute_input": "2024-11-17T00:38:37.362560Z",
          "iopub.status.idle": "2024-11-17T00:38:37.371855Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.362524Z",
          "shell.execute_reply": "2024-11-17T00:38:37.370960Z"
        },
        "outputId": "c7ec6f13-a176-4f92-9be8-6579b3461765"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.373249Z",
          "iopub.execute_input": "2024-11-17T00:38:37.373866Z",
          "iopub.status.idle": "2024-11-17T00:38:37.382191Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.373817Z",
          "shell.execute_reply": "2024-11-17T00:38:37.381292Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.383398Z",
          "iopub.execute_input": "2024-11-17T00:38:37.383719Z",
          "iopub.status.idle": "2024-11-17T00:38:37.391169Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.383686Z",
          "shell.execute_reply": "2024-11-17T00:38:37.390397Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.392232Z",
          "iopub.execute_input": "2024-11-17T00:38:37.392547Z",
          "iopub.status.idle": "2024-11-17T00:38:37.400167Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.392508Z",
          "shell.execute_reply": "2024-11-17T00:38:37.399288Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.401394Z",
          "iopub.execute_input": "2024-11-17T00:38:37.401898Z",
          "iopub.status.idle": "2024-11-17T00:38:37.408707Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.401854Z",
          "shell.execute_reply": "2024-11-17T00:38:37.407927Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:37.409850Z",
          "iopub.execute_input": "2024-11-17T00:38:37.410262Z",
          "iopub.status.idle": "2024-11-17T00:38:38.158555Z",
          "shell.execute_reply.started": "2024-11-17T00:38:37.410229Z",
          "shell.execute_reply": "2024-11-17T00:38:38.157586Z"
        },
        "outputId": "3c34b5ea-e531-4cfa-c9f9-fde935d3f4b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 34,734,080 || all params: 396,555,200 || trainable%: 8.7590\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ikF6Yfkz1myd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:38.159943Z",
          "iopub.execute_input": "2024-11-17T00:38:38.160297Z",
          "iopub.status.idle": "2024-11-17T00:38:38.183459Z",
          "shell.execute_reply.started": "2024-11-17T00:38:38.160261Z",
          "shell.execute_reply": "2024-11-17T00:38:38.182499Z"
        },
        "outputId": "6c4a1427-930f-4804-e9f1-8ba28aafad43"
      },
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=320, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=320, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2560, out_features=960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2560, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((960,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:38.184683Z",
          "iopub.execute_input": "2024-11-17T00:38:38.184988Z",
          "iopub.status.idle": "2024-11-17T00:38:38.210399Z",
          "shell.execute_reply.started": "2024-11-17T00:38:38.184956Z",
          "shell.execute_reply": "2024-11-17T00:38:38.209342Z"
        },
        "outputId": "44ede333-9f5c-4cd2-e8d4-67891304ea6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 239268800\nTrainable parameters : 34734080\nTrainable percentage: 14.52%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:38.211626Z",
          "iopub.execute_input": "2024-11-17T00:38:38.212004Z",
          "iopub.status.idle": "2024-11-17T00:38:38.218904Z",
          "shell.execute_reply.started": "2024-11-17T00:38:38.211961Z",
          "shell.execute_reply": "2024-11-17T00:38:38.218080Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 2\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:38.220092Z",
          "iopub.execute_input": "2024-11-17T00:38:38.220459Z",
          "iopub.status.idle": "2024-11-17T00:38:38.258330Z",
          "shell.execute_reply.started": "2024-11-17T00:38:38.220412Z",
          "shell.execute_reply": "2024-11-17T00:38:38.257425Z"
        },
        "outputId": "bf493250-8092-4ba7-c741-a82a2710b3bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 39,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Nov17_00-38-38_84769eed1484,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:38.259683Z",
          "iopub.execute_input": "2024-11-17T00:38:38.259983Z",
          "iopub.status.idle": "2024-11-17T00:38:39.591605Z",
          "shell.execute_reply.started": "2024-11-17T00:38:38.259950Z",
          "shell.execute_reply": "2024-11-17T00:38:39.590724Z"
        },
        "outputId": "03d8fbe8-0a5a-4693-b7bb-936c0de1f431"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 40,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7e7f12f02590>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T00:38:39.602309Z",
          "iopub.execute_input": "2024-11-17T00:38:39.602596Z",
          "iopub.status.idle": "2024-11-17T01:01:46.963342Z",
          "shell.execute_reply.started": "2024-11-17T00:38:39.602563Z",
          "shell.execute_reply": "2024-11-17T01:01:46.962395Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:01:46.964467Z",
          "iopub.execute_input": "2024-11-17T01:01:46.964786Z",
          "iopub.status.idle": "2024-11-17T01:02:25.537768Z",
          "shell.execute_reply.started": "2024-11-17T01:01:46.964745Z",
          "shell.execute_reply": "2024-11-17T01:02:25.536805Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:25.538889Z",
          "iopub.execute_input": "2024-11-17T01:02:25.539213Z",
          "iopub.status.idle": "2024-11-17T01:02:26.971782Z",
          "shell.execute_reply.started": "2024-11-17T01:02:25.539178Z",
          "shell.execute_reply": "2024-11-17T01:02:26.970789Z"
        },
        "outputId": "3cbf3517-6fbd-43c9-b223-f2319ea658b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 960,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2560,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 15,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 5,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 960,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2560,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 15,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 5,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:26.973096Z",
          "iopub.execute_input": "2024-11-17T01:02:26.973418Z",
          "iopub.status.idle": "2024-11-17T01:02:27.308773Z",
          "shell.execute_reply.started": "2024-11-17T01:02:26.973385Z",
          "shell.execute_reply": "2024-11-17T01:02:27.307723Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:27.310110Z",
          "iopub.execute_input": "2024-11-17T01:02:27.310504Z",
          "iopub.status.idle": "2024-11-17T01:02:27.321969Z",
          "shell.execute_reply.started": "2024-11-17T01:02:27.310458Z",
          "shell.execute_reply": "2024-11-17T01:02:27.320972Z"
        },
        "id": "CJeSvAZIUk9a",
        "outputId": "0d91b458-c2fe-4c0d-e662-389256455002"
      },
      "outputs": [
        {
          "execution_count": 45,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:27.323302Z",
          "iopub.execute_input": "2024-11-17T01:02:27.323931Z",
          "iopub.status.idle": "2024-11-17T01:02:28.200988Z",
          "shell.execute_reply.started": "2024-11-17T01:02:27.323884Z",
          "shell.execute_reply": "2024-11-17T01:02:28.200146Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "hK5OfiCYUk9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:28.202265Z",
          "iopub.execute_input": "2024-11-17T01:02:28.202667Z",
          "iopub.status.idle": "2024-11-17T01:02:30.151263Z",
          "shell.execute_reply.started": "2024-11-17T01:02:28.202598Z",
          "shell.execute_reply": "2024-11-17T01:02:30.150169Z"
        },
        "id": "cctizDWGUk9b",
        "outputId": "b2db16d5-84fc-41e4-f2ed-604abc62ec3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 960,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2560,\n  \"is_llama_config\": true,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 15,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 5,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_interleaved\": false,\n  \"rope_scaling\": null,\n  \"rope_theta\": 100000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-360M-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-360M-Instruct/snapshots/4873f67095301d304753fae05bc09ec766634e50/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\n",
          "output_type": "stream"
        },
        {
          "execution_count": 47,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=960, out_features=960, bias=False)\n          (k_proj): Linear4bit(in_features=960, out_features=320, bias=False)\n          (v_proj): Linear4bit(in_features=960, out_features=320, bias=False)\n          (o_proj): Linear4bit(in_features=960, out_features=960, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=960, out_features=2560, bias=False)\n          (up_proj): Linear4bit(in_features=960, out_features=2560, bias=False)\n          (down_proj): Linear4bit(in_features=2560, out_features=960, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((960,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:30.152426Z",
          "iopub.execute_input": "2024-11-17T01:02:30.152740Z",
          "iopub.status.idle": "2024-11-17T01:02:30.165363Z",
          "shell.execute_reply.started": "2024-11-17T01:02:30.152706Z",
          "shell.execute_reply": "2024-11-17T01:02:30.164461Z"
        },
        "id": "8Zo6ubZ4Uk9b",
        "outputId": "75a72874-1e1e-4ed7-e10a-b109f45de1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 204534720\nTrainable parameters : 47248320\nTrainable percentage: 23.10%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:30.166653Z",
          "iopub.execute_input": "2024-11-17T01:02:30.166960Z",
          "iopub.status.idle": "2024-11-17T01:02:30.199299Z",
          "shell.execute_reply.started": "2024-11-17T01:02:30.166927Z",
          "shell.execute_reply": "2024-11-17T01:02:30.198439Z"
        },
        "id": "kJ07Vcc2Uk9b",
        "outputId": "5ff1678d-07c6-4e7b-aeab-2ee2772dd0f5"
      },
      "outputs": [
        {
          "execution_count": 49,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(49152, 960, padding_idx=2)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=960, bias=False)\n                  (default): Linear(in_features=64, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=320, bias=False)\n                  (default): Linear(in_features=64, out_features=320, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=320, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=320, bias=False)\n                  (default): Linear(in_features=64, out_features=320, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=960, bias=False)\n                  (default): Linear(in_features=64, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n                  (default): Linear(in_features=64, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=960, out_features=2560, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=960, out_features=64, bias=False)\n                  (default): Linear(in_features=960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2560, bias=False)\n                  (default): Linear(in_features=64, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2560, out_features=64, bias=False)\n                  (default): Linear(in_features=2560, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=960, bias=False)\n                  (default): Linear(in_features=64, out_features=960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((960,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:30.200372Z",
          "iopub.execute_input": "2024-11-17T01:02:30.200678Z",
          "iopub.status.idle": "2024-11-17T01:02:30.233397Z",
          "shell.execute_reply.started": "2024-11-17T01:02:30.200644Z",
          "shell.execute_reply": "2024-11-17T01:02:30.232411Z"
        },
        "id": "0VgWgx88Uk9b",
        "outputId": "9a4b4792-3ec5-47fe-9476-4555b9820f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 274002880\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt):\n",
        "  messages = [\n",
        "    {'role' : 'human', 'content' : prompt},\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = 'pt',\n",
        "    truncation = True,\n",
        "    padding = True,\n",
        "  ).to('cuda')\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    input_ids = inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:30.234463Z",
          "iopub.execute_input": "2024-11-17T01:02:30.234763Z",
          "iopub.status.idle": "2024-11-17T01:02:30.242900Z",
          "shell.execute_reply.started": "2024-11-17T01:02:30.234724Z",
          "shell.execute_reply": "2024-11-17T01:02:30.242052Z"
        },
        "id": "dBV_60gLUk9b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt):\n",
        "  messages = [\n",
        "    {'role' : 'human', 'content' : prompt},\n",
        "  ]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = 'pt',\n",
        "    truncation = True,\n",
        "    padding = True,\n",
        "  ).to('cuda')\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    input_ids = inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:30.244031Z",
          "iopub.execute_input": "2024-11-17T01:02:30.244308Z",
          "iopub.status.idle": "2024-11-17T01:02:30.252296Z",
          "shell.execute_reply.started": "2024-11-17T01:02:30.244276Z",
          "shell.execute_reply": "2024-11-17T01:02:30.251353Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:02:30.253597Z",
          "iopub.execute_input": "2024-11-17T01:02:30.254241Z",
          "iopub.status.idle": "2024-11-17T01:02:30.264713Z",
          "shell.execute_reply.started": "2024-11-17T01:02:30.254196Z",
          "shell.execute_reply": "2024-11-17T01:02:30.263980Z"
        },
        "id": "zOIvVOUGUk9b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:15:34.599958Z",
          "iopub.execute_input": "2024-11-17T01:15:34.600831Z",
          "iopub.status.idle": "2024-11-17T01:16:48.770788Z",
          "shell.execute_reply.started": "2024-11-17T01:15:34.600787Z",
          "shell.execute_reply": "2024-11-17T01:16:48.769841Z"
        },
        "outputId": "e83f2aae-46a7-418b-9776-f632c7e59729"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human Explain how the order of          |  <|im_start|>human Explain how the order of        \noperations in BIDMAS ensures consistency in         |  operations in BIDMAS ensures consistency in       \nmathematical calculations.<|im_end|>                |  mathematical calculations.<|im_end|>              \n<|im_start|>assistant The order of operations,      |  <|im_start|>assistant The order of operations,    \nalso known as the PEMDAS rule, is a set of rules    |  also known as the PEMDAS rule, is a set of rules  \nthat dictates the order in which mathematical       |  that dictates the order in which mathematical     \noperations should be performed when a calculation   |  operations should be performed when a calculation \nis presented. This rule is essential for ensuring   |  is presented. This rule is essential for ensuring \nconsistency in mathematical calculations, as it     |  consistency in mathematical calculations, as it   \nprevents errors and maintains the integrity of the  |  prevents errors and maintains the integrity of the\nmathematical expression.  The PEMDAS rule is based  |  mathematical expression.  The PEMDAS rule is based\non the order of operations, which is as follows:    |  on the order of operations, which is as follows:  \nParentheses, Exponents, Multiplication and          |  Parentheses, Exponents, Multiplication and        \nDivision, and Addition and Subtraction. This means  |  Division, and Addition and Subtraction. This means\nthat when a calculation is presented, the order of  |  that when a calculation is presented, the order of\noperations is applied in the following sequence:    |  operations is applied in the following sequence:  \n1. Parentheses: If there are parentheses in the     |  1. Parentheses: If there are parentheses in the   \nexpression, the calculation should be performed     |  expression, the calculation should be performed   \nwithin those parentheses. 2. Exponents: If there    |  within those parentheses. 2. Exponents: If there  \nare exponents in the expression, the calculation    |  are exponents in the expression, the calculation  \nshould be performed according to the order of       |  should be performed according to the order of     \noperations. 3. Multiplication and Division: These   |  operations. 3. Multiplication and Division: These \noperations should be performed from left to right.  |  operations should be performed from left to right.\n4. Addition and Subtraction: These operations       |  4. Addition and Subtraction: These operations     \nshould be performed from left to right.  By         |  should be performed from left to right.  By       \nfollowing the order of operations, the PEMDAS rule  |  following the order of operations, the PEMDAS rule\nensures that mathematical calculations are          |  ensures that mathematical calculations are        \nperformed consistently, avoiding errors and         |  performed consistently and accurately. This rule  \nmaintaining the integrity of the mathematical       |  is widely used in mathematics, particularly in    \nexpression. This rule is widely used in             |  algebra and arithmetic, where it helps to maintain\nmathematics, particularly in algebra and            |  the integrity of the mathematical expression and  \narithmetic, where it is essential for ensuring the  |  prevent errors.  For example, consider the        \naccuracy of calculations.<|im_end|>                 |  expression 2x + 3. According to the PEMDAS rule,  \n                                                    |  the calculation should be performed as follows:   \n                                                    |  1. Parentheses: First, the expression 2x should be\n                                                    |  evaluated. 2. Exponents: Next, the exponent 3     \n                                                    |  should be evaluated. 3. Multiplication and        \n                                                    |  Division: These operations should be performed    \n                                                    |  from left to right. In this case, the expression  \n                                                    |  2x is equal to 4, so the calculation should be    \n                                                    |  performed as follows: 2x + 3 = 4 + 3 = 7.  By     \n                                                    |  following the order of operations, the PEMDAS rule\n                                                    |  ensures that mathematical calculations are        \n                                                    |  performed consistently and accurately, making it a\n                                                    |  fundamental principle in mathematics.<|im_end|>   \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:06:03.433816Z",
          "iopub.execute_input": "2024-11-17T01:06:03.434155Z",
          "iopub.status.idle": "2024-11-17T01:07:33.596369Z",
          "shell.execute_reply.started": "2024-11-17T01:06:03.434120Z",
          "shell.execute_reply": "2024-11-17T01:07:33.595433Z"
        },
        "outputId": "b8b52ebb-9dd5-4203-fb08-7ed6a75e7bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human What is the function of the       |  <|im_start|>human What is the function of the     \nthree tiny bones in the middle ear and how do they  |  three tiny bones in the middle ear and how do they\ncontribute to the process of hearing?<|im_end|>     |  contribute to the process of hearing?<|im_end|>   \n<|im_start|>assistant The three tiny bones in the   |  <|im_start|>assistant The three tiny bones in the \nmiddle ear, also known as the ossicles, are known   |  middle ear, also known as the ossicles, are known \nas the malleus, the incus, and the stapula. They    |  as the malleus, the incus, and the stapula. They  \nare also known as the ossicles because they are     |  are also known as the ossicles because they are   \nmade of ossified cartilage. The ossicles are        |  made of ossified cartilage. The ossicles are      \nresponsible for the sound amplification and         |  responsible for the sound amplification and       \ntransmission process in the middle ear.  The        |  transmission process in the middle ear.  The      \nossicles are connected to the eardrum and the       |  ossicles are connected to the eardrum and the     \ncochlea, which are the main components of the ear.  |  cochlea, which are the main components of the ear.\nThe ossicles are positioned in a way that they can  |  The ossicles are the first step in the sound      \namplify sound waves. The malleus, also known as     |  amplification process. The ossicles are connected \nthe hammer, is connected to the eardrum. When       |  to the eardrum by the stapula, which is a small,  \nsound waves hit the eardrum, they cause the         |  thin, and flexible bone. The stapula is connected \nossicles to move. The incus, also known as the      |  to the incus, which is a larger, more rigid bone. \nanvil, is connected to the ossicles. When the       |  The ossicles work as follows: the stapula is      \nossicles move, they transmit the sound waves to     |  connected to the incus, which is connected to the \nthe cochlea, which is the main auditory organ.      |  malleus, which is connected to the eardrum. The   \nThe ossicles work together to amplify sound waves   |  stapula is connected to the incus, which is       \nby changing the shape of the eardrum. The ossicles  |  connected to the malleus, which is connected to   \nalso help to transmit sound waves to the cochlea    |  the cochlea.  The stapula, incus, and malleus are \nby changing the shape of the cochlea. This process  |  connected to the eardrum by the stapula, incus,   \nis known as the ossicle-cochlear pathway.  The      |  and the malleus. The stapula is connected to the  \nossicles are responsible for the sound              |  incus, which is connected to the malleus. The     \namplification and transmission process in the       |  incus is connected to the malleus, which is       \nmiddle ear. They help to amplify sound waves by     |  connected to the cochlea.  The ossicles work as   \nchanging the shape of the eardrum. They also help   |  follows: the stapula is connected to the incus,   \nto transmit sound waves to the cochlea by changing  |  which is connected to the malleus. The incus is   \nthe shape of the cochlea. This process is known as  |  connected to the malleus, which is connected to   \nthe ossicle-cochlear pathway.  In summary, the      |  the cochlea. The stapula is connected to the      \nthree tiny bones in the middle ear, the ossicles,   |  incus, which is connected to the malleus.  The    \nare responsible for the sound amplification and     |  ossicles are responsible for the sound            \ntransmission process in the middle ear. They help   |  amplification process in the middle ear. They     \nto amplify sound waves by changing the shape of     |  amplify the sound waves that travel through the   \nthe eardrum and transmit sound waves to the         |  air and transmit them to the eardrum. The ossicles\ncochlea by changing the shape of the                |  also help to transmit the sound waves to the      \ncochlea.<|im_end|>                                  |  cochlea, which is responsible for converting sound\n                                                    |  waves into electrical signals that the brain      \n                                                    |  interprets as sound.  In summary, the three tiny  \n                                                    |  bones in the middle ear, the ossicles, work as a  \n                                                    |  chain to amplify and transmit sound waves in the  \n                                                    |  ear. They help to convert sound waves into        \n                                                    |  electrical signals that the brain interprets as   \n                                                    |  sound.<|im_end|>                                  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:21:00.548411Z",
          "iopub.execute_input": "2024-11-17T01:21:00.549145Z",
          "iopub.status.idle": "2024-11-17T01:22:58.196820Z",
          "shell.execute_reply.started": "2024-11-17T01:21:00.549098Z",
          "shell.execute_reply": "2024-11-17T01:22:58.195773Z"
        },
        "outputId": "e80a6bf7-33a4-4419-c501-135ba922f94e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human How can students learn about the  |  <|im_start|>human How can students learn about the\nreal-life applications of chemistry in everyday     |  real-life applications of chemistry in everyday   \nlife despite limited exposure to chemistry beyond   |  life despite limited exposure to chemistry beyond \nthe classroom?<|im_end|> <|im_start|>assistant 1.   |  the classroom?<|im_end|> <|im_start|>assistant 1. \nUse real-life examples: Show students how           |  Use real-life examples: Show students how         \nchemistry is used in everyday life by using         |  chemistry is used in everyday life by using       \nexamples from their own lives. For instance,        |  examples from their own lives. For instance,      \nexplain how detergents work in the washing          |  explain how detergents work in the washing        \nmachine, how batteries work in the phone, or how    |  machine, how batteries work in the phone, or how  \nfertilizers are used in agriculture.  2. Use        |  fertilizers are used in agriculture.  2. Use      \ninteractive activities: Incorporate chemistry       |  interactive activities: Incorporate chemistry     \nexperiments into the classroom to make learning     |  experiments into the classroom to make learning   \nmore engaging. For example, demonstrate how a       |  more engaging. For example, demonstrate how a     \nBunsen burner works, how a chemical reaction is     |  Bunsen burner works, how a chemical reaction is   \nused in the production of soap, or how a chemical   |  used in the production of soap, or how a chemical \ncompound is used in the production of a common      |  compound is used in the production of a common    \nfood item.  3. Use media and documentaries: Show    |  food item.  3. Use media and documentaries: Show  \nstudents how chemistry is used in popular media     |  students how chemistry is used in popular media   \nand documentaries. For example, demonstrate how a   |  and documentaries. For example, demonstrate how a \nchemical compound is used in the production of a    |  chemical compound is used in the production of a  \ncommon food item, how a chemical reaction is used   |  common food item, how a chemical reaction is used \nin the production of a common medicine, or how a    |  in the production of a common medicine, or how a  \nchemical compound is used in the production of a    |  chemical compound is used in the production of a  \ncommon food item.  4. Use real-world data: Provide  |  common food item.  4. Use real-world data: Provide\nstudents with real-world data about the             |  students with real-world data about the           \napplications of chemistry in everyday life. For     |  applications of chemistry in everyday life. For   \nexample, show them how a chemical compound is used  |  example, show them how a chemical compound is used\nin the production of a common food item, how a      |  in the production of a common food item, how a    \nchemical reaction is used in the production of a    |  chemical reaction is used in the production of a  \ncommon medicine, or how a chemical compound is      |  common medicine, or how a chemical compound is    \nused in the production of a common food item.  5.   |  used in the production of a common food item.  5. \nUse online resources: Utilize online resources      |  Use online resources: Utilize online resources    \nsuch as YouTube, TED Talks, and online forums to    |  such as YouTube, TED Talks, and online forums to  \nprovide students with information about the real-   |  provide students with information about the real- \nlife applications of chemistry in everyday life.    |  life applications of chemistry in everyday life.  \nFor example, show students how a chemical compound  |  For example, show students how a chemical compound\nis used in the production of a common food item,    |  is used in the production of a common food item,  \nhow a chemical reaction is used in the production   |  how a chemical reaction is used in the production \nof a common medicine, or how a chemical compound    |  of a common medicine, or how a chemical compound  \nis used in the production of a common food item.    |  is used in the production of a common food item.  \n6. Use case studies: Use case studies to            |  6. Use case studies: Use case studies to          \ndemonstrate the real-life applications of           |  demonstrate the real-life applications of         \nchemistry in everyday life. For example, show       |  chemistry in everyday life. For example, show     \nstudents how a chemical compound is used in the     |  students how a chemical compound is used in the   \nproduction of a common food item, how a chemical    |  production of a common food item, how a chemical  \nreaction is used in the production of a common      |  reaction is used in the production of a common    \nmedicine, or how a chemical compound is used in     |  medicine, or how a chemical compound is used in   \nthe production of a common food item.  7. Use       |  the production of a common food item.  7. Use     \ninteractive simulations: Use interactive            |  interactive simulations: Use interactive          \nsimulations to demonstrate the real-life            |  simulations to demonstrate the real-life          \napplications of chemistry in everyday life. For     |  applications of chemistry in everyday life. For   \nexample, show students how a chemical compound is   |  example, show students how a chemical compound is \nused in the production of a common food item, how   |  used in the production of a common food item, how \na chemical reaction is used in the production of a  |  a chemical reaction is used in the production of a\ncommon medicine, or how a chemical compound is      |  common medicine, or how a chemical compound is    \nused in the production of a common food item.  By   |  used in the production of a common food item.  8. \nusing these methods, students will gain a better    |  Use real-world examples: Use real-world examples  \nunderstanding of the real-life applications of      |  to demonstrate the real-life applications of      \nchemistry in everyday life, even if they have       |  chemistry in everyday life. For example, show     \nlimited exposure to chemistry in the                |  students how a chemical compound is used in the   \nclassroom.<|im_end|>                                |  production of a common food item, how a chemical  \n                                                    |  reaction is used in the production of a common    \n                                                    |  medicine, or how a chemical compound is used in   \n                                                    |  the production of a common food item.  By using   \n                                                    |  these methods, students can gain a deeper         \n                                                    |  understanding of the real-life applications of    \n                                                    |  chemistry in everyday life beyond the             \n                                                    |  classroom.<|im_end|>                              \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:11:05.063952Z",
          "iopub.execute_input": "2024-11-17T01:11:05.064243Z",
          "iopub.status.idle": "2024-11-17T01:13:18.689403Z",
          "shell.execute_reply.started": "2024-11-17T01:11:05.064211Z",
          "shell.execute_reply": "2024-11-17T01:13:18.688441Z"
        },
        "outputId": "bdce0bdd-c99b-4563-c924-0fcd882d558c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human What were the social and          |  <|im_start|>human What were the social and        \neconomic impacts of the Industrial Revolution in    |  economic impacts of the Industrial Revolution in  \n19th century England?<|im_end|>                     |  19th century England?<|im_end|>                   \n<|im_start|>assistant The Industrial Revolution in  |  <|im_start|>assistant The Industrial Revolution in\n19th century England had a profound impact on the   |  19th century England had a profound impact on the \nsocial and economic landscape of the time. The      |  social and economic landscape of the time. The    \nrapid growth of factories and the development of    |  rapid growth of factories and the development of  \nnew technologies led to significant changes in the  |  new technologies led to significant changes in the\nway people lived, worked, and interacted with one   |  way people lived, worked, and interacted with one \nanother.  The rapid growth of factories led to the  |  another.  The rapid growth of factories led to the\nemergence of a new class of workers, who were       |  emergence of a new class of workers, who were     \nlargely unskilled and uneducated. Many of these     |  largely unskilled and uneducated. Many of these   \nworkers were forced to live in overcrowded and      |  workers were forced to live in overcrowded and    \nunsanitary conditions, living in the very same      |  unsanitary conditions, living in the very same    \nareas where the factories were located. This led    |  areas where the factories were located. This led  \nto widespread poverty, disease, and social unrest.  |  to widespread poverty, disease, and social unrest.\nThe growth of the textile industry also led to the  |  The growth of the textile industry also led to the\nemergence of new social classes, as the wealthy     |  emergence of new social classes, as the wealthy   \nand the middle class became increasingly wealthy    |  and the middle class became increasingly wealthy  \nand the poor became increasingly poor. The working  |  and the poor became increasingly poor. The working\nclass, who were largely unskilled and uneducated,   |  class, who were largely unskilled and uneducated, \nwere forced to live in the same areas as the        |  were forced to live in the same areas as the      \nfactories, leading to widespread poverty, disease,  |  factories, leading to widespread poverty, disease,\nand social unrest.  The Industrial Revolution also  |  and social unrest.  The Industrial Revolution also\nled to the growth of new markets and industries,    |  led to the growth of new markets and industries,  \nwhich in turn led to the growth of new social       |  which in turn led to the growth of new cities and \nclasses. The growth of the coal industry, for       |  the expansion of urban areas. This led to the     \nexample, led to the emergence of new social         |  emergence of new social classes, as the wealthy   \nclasses, as the wealthy and the middle class        |  and the middle class became increasingly wealthy  \nbecame increasingly wealthy and the poor became     |  and the poor became increasingly poor.  The growth\nincreasingly poor.  The growth of the railways      |  of the railways also played a significant role in \nalso led to the emergence of new social classes,    |  the social and economic landscape of 19th century \nas the wealthy and the middle class became          |  England. The railways allowed people to travel    \nincreasingly wealthy and the poor became            |  more quickly and easily, which in turn led to the \nincreasingly poor. The growth of the coal industry  |  growth of new cities and the expansion of urban   \nalso led to the emergence of new social classes,    |  areas. This led to the emergence of new social    \nas the wealthy and the middle class became          |  classes, as the wealthy and the middle class      \nincreasingly wealthy and the poor became            |  became increasingly wealthy and the poor became   \nincreasingly poor.  The Industrial Revolution also  |  increasingly poor.  Overall, the Industrial       \nled to the growth of new social classes, as the     |  Revolution in 19th century England had a profound \nworking class became increasingly unskilled and     |  impact on the social and economic landscape of the\nuneducated. Many of these workers were forced to    |  time, leading to widespread poverty, disease, and \nlive in overcrowded and unsanitary conditions,      |  social unrest. The growth of new cities and the   \nliving in the very same areas where the factories   |  expansion of urban areas also led to the emergence\nwere located.  The growth of the textile industry   |  of new social classes, as the wealthy and the     \nalso led to the emergence of new social classes,    |  middle class became increasingly wealthy and the  \nas the wealthy and the middle class became          |  poor became increasingly poor.<|im_end|>          \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor. The growth of the coal industry  |                                                    \nalso led to the emergence of new social classes,    |                                                    \nas the wealthy and the middle class became          |                                                    \nincreasingly wealthy and the poor became            |                                                    \nincreasingly poor.  The growth of the textile       |                                                    \nindustry also led to the emergence of new social    |                                                    \nclasses, as the wealthy and the middle class        |                                                    \nbecame increasingly wealthy and the poor became     |                                                    \nincreasingly poor.                                  |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-17T01:24:44.288570Z",
          "iopub.execute_input": "2024-11-17T01:24:44.288977Z",
          "iopub.status.idle": "2024-11-17T01:26:38.506372Z",
          "shell.execute_reply.started": "2024-11-17T01:24:44.288933Z",
          "shell.execute_reply": "2024-11-17T01:26:38.505378Z"
        },
        "outputId": "a975626f-9ee6-415b-d2e3-df1074173454"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    204534720                      |                     274002880                     \n================================================== | ==================================================\n<|im_start|>system You are a helpful AI assistant   |  <|im_start|>system You are a helpful AI assistant \nnamed SmolLM, trained by Hugging Face<|im_end|>     |  named SmolLM, trained by Hugging Face<|im_end|>   \n<|im_start|>human What is the observable universe,  |  <|im_start|>human What is the observable universe,\nand why does its size not correspond to the age of  |  and why does its size not correspond to the age of\nthe universe multiplied by the speed of light? How  |  the universe multiplied by the speed of light? How\ndoes the expansion of the universe affect our       |  does the expansion of the universe affect our     \nperception of its size?<|im_end|>                   |  perception of its size?<|im_end|>                 \n<|im_start|>assistant The observable universe is    |  <|im_start|>assistant The observable universe is  \nthe region of space that we can see and observe     |  the region of space that we can see and observe   \nfrom a given location. It is the region that is     |  from a given location. It is the region that is   \nwithin the horizon of light that has reached us     |  within the horizon of light that has reached us   \nfrom all other locations in the universe. The       |  from all other locations in the universe. The     \nobservable universe is vast, with a diameter of     |  observable universe is vast, with a diameter of   \napproximately 93 billion light-years.  The reason   |  approximately 93 billion light-years.  The reason \nthe size of the observable universe does not        |  the size of the observable universe does not      \ncorrespond to the age of the universe multiplied    |  correspond to the age of the universe multiplied  \nby the speed of light is that the universe is       |  by the speed of light is that the universe is     \nexpanding. The expansion of the universe is not a   |  expanding. The expansion of the universe is not a \nlinear process but rather a rapid expansion. This   |  linear process but rather a rapid expansion. This \nexpansion is known as the cosmological expansion.   |  expansion is known as the cosmological expansion. \nThe expansion of the universe affects our           |  The expansion of the universe affects our         \nperception of its size in several ways:  1. **Size  |  perception of its size in several ways:  1. **Size\nof the universe**: The expansion of the universe    |  of the universe**: The expansion of the universe  \ncauses the distance between objects to increase     |  causes the distance between objects to increase   \nover time. This means that the size of the          |  over time. This means that the size of the        \nobservable universe is not constant but is          |  observable universe is not constant but is        \nincreasing over time.  2. **Size of the universe    |  increasing over time.  2. **Size of the observable\nas a whole**: The observable universe is not the    |  universe**: The observable universe is not the    \nentire universe. The universe is vast, and the      |  same as the universe as a whole. The universe as a\nobservable universe is just one part of it.  3.     |  whole is much larger than the observable universe.\n**Size of the observable universe as a whole**:     |  3. **Size of the observable universe**: The       \nThe observable universe is not the entire           |  observable universe is not the same as the        \nuniverse. The universe is vast, and the observable  |  universe as a whole. The universe as a whole is   \nuniverse is just one part of it.  4. **Size of the  |  much larger than the observable universe.  4.     \nobservable universe as a whole**: The universe is   |  **Size of the observable universe**: The universe \nnot the entire universe. The universe is vast, and  |  as a whole is not the same as the universe as a   \nthe observable universe is just one part of it.     |  whole. The universe as a whole is much larger than\n5. **Size of the observable universe as a whole**:  |  the observable universe.  The expansion of the    \nThe universe is not the entire universe. The        |  universe affects our perception of its size in    \nuniverse is vast, and the observable universe is    |  several ways:  1. **Size of the universe**: The   \njust one part of it.  The expansion of the          |  expansion of the universe causes the distance     \nuniverse affects our perception of its size in      |  between objects to increase over time. This means \nseveral ways:  1. **Size of the universe**: The     |  that the size of the observable universe is not   \nexpansion of the universe causes the distance       |  constant but is increasing over time.  2. **Size  \nbetween objects to increase over time. This means   |  of the observable universe**: The observable      \nthat the size of the observable universe is not     |  universe is not the same as the universe as a     \nconstant but is increasing over time.  2. **Size    |  whole. The universe as a whole is much larger than\nof the universe as a whole**: The universe is       |  the observable universe.  3. **Size of the        \nvast, and the observable universe is just one part  |  observable universe**: The universe as a whole is \nof it.  3. **Size of the observable universe as a   |  not the same as the universe as a whole. The      \nwhole**: The universe is not the entire universe.   |  universe as a whole is much larger than the       \nThe universe is vast, and the observable universe   |  observable universe.  4. **Size of the observable \nis just one part of it.  4. **Size of the           |  universe**: The universe as a whole is not the    \nobservable universe as a whole**: The universe is   |  same as the universe as a whole. The universe as a\nnot the entire universe. The universe is vast, and  |  whole is much larger than the observable universe.\nthe observable universe is just one part of it.     |  In summary, the observable universe is not the    \nIn summary, the observable universe is not the      |  same as the universe as a whole. The universe as a\nentire universe, the universe is expanding, and     |  whole is much larger than the observable universe.\nthe size of the observable universe is not          |  The expansion of the universe affects our         \nconstant. These factors affect our perception of    |  perception of its size in several ways: it causes \nthe size of the universe and its                    |  the distance between objects to increase over     \nexpansion.<|im_end|>                                |  time, it causes the size of the observable        \n                                                    |  universe to increase over time, it causes the     \n                                                    |  universe as a whole to be much larger than the    \n                                                    |  observable universe, and it causes the universe as\n                                                    |  a whole to be much larger than the observable     \n                                                    |  universe.<|im_end|>                               \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}