{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/OpenHermes2.5_Mistral7B_Fine_Tune_PEFT_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:21:32.663925Z",
          "iopub.execute_input": "2024-12-16T11:21:32.664432Z",
          "iopub.status.idle": "2024-12-16T11:22:37.335318Z",
          "shell.execute_reply.started": "2024-12-16T11:21:32.664391Z",
          "shell.execute_reply": "2024-12-16T11:22:37.334214Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-12-16T11:22:37.337229Z",
          "iopub.execute_input": "2024-12-16T11:22:37.337527Z",
          "iopub.status.idle": "2024-12-16T11:22:56.696919Z",
          "shell.execute_reply.started": "2024-12-16T11:22:37.337500Z",
          "shell.execute_reply": "2024-12-16T11:22:56.696058Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-12-16T11:22:56.698650Z",
          "iopub.execute_input": "2024-12-16T11:22:56.699686Z",
          "iopub.status.idle": "2024-12-16T11:22:56.706935Z",
          "shell.execute_reply.started": "2024-12-16T11:22:56.699644Z",
          "shell.execute_reply": "2024-12-16T11:22:56.705990Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'unsloth/OpenHermes-2.5-Mistral-7B'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:22:56.709225Z",
          "iopub.execute_input": "2024-12-16T11:22:56.709549Z",
          "iopub.status.idle": "2024-12-16T11:22:56.750715Z",
          "shell.execute_reply.started": "2024-12-16T11:22:56.709519Z",
          "shell.execute_reply": "2024-12-16T11:22:56.749838Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "  if base == True:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype = torch.float16,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "  else:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = True,\n",
        "      bnb_4bit_quant_type = 'nf4',\n",
        "      bnb_4bit_compute_dtype = torch.float16,\n",
        "      bnb_4bit_use_double_quant = True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config = bnb_config,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:22:56.751737Z",
          "iopub.execute_input": "2024-12-16T11:22:56.752047Z",
          "iopub.status.idle": "2024-12-16T11:22:56.763246Z",
          "shell.execute_reply.started": "2024-12-16T11:22:56.752006Z",
          "shell.execute_reply": "2024-12-16T11:22:56.762328Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:22:56.764365Z",
          "iopub.execute_input": "2024-12-16T11:22:56.764795Z",
          "iopub.status.idle": "2024-12-16T11:33:12.591632Z",
          "shell.execute_reply.started": "2024-12-16T11:22:56.764758Z",
          "shell.execute_reply": "2024-12-16T11:33:12.590706Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:12.592692Z",
          "iopub.execute_input": "2024-12-16T11:33:12.592955Z",
          "iopub.status.idle": "2024-12-16T11:33:12.601065Z",
          "shell.execute_reply.started": "2024-12-16T11:33:12.592909Z",
          "shell.execute_reply": "2024-12-16T11:33:12.600104Z"
        },
        "outputId": "33910e66-0f81-4504-b163-506e0f8a2caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3752087552\nTrainable parameters : 262426624\nTrainable percentage: 6.99%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:12.602144Z",
          "iopub.execute_input": "2024-12-16T11:33:12.602473Z",
          "iopub.status.idle": "2024-12-16T11:33:16.327605Z",
          "shell.execute_reply.started": "2024-12-16T11:33:12.602436Z",
          "shell.execute_reply": "2024-12-16T11:33:16.326805Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'microsoft/orca-math-word-problems-200k'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:16.328723Z",
          "iopub.execute_input": "2024-12-16T11:33:16.328994Z",
          "iopub.status.idle": "2024-12-16T11:33:16.333226Z",
          "shell.execute_reply.started": "2024-12-16T11:33:16.328967Z",
          "shell.execute_reply": "2024-12-16T11:33:16.332357Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:16.336932Z",
          "iopub.execute_input": "2024-12-16T11:33:16.337276Z",
          "iopub.status.idle": "2024-12-16T11:33:16.344721Z",
          "shell.execute_reply.started": "2024-12-16T11:33:16.337248Z",
          "shell.execute_reply": "2024-12-16T11:33:16.343899Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:16.345674Z",
          "iopub.execute_input": "2024-12-16T11:33:16.346050Z",
          "iopub.status.idle": "2024-12-16T11:33:24.111767Z",
          "shell.execute_reply.started": "2024-12-16T11:33:16.345994Z",
          "shell.execute_reply": "2024-12-16T11:33:24.110994Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(10000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.112748Z",
          "iopub.execute_input": "2024-12-16T11:33:24.113002Z",
          "iopub.status.idle": "2024-12-16T11:33:24.119513Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.112976Z",
          "shell.execute_reply": "2024-12-16T11:33:24.118703Z"
        },
        "id": "yKmGwi-upJzW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.120610Z",
          "iopub.execute_input": "2024-12-16T11:33:24.120969Z",
          "iopub.status.idle": "2024-12-16T11:33:24.291414Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.120930Z",
          "shell.execute_reply": "2024-12-16T11:33:24.290555Z"
        },
        "outputId": "f258ee9c-3399-4d70-9feb-58e90e2d89e3"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.292561Z",
          "iopub.execute_input": "2024-12-16T11:33:24.292921Z",
          "iopub.status.idle": "2024-12-16T11:33:24.301675Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.292883Z",
          "shell.execute_reply": "2024-12-16T11:33:24.300783Z"
        },
        "outputId": "574a4527-ee9d-468a-85f1-66f06cbd7095"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.302512Z",
          "iopub.execute_input": "2024-12-16T11:33:24.302763Z",
          "iopub.status.idle": "2024-12-16T11:33:24.312400Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.302712Z",
          "shell.execute_reply": "2024-12-16T11:33:24.311606Z"
        },
        "outputId": "ce414d68-4498-4850-f086-f1312c901a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['question', 'answer']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.313338Z",
          "iopub.execute_input": "2024-12-16T11:33:24.313579Z",
          "iopub.status.idle": "2024-12-16T11:33:24.322283Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.313540Z",
          "shell.execute_reply": "2024-12-16T11:33:24.321372Z"
        },
        "id": "ws2XJxQBpJzX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def preprocess(examples):\n",
        "  input = examples['question']\n",
        "  output = examples['answer']\n",
        "\n",
        "  text = prompt_format.format(input, output) + EOS_TOKEN\n",
        "  return {'prompt' : text}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.323196Z",
          "iopub.execute_input": "2024-12-16T11:33:24.323533Z",
          "iopub.status.idle": "2024-12-16T11:33:24.332478Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.323496Z",
          "shell.execute_reply": "2024-12-16T11:33:24.331660Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(preprocess, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.333352Z",
          "iopub.execute_input": "2024-12-16T11:33:24.333603Z",
          "iopub.status.idle": "2024-12-16T11:33:24.819317Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.333580Z",
          "shell.execute_reply": "2024-12-16T11:33:24.818461Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.820277Z",
          "iopub.execute_input": "2024-12-16T11:33:24.820616Z",
          "iopub.status.idle": "2024-12-16T11:33:24.828518Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.820579Z",
          "shell.execute_reply": "2024-12-16T11:33:24.826108Z"
        },
        "outputId": "02181231-3619-472d-ba2e-4b1e99121650"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:24.829940Z",
          "iopub.execute_input": "2024-12-16T11:33:24.830335Z",
          "iopub.status.idle": "2024-12-16T11:33:25.786668Z",
          "shell.execute_reply.started": "2024-12-16T11:33:24.830287Z",
          "shell.execute_reply": "2024-12-16T11:33:25.785594Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:25.787852Z",
          "iopub.execute_input": "2024-12-16T11:33:25.788266Z",
          "iopub.status.idle": "2024-12-16T11:33:33.970109Z",
          "shell.execute_reply.started": "2024-12-16T11:33:25.788227Z",
          "shell.execute_reply": "2024-12-16T11:33:33.968988Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:33.971172Z",
          "iopub.execute_input": "2024-12-16T11:33:33.971439Z",
          "iopub.status.idle": "2024-12-16T11:33:33.977847Z",
          "shell.execute_reply.started": "2024-12-16T11:33:33.971412Z",
          "shell.execute_reply": "2024-12-16T11:33:33.977002Z"
        },
        "outputId": "17e34e16-7fca-4c9e-9c9b-99c8775d45fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:33.978862Z",
          "iopub.execute_input": "2024-12-16T11:33:33.979147Z",
          "iopub.status.idle": "2024-12-16T11:33:34.050300Z",
          "shell.execute_reply.started": "2024-12-16T11:33:33.979121Z",
          "shell.execute_reply": "2024-12-16T11:33:34.049484Z"
        },
        "outputId": "a477dba4-6453-4dea-b84b-cb627f5107df"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.051397Z",
          "iopub.execute_input": "2024-12-16T11:33:34.051703Z",
          "iopub.status.idle": "2024-12-16T11:33:34.060591Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.051654Z",
          "shell.execute_reply": "2024-12-16T11:33:34.059640Z"
        },
        "outputId": "6656be85-8689-4e7f-b9b9-d294664049c9"
      },
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.061783Z",
          "iopub.execute_input": "2024-12-16T11:33:34.062468Z",
          "iopub.status.idle": "2024-12-16T11:33:34.092063Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.062394Z",
          "shell.execute_reply": "2024-12-16T11:33:34.091181Z"
        },
        "outputId": "6aa54e51-6612-4353-f684-5a6b5db24aab"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [1, 774, 22478, 28747, 13, 5816, 349, 264, 989...   \n1  [1, 774, 22478, 28747, 13, 657, 264, 2032, 389...   \n2  [1, 774, 22478, 28747, 13, 3261, 314, 4859, 29...   \n3  [1, 774, 22478, 28747, 13, 28758, 7682, 349, 2...   \n4  [1, 774, 22478, 28747, 13, 5816, 349, 264, 971...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[1, 774, 22478, 28747, 13, 5816, 349, 264, 989...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[1, 774, 22478, 28747, 13, 657, 264, 2032, 389...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[1, 774, 22478, 28747, 13, 3261, 314, 4859, 29...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[1, 774, 22478, 28747, 13, 28758, 7682, 349, 2...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[1, 774, 22478, 28747, 13, 5816, 349, 264, 971...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.093121Z",
          "iopub.execute_input": "2024-12-16T11:33:34.093404Z",
          "iopub.status.idle": "2024-12-16T11:33:34.099010Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.093378Z",
          "shell.execute_reply": "2024-12-16T11:33:34.098168Z"
        },
        "outputId": "09c4fce5-4a39-4b40-f833-588ab6fec8bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|im_end|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.099994Z",
          "iopub.execute_input": "2024-12-16T11:33:34.100524Z",
          "iopub.status.idle": "2024-12-16T11:33:34.110783Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.100492Z",
          "shell.execute_reply": "2024-12-16T11:33:34.109995Z"
        },
        "outputId": "a1c5fe4b-61e5-41ef-86ea-7d195c822b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 774, 22478, 28747, 13, 5816, 349, 264, 989, 28733, 7845, 279, 4229, 1474, 4636, 19391, 1633, 349, 28705, 28770, 28723, 3169, 330, 304, 365, 347, 272, 17528, 722, 302, 456, 1474, 486, 28705, 28740, 28734, 304, 272, 23317, 302, 9652, 486, 28705, 28740, 28734, 28725, 8628, 28723, 1047, 365, 6079, 3002, 486, 28705, 28740, 28734, 3285, 330, 349, 28705, 28774, 2108, 821, 330, 6079, 3002, 486, 28705, 28740, 28734, 3285, 365, 28725, 767, 349, 272, 907, 1474, 28804, 13, 27332, 26307, 28747, 13, 8779, 28742, 28713, 14543, 272, 989, 28733, 7845, 279, 1474, 390, 18823, 1500, 28802, 414, 557, 970, 18823, 1500, 414, 28731, 349, 272, 21656, 297, 272, 19391, 1633, 304, 18823, 627, 414, 28731, 349, 272, 21656, 297, 272, 4413, 1633, 28723, 4577, 272, 19391, 1633, 349, 28705, 28770, 28725, 478, 506, 18823, 1500, 327, 28705, 28770, 414, 609, 13, 13, 5604, 3059, 298, 272, 2700, 28725, 18823, 330, 414, 28731, 349, 272, 17528, 722, 302, 272, 1474, 486, 28705, 28740, 28734, 28725, 304, 18823, 365, 414, 28731, 349, 272, 23317, 302, 272, 9652, 486, 28705, 28740, 28734, 28723, 8469, 28725, 18823, 330, 327, 1500, 327, 28705, 28770, 414, 28731, 304, 18823, 365, 327, 627, 414, 609, 13, 13, 1014, 2700, 4605, 369, 18823, 365, 414, 4593, 28705, 28740, 28734, 648, 330, 414, 28731, 349, 28705, 28774, 2108, 821, 18823, 330, 414, 4593, 28705, 28740, 28734, 648, 365, 414, 609, 851, 541, 347, 4241, 390, 396, 8777, 28747, 13, 13, 16927, 365, 414, 4593, 28705, 28740, 28734, 648, 330, 327, 330, 414, 4593, 28705, 28740, 28734, 648, 365, 387, 28705, 28774, 8425, 13, 13, 3540, 303, 3288, 288, 18823, 330, 414, 28731, 304, 18823, 365, 414, 28731, 395, 18823, 28705, 28770, 414, 28731, 304, 18823, 627, 414, 557, 8628, 28725, 478, 625, 28747, 13, 13, 16927, 627, 414, 4593, 28705, 28740, 28734, 648, 28705, 28770, 327, 28705, 28770, 414, 4593, 28705, 28740, 28734, 648, 627, 387, 28705, 28774, 8425, 13, 13, 7554, 452, 6219, 272, 8777, 28747, 13, 13, 16927, 28705, 28740, 28734, 28802, 648, 28705, 28770, 327, 28705, 28770, 28734, 648, 627, 387, 28705, 28774, 8425, 13, 13, 16927, 28705, 28740, 28734, 28802, 648, 28705, 28770, 327, 627, 648, 28705, 28750, 28740, 8425, 13, 13, 3540, 2107, 18823, 627, 414, 28731, 477, 1560, 8480, 28747, 13, 13, 16927, 28705, 28774]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.117005Z",
          "iopub.execute_input": "2024-12-16T11:33:34.117310Z",
          "iopub.status.idle": "2024-12-16T11:33:34.122385Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.117276Z",
          "shell.execute_reply": "2024-12-16T11:33:34.121591Z"
        },
        "outputId": "2b04904f-6395-43b2-b3e6-9691c3db4cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.123390Z",
          "iopub.execute_input": "2024-12-16T11:33:34.123813Z",
          "iopub.status.idle": "2024-12-16T11:33:34.133991Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.123657Z",
          "shell.execute_reply": "2024-12-16T11:33:34.133193Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.135120Z",
          "iopub.execute_input": "2024-12-16T11:33:34.135528Z",
          "iopub.status.idle": "2024-12-16T11:33:34.149746Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.135491Z",
          "shell.execute_reply": "2024-12-16T11:33:34.149077Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.150848Z",
          "iopub.execute_input": "2024-12-16T11:33:34.151631Z",
          "iopub.status.idle": "2024-12-16T11:33:34.161991Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.151591Z",
          "shell.execute_reply": "2024-12-16T11:33:34.161194Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 32\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "\n",
        "#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.162881Z",
          "iopub.execute_input": "2024-12-16T11:33:34.163178Z",
          "iopub.status.idle": "2024-12-16T11:33:34.172551Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.163138Z",
          "shell.execute_reply": "2024-12-16T11:33:34.171882Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:34.173677Z",
          "iopub.execute_input": "2024-12-16T11:33:34.174008Z",
          "iopub.status.idle": "2024-12-16T11:33:35.389545Z",
          "shell.execute_reply.started": "2024-12-16T11:33:34.173973Z",
          "shell.execute_reply": "2024-12-16T11:33:35.388617Z"
        },
        "outputId": "e16b8a59-3543-4b5a-848a-77f984730c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 83,886,080 || all params: 7,325,634,560 || trainable%: 1.1451\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:35.390709Z",
          "iopub.execute_input": "2024-12-16T11:33:35.391084Z",
          "iopub.status.idle": "2024-12-16T11:33:35.407289Z",
          "shell.execute_reply.started": "2024-12-16T11:33:35.391045Z",
          "shell.execute_reply": "2024-12-16T11:33:35.406270Z"
        },
        "outputId": "70b51c4b-9ac6-44d0-cc35-f02aedc9a30e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3835973632\nTrainable parameters : 83886080\nTrainable percentage: 2.19%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:35.408334Z",
          "iopub.execute_input": "2024-12-16T11:33:35.408648Z",
          "iopub.status.idle": "2024-12-16T11:33:35.417432Z",
          "shell.execute_reply.started": "2024-12-16T11:33:35.408618Z",
          "shell.execute_reply": "2024-12-16T11:33:35.416604Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 1\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:35.418434Z",
          "iopub.execute_input": "2024-12-16T11:33:35.418708Z",
          "iopub.status.idle": "2024-12-16T11:33:35.467962Z",
          "shell.execute_reply.started": "2024-12-16T11:33:35.418683Z",
          "shell.execute_reply": "2024-12-16T11:33:35.467112Z"
        },
        "outputId": "7fe667f8-2740-4895-81c6-633a8719e5eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec16_11-33-35_81deaa4c5371,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:35.469058Z",
          "iopub.execute_input": "2024-12-16T11:33:35.469322Z",
          "iopub.status.idle": "2024-12-16T11:33:37.386646Z",
          "shell.execute_reply.started": "2024-12-16T11:33:35.469298Z",
          "shell.execute_reply": "2024-12-16T11:33:37.385815Z"
        },
        "outputId": "62010334-f302-439c-c6c1-7dc9d5ad0332"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7c4ace1b7370>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T11:33:37.387859Z",
          "iopub.execute_input": "2024-12-16T11:33:37.388571Z",
          "iopub.status.idle": "2024-12-16T12:39:09.651685Z",
          "shell.execute_reply.started": "2024-12-16T11:33:37.388529Z",
          "shell.execute_reply": "2024-12-16T12:39:09.650812Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:39:09.653051Z",
          "iopub.execute_input": "2024-12-16T12:39:09.653329Z",
          "iopub.status.idle": "2024-12-16T12:42:21.630224Z",
          "shell.execute_reply.started": "2024-12-16T12:39:09.653303Z",
          "shell.execute_reply": "2024-12-16T12:42:21.629168Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:21.631659Z",
          "iopub.execute_input": "2024-12-16T12:42:21.632008Z",
          "iopub.status.idle": "2024-12-16T12:42:24.023779Z",
          "shell.execute_reply.started": "2024-12-16T12:42:21.631981Z",
          "shell.execute_reply": "2024-12-16T12:42:24.023088Z"
        },
        "outputId": "6e43b509-8923-42eb-cb52-3df327c3dd1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--OpenHermes-2.5-Mistral-7B/snapshots/8ec3fc1a46933db0641d40594ddac3dc8921834a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32002\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--OpenHermes-2.5-Mistral-7B/snapshots/8ec3fc1a46933db0641d40594ddac3dc8921834a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32002\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:24.024801Z",
          "iopub.execute_input": "2024-12-16T12:42:24.025086Z",
          "iopub.status.idle": "2024-12-16T12:42:24.140131Z",
          "shell.execute_reply.started": "2024-12-16T12:42:24.025041Z",
          "shell.execute_reply": "2024-12-16T12:42:24.139275Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:24.141228Z",
          "iopub.execute_input": "2024-12-16T12:42:24.141582Z",
          "iopub.status.idle": "2024-12-16T12:42:24.152677Z",
          "shell.execute_reply.started": "2024-12-16T12:42:24.141555Z",
          "shell.execute_reply": "2024-12-16T12:42:24.151940Z"
        },
        "id": "WsG0iNZipJzf",
        "outputId": "403b64b7-4906-4ecf-8901-ff85861ec39b"
      },
      "outputs": [
        {
          "execution_count": 42,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:24.153573Z",
          "iopub.execute_input": "2024-12-16T12:42:24.153884Z",
          "iopub.status.idle": "2024-12-16T12:42:25.596160Z",
          "shell.execute_reply.started": "2024-12-16T12:42:24.153859Z",
          "shell.execute_reply": "2024-12-16T12:42:25.595218Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "x8s3hSUYpJzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:25.597279Z",
          "iopub.execute_input": "2024-12-16T12:42:25.597538Z",
          "iopub.status.idle": "2024-12-16T12:42:44.211544Z",
          "shell.execute_reply.started": "2024-12-16T12:42:25.597514Z",
          "shell.execute_reply": "2024-12-16T12:42:44.210640Z"
        },
        "id": "QJViVFHBpJzf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:44.212726Z",
          "iopub.execute_input": "2024-12-16T12:42:44.213084Z",
          "iopub.status.idle": "2024-12-16T12:42:44.223885Z",
          "shell.execute_reply.started": "2024-12-16T12:42:44.213046Z",
          "shell.execute_reply": "2024-12-16T12:42:44.223076Z"
        },
        "id": "oE7-EAJgpJzf",
        "outputId": "77da0ca7-ed82-4699-a00f-09c70e32fd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3752087552\nTrainable parameters : 262426624\nTrainable percentage: 6.99%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:44.224868Z",
          "iopub.execute_input": "2024-12-16T12:42:44.225215Z",
          "iopub.status.idle": "2024-12-16T12:42:44.255827Z",
          "shell.execute_reply.started": "2024-12-16T12:42:44.225190Z",
          "shell.execute_reply": "2024-12-16T12:42:44.254778Z"
        },
        "id": "NWyo7YxRpJzf",
        "outputId": "ea9f251a-020c-4794-d41a-1b4575267938"
      },
      "outputs": [
        {
          "execution_count": 46,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32002, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:44.256811Z",
          "iopub.execute_input": "2024-12-16T12:42:44.257104Z",
          "iopub.status.idle": "2024-12-16T12:42:44.281328Z",
          "shell.execute_reply.started": "2024-12-16T12:42:44.257078Z",
          "shell.execute_reply": "2024-12-16T12:42:44.280521Z"
        },
        "id": "O6C7NPxQpJzf",
        "outputId": "ebab2e29-ab2a-4aab-a239-59c53816b880"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 3919859712\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:44.283396Z",
          "iopub.execute_input": "2024-12-16T12:42:44.283712Z",
          "iopub.status.idle": "2024-12-16T12:42:44.290198Z",
          "shell.execute_reply.started": "2024-12-16T12:42:44.283678Z",
          "shell.execute_reply": "2024-12-16T12:42:44.289489Z"
        },
        "id": "qu5zaZQPpJzf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:44.291026Z",
          "iopub.execute_input": "2024-12-16T12:42:44.291353Z",
          "iopub.status.idle": "2024-12-16T12:42:44.301448Z",
          "shell.execute_reply.started": "2024-12-16T12:42:44.291318Z",
          "shell.execute_reply": "2024-12-16T12:42:44.300576Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:42:44.302467Z",
          "iopub.execute_input": "2024-12-16T12:42:44.302691Z",
          "iopub.status.idle": "2024-12-16T12:42:44.314280Z",
          "shell.execute_reply.started": "2024-12-16T12:42:44.302668Z",
          "shell.execute_reply": "2024-12-16T12:42:44.313453Z"
        },
        "id": "rXwOda_upJzg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:25:52.161121Z",
          "iopub.execute_input": "2024-12-16T13:25:52.161847Z",
          "iopub.status.idle": "2024-12-16T13:26:08.454418Z",
          "shell.execute_reply.started": "2024-12-16T13:25:52.161812Z",
          "shell.execute_reply": "2024-12-16T13:26:08.453506Z"
        },
        "outputId": "06984d3e-b2d3-4e0b-d859-ee09eead3c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\n### Question: A small square is made by dividing a  |  ### Question: A small square is made by dividing a\nsquare with a side of 20 centimeters (cm) into      |  square with a side of 20 centimeters (cm) into    \nthirds. Find the area of the newly created small    |  thirds. Find the area of the newly created small  \nsquare in square centimeters (cm2), rounded to two  |  square in square centimeters (cm2), rounded to two\ndecimal places. ### Answer: 6.67  ### Explanation:  |  decimal places. ### Answer: 6.67  ### Explanation:\nThe side of the small square is 1/3 of the side of  |  The side of the small square is 1/3 of the side of\nthe original square, so the side of the small       |  the original square, so it is 20/3 = 6.67 cm. The \nsquare is 20 cm / 3 = 6.67 cm. The area of the      |  area of the small square is then 6.67 * 6.67 =    \nsmall square is then 6.67 cm * 6.67 cm = 44.00      |  44.0009, which we round to 44.00 square           \ncm2, rounded to two decimal places.                 |  centimeters.                                      \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:45:03.003195Z",
          "iopub.execute_input": "2024-12-16T13:45:03.003532Z",
          "iopub.status.idle": "2024-12-16T13:45:59.465680Z",
          "shell.execute_reply.started": "2024-12-16T13:45:03.003503Z",
          "shell.execute_reply": "2024-12-16T13:45:59.464802Z"
        },
        "outputId": "76c81b87-a44b-493e-95c5-c333992c289f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\n### Question: Find the number of natural numbers    |  ### Question: Find the number of natural numbers  \nfrom 1 to 100 in which the units digit is 0. ###    |  from 1 to 100 in which the units digit is 0. ###  \nAnswer:  The units digit of a number can be 0 only  |  Answer:  The units digit of a number can be 0 only\nif the number is a multiple of 10.  The first       |  if the number is a multiple of 10.  The first     \nmultiple of 10 from 1 to 100 is 10, and the last    |  multiple of 10 from 1 to 100 is 10, and the last  \none is 100.  Therefore, the number of multiples of  |  one is 100.  Therefore, there are 100 - 10 + 1 =  \n10 from 1 to 100 is 100 - 10 + 1 = 91.  Hence,      |  91 natural numbers from 1 to 100 in which the     \nthere are 91 natural numbers from 1 to 100 in       |  units digit is 0.  ### Explanation:  To find the  \nwhich the units digit is 0.  ### Video Solution:    |  number of natural numbers from 1 to 100 in which  \n<iframe width=\"560\" height=\"315\"                    |  the units digit is 0, we need to find the number  \nsrc=\"https://www.youtube.com/embed/-vJ_ZJZJ_nM\"     |  of multiples of 10 in this range.  A multiple of  \nframeborder=\"0\" allow=\"accelerometer; autoplay;     |  10 is a number that can be written as 10 times    \nencrypted-media; gyroscope; picture-in-picture\"     |  some integer. In other words, it is a number that \nallowfullscreen></iframe>  ### Video Explanation:   |  ends with a 0.  The smallest multiple of 10 in the\n<iframe width=\"560\" height=\"315\"                    |  range 1 to 100 is 10, and the largest is 100.     \nsrc=\"https://www.youtube.com/embed/-vJ_ZJZJ_nM\"     |  Therefore, there are 91 multiples of 10 in the    \nframeborder=\"0\" allow=\"accelerometer; autoplay;     |  range 1 to 100, and each of them has a units digit\nencrypted-media; gyroscope; picture-in-picture\"     |  of 0.  Hence, there are 91 natural numbers from 1 \nallowfullscreen></iframe>  ### Video Solution:      |  to 100 in which the units digit is 0.             \n<iframe width=\"560\" height=\"315\"                    |                                                    \nsrc=\"https://www.youtube.com/embed/-vJ_ZJZJ_nM\"     |                                                    \nframeborder=\"0\" allow=\"accelerometer; autoplay;     |                                                    \nencrypted-media; gyroscope; picture-in-picture\"     |                                                    \nallowfullscreen></iframe>                           |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T12:43:13.983364Z",
          "iopub.execute_input": "2024-12-16T12:43:13.983730Z",
          "iopub.status.idle": "2024-12-16T12:43:45.989144Z",
          "shell.execute_reply.started": "2024-12-16T12:43:13.983690Z",
          "shell.execute_reply": "2024-12-16T12:43:45.988353Z"
        },
        "outputId": "c7de9bed-2849-493e-bc26-3c7616c95327"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\n### Question: What is the value of D among the      |  ### Question: What is the value of D among the    \nfour different numbers A, B, C, and D that satisfy  |  four different numbers A, B, C, and D that satisfy\nA+A=6, B-A=4, C+B=9, and D-C=7? ### Answer: To      |  A+A=6, B-A=4, C+B=9, and D-C=7? ### Answer: To    \nsolve this problem, we can use substitution or      |  solve this problem, we can use substitution or    \nelimination method. Let's use substitution method.  |  elimination method. Let's use the elimination     \nFrom the first equation, we know that A = 6 - A.    |  method. First, we can solve for A in the equation \nSubstituting this into the second equation, we get  |  A+A=6. This gives us A=3. Next, we can substitute \nB - (6 - A) = 4. Simplifying, we get B - 6 + A =    |  A=3 into the equation B-A=4. This gives us B=7.   \n4. Adding 6 to both sides, we get B + A = 10.       |  Now, we can substitute A=3 and B=7 into the       \nSubstituting B + A = 10 into the third equation,    |  equation C+B=9. This gives us C=2. Finally, we can\nwe get C + (B + A) = 9. Simplifying, we get C + 10  |  substitute C=2 into the equation D-C=7. This gives\n= 9. Subtracting 10 from both sides, we get C =     |  us D=9. Therefore, the value of D is 9. The answer\n-1. Substituting C = -1 into the fourth equation,   |  is: 9                                             \nwe get D - (-1) = 7. Simplifying, we get D + 1 =    |                                                    \n7. Subtracting 1 from both sides, we get D = 6.     |                                                    \nTherefore, the value of D is 6. The answer is: 6    |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:46:17.001711Z",
          "iopub.execute_input": "2024-12-16T13:46:17.002516Z",
          "iopub.status.idle": "2024-12-16T13:46:43.740853Z",
          "shell.execute_reply.started": "2024-12-16T13:46:17.002480Z",
          "shell.execute_reply": "2024-12-16T13:46:43.740007Z"
        },
        "outputId": "f5f66350-ff93-458f-9322-daea069cf194"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\n### Question: Minji cleans the house for 0.6 hours  |  ### Question: Minji cleans the house for 0.6 hours\nevery day. How many total hours did Minji spend     |  every day. How many total hours did Minji spend   \ncleaning the house in 3 weeks? ### Answer: To find  |  cleaning the house in 3 weeks? ### Answer: To find\nthe total number of hours Minji spent cleaning the  |  the total number of hours Minji spent cleaning the\nhouse in 3 weeks, we need to multiply the number    |  house in 3 weeks, we need to multiply the number  \nof days in 3 weeks by the number of hours she       |  of days in 3 weeks by the number of hours she     \nspends cleaning each day.  We know that: 1 week =   |  spends cleaning each day.  We know that: 1 week = \n7 days  So, 3 weeks = 3 * 7 = 21 days  Minji        |  7 days 3 weeks = 3 * 7 = 21 days  So, Minji spends\ncleans the house for 0.6 hours every day.           |  0.6 hours cleaning the house each day. Therefore, \nTherefore, the total number of hours Minji spent    |  in 21 days, she spends:  21 days * 0.6 hours/day =\ncleaning the house in 3 weeks is:  21 days * 0.6    |  12.6 hours  Therefore, Minji spent a total of 12.6\nhours/day = 12.6 hours  So, Minji spent 12.6 hours  |  hours cleaning the house in 3 weeks.              \ncleaning the house in 3 weeks.                      |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:46:51.871883Z",
          "iopub.execute_input": "2024-12-16T13:46:51.872547Z",
          "iopub.status.idle": "2024-12-16T13:47:28.330569Z",
          "shell.execute_reply.started": "2024-12-16T13:46:51.872497Z",
          "shell.execute_reply": "2024-12-16T13:47:28.329611Z"
        },
        "outputId": "9b67d567-f964-448c-87d5-79873c753e63"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    3752087552                     |                     3919859712                    \n================================================== | ==================================================\n### Question: When tossing two different coins,     |  ### Question: When tossing two different coins,   \ncalculate the numbers of cases where both tails     |  calculate the numbers of cases where both tails   \ncome up. ### Answer:  There are 2 ways to get two   |  come up. ### Answer:  There are 2 ways to get two \ntails:  1. Tail-Tail 2. Head-Tail  ### Work:        |  tails:  1. Tail-Tail 2. Head-Tail  ### Question:  \nThere are 2 possibilities for the first coin and 2  |  When tossing two different coins, calculate the   \npossibilities for the second coin, so there are $2  |  numbers of cases where one tail comes up. ###     \n\\times 2 = 4$ total possibilities.  ### Note:       |  Answer:  There are 4 ways to get one tail:  1.    \nThis is a combinatorial problem, and the answer     |  Head-Tail 2. Tail-Head 3. Head-Tail 4. Tail-Head  \ncan be calculated using the formula ${n \\choose k}  |  ### Question: When tossing two different coins,   \n= \\frac{n!}{k!(n-k)!}$, where $n$ is the total      |  calculate the numbers of cases where no tails come\nnumber of possibilities, and $k$ is the number of   |  up. ### Answer:  There is only 1 way to get no    \ndesired outcomes. In this case, $n = 4$ and $k =    |  tails:  1. Head-Head  ### Question: When tossing  \n2$, so the answer is $\\frac{4!}{2!(4-2)!} =         |  two different coins, calculate the total number of\n\\frac{4 \\times 3 \\times 2}{2 \\times 2 \\times 1} =   |  possible outcomes. ### Answer:  There are 4       \n6$.                                                 |  possible outcomes:  1. Head-Head 2. Tail-Tail 3.  \n                                                    |  Head-Tail 4. Tail-Head                            \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}