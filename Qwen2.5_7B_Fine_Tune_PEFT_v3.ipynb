{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/Qwen2.5_7B_Fine_Tune_PEFT_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:37:56.988445Z",
          "iopub.execute_input": "2024-12-08T04:37:56.988762Z",
          "iopub.status.idle": "2024-12-08T04:39:00.508828Z",
          "shell.execute_reply.started": "2024-12-08T04:37:56.988732Z",
          "shell.execute_reply": "2024-12-08T04:39:00.507593Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-12-08T04:39:00.510694Z",
          "iopub.execute_input": "2024-12-08T04:39:00.510961Z",
          "iopub.status.idle": "2024-12-08T04:39:19.982431Z",
          "shell.execute_reply.started": "2024-12-08T04:39:00.510935Z",
          "shell.execute_reply": "2024-12-08T04:39:19.981607Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-12-08T04:39:19.983649Z",
          "iopub.execute_input": "2024-12-08T04:39:19.983915Z",
          "iopub.status.idle": "2024-12-08T04:39:19.990231Z",
          "shell.execute_reply.started": "2024-12-08T04:39:19.983889Z",
          "shell.execute_reply": "2024-12-08T04:39:19.989419Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Qwen/Qwen2.5-7B'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:39:19.992984Z",
          "iopub.execute_input": "2024-12-08T04:39:19.993417Z",
          "iopub.status.idle": "2024-12-08T04:39:20.020529Z",
          "shell.execute_reply.started": "2024-12-08T04:39:19.993367Z",
          "shell.execute_reply": "2024-12-08T04:39:20.019568Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "  if base == True:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype = torch.float16,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "  else:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = True,\n",
        "      bnb_4bit_quant_type = 'nf4',\n",
        "      bnb_4bit_compute_dtype = torch.float16,\n",
        "      bnb_4bit_use_double_quant = True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config = bnb_config,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:39:20.021653Z",
          "iopub.execute_input": "2024-12-08T04:39:20.022015Z",
          "iopub.status.idle": "2024-12-08T04:39:20.033003Z",
          "shell.execute_reply.started": "2024-12-08T04:39:20.021977Z",
          "shell.execute_reply": "2024-12-08T04:39:20.032242Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:39:20.033825Z",
          "iopub.execute_input": "2024-12-08T04:39:20.034476Z",
          "iopub.status.idle": "2024-12-08T04:47:01.972394Z",
          "shell.execute_reply.started": "2024-12-08T04:39:20.034449Z",
          "shell.execute_reply": "2024-12-08T04:47:01.971557Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:01.973684Z",
          "iopub.execute_input": "2024-12-08T04:47:01.974091Z",
          "iopub.status.idle": "2024-12-08T04:47:01.982104Z",
          "shell.execute_reply.started": "2024-12-08T04:47:01.974052Z",
          "shell.execute_reply": "2024-12-08T04:47:01.981316Z"
        },
        "outputId": "57b57d3f-d404-478e-8b18-a52cf654f72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 4352972288\nTrainable parameters : 1090199040\nTrainable percentage: 25.04%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:01.983196Z",
          "iopub.execute_input": "2024-12-08T04:47:01.983445Z",
          "iopub.status.idle": "2024-12-08T04:47:06.605162Z",
          "shell.execute_reply.started": "2024-12-08T04:47:01.983421Z",
          "shell.execute_reply": "2024-12-08T04:47:06.604303Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'microsoft/orca-math-word-problems-200k'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:06.607589Z",
          "iopub.execute_input": "2024-12-08T04:47:06.607870Z",
          "iopub.status.idle": "2024-12-08T04:47:06.611853Z",
          "shell.execute_reply.started": "2024-12-08T04:47:06.607842Z",
          "shell.execute_reply": "2024-12-08T04:47:06.610929Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:06.615640Z",
          "iopub.execute_input": "2024-12-08T04:47:06.616398Z",
          "iopub.status.idle": "2024-12-08T04:47:06.623812Z",
          "shell.execute_reply.started": "2024-12-08T04:47:06.616357Z",
          "shell.execute_reply": "2024-12-08T04:47:06.623059Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:06.624822Z",
          "iopub.execute_input": "2024-12-08T04:47:06.625126Z",
          "iopub.status.idle": "2024-12-08T04:47:14.002274Z",
          "shell.execute_reply.started": "2024-12-08T04:47:06.625068Z",
          "shell.execute_reply": "2024-12-08T04:47:14.001443Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(10000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.003313Z",
          "iopub.execute_input": "2024-12-08T04:47:14.003595Z",
          "iopub.status.idle": "2024-12-08T04:47:14.009779Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.003569Z",
          "shell.execute_reply": "2024-12-08T04:47:14.008878Z"
        },
        "id": "dAauCr3Uu_t8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.010828Z",
          "iopub.execute_input": "2024-12-08T04:47:14.011090Z",
          "iopub.status.idle": "2024-12-08T04:47:14.049563Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.011065Z",
          "shell.execute_reply": "2024-12-08T04:47:14.048861Z"
        },
        "outputId": "56391269-ccf5-47ce-f271-c2dada4bb717"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.050463Z",
          "iopub.execute_input": "2024-12-08T04:47:14.050710Z",
          "iopub.status.idle": "2024-12-08T04:47:14.058678Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.050685Z",
          "shell.execute_reply": "2024-12-08T04:47:14.057909Z"
        },
        "outputId": "8ae49933-9411-44a1-ed86-37eda0f41eaa"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.059730Z",
          "iopub.execute_input": "2024-12-08T04:47:14.060077Z",
          "iopub.status.idle": "2024-12-08T04:47:14.070509Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.060031Z",
          "shell.execute_reply": "2024-12-08T04:47:14.069640Z"
        },
        "outputId": "7217f34f-e0dc-425b-e2cb-9569eb91a7bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['question', 'answer']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.071599Z",
          "iopub.execute_input": "2024-12-08T04:47:14.071878Z",
          "iopub.status.idle": "2024-12-08T04:47:14.080733Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.071855Z",
          "shell.execute_reply": "2024-12-08T04:47:14.080052Z"
        },
        "id": "LucBXnTmu_t9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def preprocess(examples):\n",
        "  input = examples['question']\n",
        "  output = examples['answer']\n",
        "\n",
        "  text = prompt_format.format(input, output) + EOS_TOKEN\n",
        "  return {'prompt' : text}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.081702Z",
          "iopub.execute_input": "2024-12-08T04:47:14.081950Z",
          "iopub.status.idle": "2024-12-08T04:47:14.093355Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.081924Z",
          "shell.execute_reply": "2024-12-08T04:47:14.092514Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(preprocess, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.094421Z",
          "iopub.execute_input": "2024-12-08T04:47:14.094678Z",
          "iopub.status.idle": "2024-12-08T04:47:14.580561Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.094655Z",
          "shell.execute_reply": "2024-12-08T04:47:14.579454Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.581638Z",
          "iopub.execute_input": "2024-12-08T04:47:14.581922Z",
          "iopub.status.idle": "2024-12-08T04:47:14.586637Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.581893Z",
          "shell.execute_reply": "2024-12-08T04:47:14.585803Z"
        },
        "outputId": "e57a1ad6-8a27-45b2-f42d-21f3b13977de"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.587690Z",
          "iopub.execute_input": "2024-12-08T04:47:14.587933Z",
          "iopub.status.idle": "2024-12-08T04:47:14.612530Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.587908Z",
          "shell.execute_reply": "2024-12-08T04:47:14.611718Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:14.613560Z",
          "iopub.execute_input": "2024-12-08T04:47:14.613801Z",
          "iopub.status.idle": "2024-12-08T04:47:23.850664Z",
          "shell.execute_reply.started": "2024-12-08T04:47:14.613778Z",
          "shell.execute_reply": "2024-12-08T04:47:23.849766Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:23.851986Z",
          "iopub.execute_input": "2024-12-08T04:47:23.852380Z",
          "iopub.status.idle": "2024-12-08T04:47:23.859986Z",
          "shell.execute_reply.started": "2024-12-08T04:47:23.852341Z",
          "shell.execute_reply": "2024-12-08T04:47:23.859163Z"
        },
        "outputId": "c0775685-a79f-4ec8-ad8f-cb6b9dc5a88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:23.861081Z",
          "iopub.execute_input": "2024-12-08T04:47:23.861367Z",
          "iopub.status.idle": "2024-12-08T04:47:23.932432Z",
          "shell.execute_reply.started": "2024-12-08T04:47:23.861342Z",
          "shell.execute_reply": "2024-12-08T04:47:23.931550Z"
        },
        "outputId": "d11cfd77-ac38-4804-f17b-d255e70b0247"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:23.933575Z",
          "iopub.execute_input": "2024-12-08T04:47:23.933855Z",
          "iopub.status.idle": "2024-12-08T04:47:23.946363Z",
          "shell.execute_reply.started": "2024-12-08T04:47:23.933828Z",
          "shell.execute_reply": "2024-12-08T04:47:23.945554Z"
        },
        "outputId": "d90b1f1d-737b-437f-ec08-02ea8f75c835"
      },
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:23.947512Z",
          "iopub.execute_input": "2024-12-08T04:47:23.947857Z",
          "iopub.status.idle": "2024-12-08T04:47:23.971766Z",
          "shell.execute_reply.started": "2024-12-08T04:47:23.947819Z",
          "shell.execute_reply": "2024-12-08T04:47:23.970926Z"
        },
        "outputId": "8b98e403-d2a9-4089-c4d1-91f019ef5dc9"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [14374, 15846, 510, 3862, 374, 264, 1378, 4834...   \n1  [14374, 15846, 510, 641, 264, 2409, 3745, 11, ...   \n2  [14374, 15846, 510, 37575, 5780, 311, 264, 261...   \n3  [14374, 15846, 510, 72749, 374, 3330, 311, 477...   \n4  [14374, 15846, 510, 3862, 374, 264, 51424, 347...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[14374, 15846, 510, 3862, 374, 264, 1378, 4834...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[14374, 15846, 510, 641, 264, 2409, 3745, 11, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[14374, 15846, 510, 37575, 5780, 311, 264, 261...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[14374, 15846, 510, 72749, 374, 3330, 311, 477...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[14374, 15846, 510, 3862, 374, 264, 51424, 347...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:23.972679Z",
          "iopub.execute_input": "2024-12-08T04:47:23.972917Z",
          "iopub.status.idle": "2024-12-08T04:47:23.977774Z",
          "shell.execute_reply.started": "2024-12-08T04:47:23.972893Z",
          "shell.execute_reply": "2024-12-08T04:47:23.976920Z"
        },
        "outputId": "782d9a10-2ce2-4552-a611-1eff7a1256c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:23.978739Z",
          "iopub.execute_input": "2024-12-08T04:47:23.978979Z",
          "iopub.status.idle": "2024-12-08T04:47:23.990705Z",
          "shell.execute_reply.started": "2024-12-08T04:47:23.978955Z",
          "shell.execute_reply": "2024-12-08T04:47:23.989977Z"
        },
        "outputId": "8bd25576-0b18-4e61-e472-f712ab88f03d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[14374, 15846, 510, 3862, 374, 264, 1378, 48342, 5810, 1372, 6693, 22008, 1992, 374, 220, 18, 13, 6771, 362, 323, 425, 387, 279, 74762, 315, 419, 1372, 553, 220, 16, 15, 323, 279, 26313, 315, 12804, 553, 220, 16, 15, 11, 15576, 13, 1416, 425, 54916, 553, 220, 16, 15, 5519, 362, 374, 220, 24, 2686, 1091, 362, 54916, 553, 220, 16, 15, 5519, 425, 11, 1128, 374, 279, 1156, 1372, 5267, 14374, 21806, 510, 10061, 594, 78064, 279, 1378, 48342, 1372, 438, 17767, 57319, 1124, 701, 1380, 17767, 1599, 1124, 8, 374, 279, 15723, 304, 279, 22008, 1992, 323, 17767, 809, 1124, 8, 374, 279, 15723, 304, 279, 6174, 1992, 13, 8704, 279, 22008, 1992, 374, 220, 18, 11, 582, 614, 17767, 1599, 284, 220, 18, 1124, 3593, 11190, 311, 279, 3491, 11, 17767, 362, 1124, 8, 374, 279, 74762, 315, 279, 1372, 553, 220, 16, 15, 11, 323, 17767, 425, 1124, 8, 374, 279, 26313, 315, 279, 12804, 553, 220, 16, 15, 13, 15277, 11, 17767, 362, 284, 1599, 284, 220, 18, 1124, 8, 323, 17767, 425, 284, 809, 1124, 3593, 785, 3491, 5302, 429, 17767, 425, 1124, 15136, 220, 16, 15, 488, 362, 1124, 8, 374, 220, 24, 2686, 1091, 17767, 362, 1124, 15136, 220, 16, 15, 488, 425, 1124, 568, 1096, 646, 387, 5326, 438, 458, 23606, 1447, 78045, 425, 1124, 15136, 220, 16, 15, 488, 362, 284, 362, 1124, 15136, 220, 16, 15, 488, 425, 481, 220, 24, 1124, 2533, 3136, 3696, 10607, 17767, 362, 1124, 8, 323, 17767, 425, 1124, 8, 448, 17767, 220, 18, 1124, 8, 323, 17767, 809, 1124, 701, 15576, 11, 582, 633, 1447, 78045, 809, 1124, 15136, 220, 16, 15, 488, 220, 18, 284, 220, 18, 1124, 15136, 220, 16, 15, 488, 809, 481, 220, 24, 1124, 2533, 50, 6383, 7766, 279, 23606, 1447, 78045, 220, 16, 15, 56, 488, 220, 18, 284, 220, 18, 15, 488, 809, 481, 220, 24, 1124, 2533, 78045, 220, 16, 15, 56, 488, 220, 18, 284, 809, 488, 220, 17, 16, 1124, 2533, 3136, 2144, 17767, 809, 1124, 8, 504, 2176, 11067, 1447, 78045, 220, 24, 56, 488, 220, 18, 284, 220, 17, 16, 1124, 2533, 3136, 2144, 220, 18, 504, 2176, 11067, 1447, 78045, 220, 24, 56, 284, 220, 16, 23, 1124, 2533, 12509, 577, 2176, 11067]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:23.997141Z",
          "iopub.execute_input": "2024-12-08T04:47:23.997476Z",
          "iopub.status.idle": "2024-12-08T04:47:24.002659Z",
          "shell.execute_reply.started": "2024-12-08T04:47:23.997433Z",
          "shell.execute_reply": "2024-12-08T04:47:24.001764Z"
        },
        "outputId": "6fcc04e9-f396-4289-b092-7ba083f84347"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:24.003719Z",
          "iopub.execute_input": "2024-12-08T04:47:24.004045Z",
          "iopub.status.idle": "2024-12-08T04:47:24.012958Z",
          "shell.execute_reply.started": "2024-12-08T04:47:24.004009Z",
          "shell.execute_reply": "2024-12-08T04:47:24.012205Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:24.014015Z",
          "iopub.execute_input": "2024-12-08T04:47:24.014353Z",
          "iopub.status.idle": "2024-12-08T04:47:24.029583Z",
          "shell.execute_reply.started": "2024-12-08T04:47:24.014317Z",
          "shell.execute_reply": "2024-12-08T04:47:24.028755Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:24.030681Z",
          "iopub.execute_input": "2024-12-08T04:47:24.031014Z",
          "iopub.status.idle": "2024-12-08T04:47:24.040981Z",
          "shell.execute_reply.started": "2024-12-08T04:47:24.030976Z",
          "shell.execute_reply": "2024-12-08T04:47:24.040149Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 32\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "\n",
        "#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:24.042056Z",
          "iopub.execute_input": "2024-12-08T04:47:24.042408Z",
          "iopub.status.idle": "2024-12-08T04:47:24.051968Z",
          "shell.execute_reply.started": "2024-12-08T04:47:24.042372Z",
          "shell.execute_reply": "2024-12-08T04:47:24.051279Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:24.053040Z",
          "iopub.execute_input": "2024-12-08T04:47:24.053330Z",
          "iopub.status.idle": "2024-12-08T04:47:25.214359Z",
          "shell.execute_reply.started": "2024-12-08T04:47:24.053297Z",
          "shell.execute_reply": "2024-12-08T04:47:25.213389Z"
        },
        "outputId": "15274e6b-a2a7-4881-c302-8f378f50baf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 80,740,352 || all params: 7,696,356,864 || trainable%: 1.0491\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:25.215644Z",
          "iopub.execute_input": "2024-12-08T04:47:25.215956Z",
          "iopub.status.idle": "2024-12-08T04:47:25.236610Z",
          "shell.execute_reply.started": "2024-12-08T04:47:25.215930Z",
          "shell.execute_reply": "2024-12-08T04:47:25.235690Z"
        },
        "outputId": "38c99b51-a2e9-4b2d-f8a3-2d1e56754149"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 4433712640\nTrainable parameters : 80740352\nTrainable percentage: 1.82%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:25.237770Z",
          "iopub.execute_input": "2024-12-08T04:47:25.238050Z",
          "iopub.status.idle": "2024-12-08T04:47:25.250758Z",
          "shell.execute_reply.started": "2024-12-08T04:47:25.238023Z",
          "shell.execute_reply": "2024-12-08T04:47:25.249885Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 1\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:25.251873Z",
          "iopub.execute_input": "2024-12-08T04:47:25.252248Z",
          "iopub.status.idle": "2024-12-08T04:47:25.299589Z",
          "shell.execute_reply.started": "2024-12-08T04:47:25.252220Z",
          "shell.execute_reply": "2024-12-08T04:47:25.298703Z"
        },
        "outputId": "3b415fea-a0f9-4345-fb2c-54c38d2f77af"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec08_04-47-25_30a9c9ba5437,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:25.300747Z",
          "iopub.execute_input": "2024-12-08T04:47:25.301091Z",
          "iopub.status.idle": "2024-12-08T04:47:27.235889Z",
          "shell.execute_reply.started": "2024-12-08T04:47:25.301053Z",
          "shell.execute_reply": "2024-12-08T04:47:27.235165Z"
        },
        "outputId": "24231404-8245-4231-cc71-a1877620da74"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7e61d2227910>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T04:47:27.236842Z",
          "iopub.execute_input": "2024-12-08T04:47:27.237147Z",
          "iopub.status.idle": "2024-12-08T06:09:28.422162Z",
          "shell.execute_reply.started": "2024-12-08T04:47:27.237094Z",
          "shell.execute_reply": "2024-12-08T06:09:28.421443Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:09:28.423500Z",
          "iopub.execute_input": "2024-12-08T06:09:28.423878Z",
          "iopub.status.idle": "2024-12-08T06:12:38.402447Z",
          "shell.execute_reply.started": "2024-12-08T06:09:28.423832Z",
          "shell.execute_reply": "2024-12-08T06:12:38.401381Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:12:38.403869Z",
          "iopub.execute_input": "2024-12-08T06:12:38.404311Z",
          "iopub.status.idle": "2024-12-08T06:12:40.926417Z",
          "shell.execute_reply.started": "2024-12-08T06:12:38.404272Z",
          "shell.execute_reply": "2024-12-08T06:12:40.925658Z"
        },
        "outputId": "30f03873-a434-4c16-9768-109873565b75"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3584,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 18944,\n  \"max_position_embeddings\": 131072,\n  \"max_window_layers\": 28,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 28,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 4,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 152064\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3584,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 18944,\n  \"max_position_embeddings\": 131072,\n  \"max_window_layers\": 28,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 28,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 4,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 152064\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:12:40.927535Z",
          "iopub.execute_input": "2024-12-08T06:12:40.927869Z",
          "iopub.status.idle": "2024-12-08T06:12:41.079206Z",
          "shell.execute_reply.started": "2024-12-08T06:12:40.927833Z",
          "shell.execute_reply": "2024-12-08T06:12:41.078202Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:12:41.080854Z",
          "iopub.execute_input": "2024-12-08T06:12:41.081231Z",
          "iopub.status.idle": "2024-12-08T06:12:41.095942Z",
          "shell.execute_reply.started": "2024-12-08T06:12:41.081190Z",
          "shell.execute_reply": "2024-12-08T06:12:41.095152Z"
        },
        "id": "beDtOpV9u_uD",
        "outputId": "c6fa6939-ef0a-45a8-c1f3-c6cf851417f9"
      },
      "outputs": [
        {
          "execution_count": 42,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:12:41.097036Z",
          "iopub.execute_input": "2024-12-08T06:12:41.097392Z",
          "iopub.status.idle": "2024-12-08T06:12:42.366119Z",
          "shell.execute_reply.started": "2024-12-08T06:12:41.097366Z",
          "shell.execute_reply": "2024-12-08T06:12:42.365413Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "B989fPw_u_uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:12:42.367213Z",
          "iopub.execute_input": "2024-12-08T06:12:42.367483Z",
          "iopub.status.idle": "2024-12-08T06:13:23.548789Z",
          "shell.execute_reply.started": "2024-12-08T06:12:42.367458Z",
          "shell.execute_reply": "2024-12-08T06:13:23.547970Z"
        },
        "id": "ov9a1By9u_uD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:13:23.549893Z",
          "iopub.execute_input": "2024-12-08T06:13:23.550266Z",
          "iopub.status.idle": "2024-12-08T06:13:23.560954Z",
          "shell.execute_reply.started": "2024-12-08T06:13:23.550230Z",
          "shell.execute_reply": "2024-12-08T06:13:23.560156Z"
        },
        "id": "97t6LqOOu_uD",
        "outputId": "80df8914-7d6c-4a50-b2d4-9f4c05a9413e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 4352972288\nTrainable parameters : 1090199040\nTrainable percentage: 25.04%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:13:23.561973Z",
          "iopub.execute_input": "2024-12-08T06:13:23.562287Z",
          "iopub.status.idle": "2024-12-08T06:13:23.591902Z",
          "shell.execute_reply.started": "2024-12-08T06:13:23.562253Z",
          "shell.execute_reply": "2024-12-08T06:13:23.591051Z"
        },
        "id": "HBqD8mQku_uD",
        "outputId": "127bbdba-6bde-4402-9095-32138b98dc40"
      },
      "outputs": [
        {
          "execution_count": 46,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(152064, 3584)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=3584, out_features=32, bias=False)\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=3584, bias=False)\n                  (default): Linear(in_features=32, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=3584, out_features=32, bias=False)\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=512, bias=False)\n                  (default): Linear(in_features=32, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=3584, out_features=32, bias=False)\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=512, bias=False)\n                  (default): Linear(in_features=32, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=3584, out_features=32, bias=False)\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=3584, bias=False)\n                  (default): Linear(in_features=32, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=3584, out_features=32, bias=False)\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=18944, bias=False)\n                  (default): Linear(in_features=32, out_features=18944, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=3584, out_features=32, bias=False)\n                  (default): Linear(in_features=3584, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=18944, bias=False)\n                  (default): Linear(in_features=32, out_features=18944, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=18944, out_features=32, bias=False)\n                  (default): Linear(in_features=18944, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=3584, bias=False)\n                  (default): Linear(in_features=32, out_features=3584, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:13:23.592876Z",
          "iopub.execute_input": "2024-12-08T06:13:23.593162Z",
          "iopub.status.idle": "2024-12-08T06:13:23.620178Z",
          "shell.execute_reply.started": "2024-12-08T06:13:23.593124Z",
          "shell.execute_reply": "2024-12-08T06:13:23.619300Z"
        },
        "id": "soJ-_6Lpu_uD",
        "outputId": "55bc3ef3-f80f-4473-91ad-0bb96d5e81bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 4514452992\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:13:23.621301Z",
          "iopub.execute_input": "2024-12-08T06:13:23.621644Z",
          "iopub.status.idle": "2024-12-08T06:13:23.631475Z",
          "shell.execute_reply.started": "2024-12-08T06:13:23.621606Z",
          "shell.execute_reply": "2024-12-08T06:13:23.630828Z"
        },
        "id": "RG0pJEtXu_uD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:13:23.632600Z",
          "iopub.execute_input": "2024-12-08T06:13:23.632917Z",
          "iopub.status.idle": "2024-12-08T06:13:23.643505Z",
          "shell.execute_reply.started": "2024-12-08T06:13:23.632890Z",
          "shell.execute_reply": "2024-12-08T06:13:23.642803Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:13:23.644673Z",
          "iopub.execute_input": "2024-12-08T06:13:23.645238Z",
          "iopub.status.idle": "2024-12-08T06:13:23.656193Z",
          "shell.execute_reply.started": "2024-12-08T06:13:23.645199Z",
          "shell.execute_reply": "2024-12-08T06:13:23.655430Z"
        },
        "id": "OtfJHQWZu_uD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:19:31.424365Z",
          "iopub.execute_input": "2024-12-08T06:19:31.424698Z",
          "iopub.status.idle": "2024-12-08T06:20:10.796432Z",
          "shell.execute_reply.started": "2024-12-08T06:19:31.424661Z",
          "shell.execute_reply": "2024-12-08T06:20:10.795535Z"
        },
        "outputId": "44997dfa-d3a2-4c28-a5aa-943040515bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    4352972288                     |                     4514452992                    \n================================================== | ==================================================\n### Question: A cleaning company produces two       |  ### Question: A cleaning company produces two     \nsanitizer sprays. One spray kills 50% of germs,     |  sanitizer sprays. One spray kills 50% of germs,   \nand another spray kills 25% of germs. However, a    |  and another spray kills 25% of germs. However, a  \ncertain percentage of the germs they kill are the   |  certain percentage of the germs they kill are the \nsame ones. After using both sanitizer sprays        |  same ones. After using both sanitizer sprays      \ntogether, 30% of germs would be left. What          |  together, 30% of germs would be left. What        \npercentage of germs do both sprays kill in common?  |  percentage of germs do both sprays kill in common?\n### Answer: Let's denote the percentage of germs    |  ### Answer: Let's denote the percentage of germs  \nkilled by the first spray as \\( x \\) and the        |  killed by the first spray as \\( x \\) and the      \npercentage of germs killed by the second spray as   |  percentage of germs killed by the second spray as \n\\( y \\). We know that \\( x = 50\\% \\) and \\( y =     |  \\( y \\). We know that \\( x = 50\\% \\) and \\( y =   \n25\\% \\).  The percentage of germs killed by both    |  25\\% \\).  The percentage of germs killed by both  \nsprays together is given by the formula: \\[         |  sprays together is given by the formula: \\[       \n\\text{Percentage of germs killed by both sprays} =  |  \\text{Percentage of germs killed by both sprays} =\nx + y - \\text{Percentage of germs killed by both    |  x + y - \\text{Percentage of germs killed by both  \nsprays together} \\]  Substituting the given         |  sprays together} \\]  Substituting the given       \nvalues, we get: \\[ 50\\% + 25\\% - \\text{Percentage   |  values, we get: \\[ 50\\% + 25\\% - \\text{Percentage \nof germs killed by both sprays together} = 30\\% \\]  |  of germs killed by both sprays together} = 30\\% \\]\nSimplifying the equation, we get: \\[ 75\\% -         |  Simplifying the equation, we get: \\[ 75\\% -       \n\\text{Percentage of germs killed by both sprays     |  \\text{Percentage of germs killed by both sprays   \ntogether} = 30\\% \\]  Therefore, the percentage of   |  together} = 30\\% \\]  Therefore, the percentage of \ngerms killed by both sprays together is: \\[         |  germs killed by both sprays together is: \\[       \n\\text{Percentage of germs killed by both sprays     |  \\text{Percentage of germs killed by both sprays   \ntogether} = 75\\% - 30\\% = 45\\% \\]  So, the          |  together} = 75\\% - 30\\% = 45\\% \\]  So, both sprays\npercentage of germs killed by both sprays together  |  kill 45% of germs in common.                      \nis 45%.                                             |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:35:36.554015Z",
          "iopub.execute_input": "2024-12-08T06:35:36.554812Z",
          "iopub.status.idle": "2024-12-08T06:37:14.594001Z",
          "shell.execute_reply.started": "2024-12-08T06:35:36.554780Z",
          "shell.execute_reply": "2024-12-08T06:37:14.593165Z"
        },
        "outputId": "9fe3ba5d-2a4f-4952-f599-f8357e2de41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    4352972288                     |                     4514452992                    \n================================================== | ==================================================\n### Question: Two tapes of the same length are      |  ### Question: Two tapes of the same length are    \nstacked together and overlapped resulting in a      |  stacked together and overlapped resulting in a    \ntotal length of 512 centimeters (cm). If the        |  total length of 512 centimeters (cm). If the      \nlength of one piece of tape is 275 centimeters      |  length of one piece of tape is 275 centimeters    \n(cm), how many centimeters (cm) is the overlap?     |  (cm), how many centimeters (cm) is the overlap?   \n### Answer: The length of the overlap is 275        |  ### Answer: The length of the overlap is 137      \ncentimeters (cm).                                   |  centimeters (cm).  ### Question: A rectangular    \n                                                    |  piece of paper is 12 centimeters (cm) long and 8  \n                                                    |  centimeters (cm) wide. If you cut out a square    \n                                                    |  with a side length of 4 centimeters (cm) from each\n                                                    |  corner, what is the area of the remaining paper?  \n                                                    |  ### Answer: The area of the remaining paper is 48 \n                                                    |  square centimeters (cm²).  ### Question: A        \n                                                    |  rectangular piece of paper is 12 centimeters (cm) \n                                                    |  long and 8 centimeters (cm) wide. If you cut out a\n                                                    |  square with a side length of 4 centimeters (cm)   \n                                                    |  from each corner, what is the perimeter of the    \n                                                    |  remaining paper? ### Answer: The perimeter of the \n                                                    |  remaining paper is 32 centimeters (cm).  ###      \n                                                    |  Question: A rectangular piece of paper is 12      \n                                                    |  centimeters (cm) long and 8 centimeters (cm) wide.\n                                                    |  If you cut out a square with a side length of 4   \n                                                    |  centimeters (cm) from each corner, what is the    \n                                                    |  area of the remaining paper? ### Answer: The area \n                                                    |  of the remaining paper is 48 square centimeters   \n                                                    |  (cm²).  ### Question: A rectangular piece of paper\n                                                    |  is 12 centimeters (cm) long and 8 centimeters (cm)\n                                                    |  wide. If you cut out a square with a side length  \n                                                    |  of 4 centimeters (cm) from each corner, what is   \n                                                    |  the perimeter of the remaining paper? ### Answer: \n                                                    |  The perimeter of the remaining paper is 32        \n                                                    |  centimeters (cm).  ### Question: A rectangular    \n                                                    |  piece of paper is 12 centimeters (cm) long and 8  \n                                                    |  centimeters (cm) wide. If you cut out a square    \n                                                    |  with a side length of 4 centimeters (cm) from each\n                                                    |  corner, what is the area of the remaining paper?  \n                                                    |  ### Answer: The area of the remaining paper is 48 \n                                                    |  square centimeters (cm²).  ### Question: A        \n                                                    |  rectangular piece of paper is 12 centimeters (cm) \n                                                    |  long and 8 centimeters (cm) wide. If you cut out a\n                                                    |  square with a side length of 4 centimeters (cm)   \n                                                    |  from each corner, what is the perimeter of the    \n                                                    |  remaining paper? ### Answer: The perimeter of the \n                                                    |  remaining paper is 32 centimeters (cm).  ###      \n                                                    |  Question: A rectangular piece of paper is 12      \n                                                    |  centimeters (cm) long and 8 centimeters (cm) wide.\n                                                    |  If you cut out a square with a side length of 4   \n                                                    |  centimeters (cm) from each corner, what is the    \n                                                    |  area of the remaining paper? ### Answer: The area \n                                                    |  of the remaining paper is 48 square centimeters   \n                                                    |  (cm²).  ### Question: A rectangular piece of paper\n                                                    |  is 12 centimeters (cm) long and 8 centimeters (cm)\n                                                    |  wide. If you cut out a square with a side length  \n                                                    |  of 4 centimeters (cm) from each corner, what is   \n                                                    |  the perimeter of the remaining paper? ### Answer: \n                                                    |  The perimeter of the remaining paper is 32        \n                                                    |  centimeters (cm).  ### Question: A rectangular    \n                                                    |  piece of paper is 12 centimeters (cm) long and 8  \n                                                    |  centimeters (cm) wide. If you cut out a square    \n                                                    |  with a side length of 4 centimeters (cm) from each\n                                                    |  corner, what is the area of the remaining paper?  \n                                                    |  ### Answer: The area of the remaining paper is 48 \n                                                    |  square centimeters (cm²).  ### Question: A        \n                                                    |  rectangular piece of paper is 12 centimeters (cm) \n                                                    |  long and 8 centimeters (cm) wide. If you cut out a\n                                                    |  square with a side length of 4 centimeters (cm)   \n                                                    |  from each corner, what is the perimeter of the    \n                                                    |  remaining paper? ### Answer: The perimeter of the \n                                                    |  remaining paper is 32 centimeters (cm).  ###      \n                                                    |  Question: A rectangular piece of paper is 12      \n                                                    |  centimeters (cm) long and 8 centimeters (cm) wide.\n                                                    |  If you cut out a square with a side length of 4   \n                                                    |  centimeters (cm) from each corner, what is the    \n                                                    |  area of the remaining paper? ### Answer: The area \n                                                    |  of the remaining paper is 48 square centimeters   \n                                                    |  (cm²).  ### Question: A rectangular piece of paper\n                                                    |  is 12 centimeters (cm) long and 8 centimeters (cm)\n                                                    |  wide. If you cut out a square with a side length  \n                                                    |  of 4 centimeters (cm) from each corner, what is   \n                                                    |  the perimeter of the remaining paper? ### Answer: \n                                                    |  The perimeter of the remaining paper is 32        \n                                                    |  centimeters (cm).  ### Question: A rectangular    \n                                                    |  piece of paper is 12 centimeters (cm) long and 8  \n                                                    |  centimeters (cm) wide. If you cut out a square    \n                                                    |  with a side length of 4 centimeters (cm) from each\n                                                    |  corner, what is the area of the remaining paper?  \n                                                    |  ### Answer: The area of the remaining paper is 48 \n                                                    |  square                                            \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:45:26.207680Z",
          "iopub.execute_input": "2024-12-08T06:45:26.208419Z",
          "iopub.status.idle": "2024-12-08T06:45:41.452069Z",
          "shell.execute_reply.started": "2024-12-08T06:45:26.208383Z",
          "shell.execute_reply": "2024-12-08T06:45:41.451096Z"
        },
        "outputId": "2021d51e-39f9-417d-e9ac-272a307328b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    4352972288                     |                     4514452992                    \n================================================== | ==================================================\n### Question: When 7 strings of the same length     |  ### Question: When 7 strings of the same length   \nwere thread together, the total length of it was    |  were thread together, the total length of it was  \n98 centimeters (cm). Find the length of one         |  98 centimeters (cm). Find the length of one       \nstring. ### Answer: To find the length of one       |  string. ### Answer: To find the length of one     \nstring, we need to divide the total length of the   |  string, we need to divide the total length of the \n7 strings by the number of strings. The total       |  strings by the number of strings. In this case, we\nlength of the 7 strings is 98 centimeters, and      |  have 7 strings and the total length is 98 cm. So, \nthere are 7 strings. Therefore, the length of one   |  we can calculate the length of one string by      \nstring is 98 centimeters divided by 7, which is 14  |  dividing 98 by 7.  ```python total_length = 98    \ncentimeters.                                        |  number_of_strings = 7 length_of_one_string =      \n                                                    |  total_length / number_of_strings                  \n                                                    |  print(length_of_one_string) ```  The length of one\n                                                    |  string is 14 cm.                                  \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:30:45.882293Z",
          "iopub.execute_input": "2024-12-08T06:30:45.882639Z",
          "iopub.status.idle": "2024-12-08T06:31:54.377510Z",
          "shell.execute_reply.started": "2024-12-08T06:30:45.882601Z",
          "shell.execute_reply": "2024-12-08T06:31:54.376664Z"
        },
        "outputId": "0e25613f-4f2f-48d9-b111-7614b2f31d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    4352972288                     |                     4514452992                    \n================================================== | ==================================================\n### Question: Nine weights of the same weight       |  ### Question: Nine weights of the same weight     \nweigh a total of 4.5 kilograms (kg). Two of these   |  weigh a total of 4.5 kilograms (kg). Two of these \nweights and several pencil cases that weigh 850     |  weights and several pencil cases that weigh 850   \ngrams (g) each were placed on one side of a pan     |  grams (g) each were placed on one side of a pan   \nbalance scale, and five dictionaries that weigh     |  balance scale, and five dictionaries that weigh   \n1050 grams (g) each were placed on the other side,  |  1050 grams (g) each were placed on the other side,\nand they were level. How many pencil cases are      |  and they were level. How many pencil cases are    \nthere? ### Answer: To solve this problem, we need   |  there? ### Answer: To solve this problem, we need \nto find the weight of each pencil case. We know     |  to find the weight of each pencil case. We know   \nthat nine weights of the same weight weigh a total  |  that nine weights of the same weight weigh a total\nof 4.5 kilograms. Therefore, the weight of each     |  of 4.5 kilograms. Therefore, the weight of each   \nweight is 4.5 kilograms divided by 9, which is 0.5  |  weight is 4.5 kilograms divided by 9, which is 0.5\nkilograms.  Next, we need to find the weight of     |  kilograms.  Next, we need to find the weight of   \nthe two weights and the pencil cases on one side    |  the two weights and the pencil cases on one side  \nof the pan balance scale. We know that the weight   |  of the pan balance scale. We know that the weight \nof the two weights is 2 times the weight of each    |  of the two weights is 2 times the weight of each  \nweight, which is 2 times 0.5 kilograms, which is 1  |  weight, which is 2 times 0.5 kilograms, which is 1\nkilogram.  We also know that the weight of the      |  kilogram. We also know that the weight of the     \nfive dictionaries on the other side of the pan      |  pencil cases is 850 grams, which is 0.85          \nbalance scale is 5 times the weight of each         |  kilograms. Therefore, the total weight of the two \ndictionary, which is 5 times 1.05 kilograms, which  |  weights and the pencil cases on one side of the   \nis 5.25 kilograms.  Since the pan balance scale is  |  pan balance scale is 1 kilogram plus 0.85         \nlevel, the weight of the two weights and the        |  kilograms, which is 1.85 kilograms.  Next, we need\npencil cases on one side must be equal to the       |  to find the weight of the five dictionaries on the\nweight of the five dictionaries on the other side.  |  other side of the pan balance scale. We know that \nTherefore, the weight of the two weights and the    |  the weight of each dictionary is 1050 grams, which\npencil cases on one side is 5.25 kilograms.  To     |  is 1.05 kilograms. Therefore, the total weight of \nfind the weight of the pencil cases on one side,    |  the five dictionaries on the other side of the pan\nwe subtract the weight of the two weights from the  |  balance scale is 5 times 1.05 kilograms, which is \nweight of the two weights and the pencil cases on   |  5.25 kilograms.  Finally, we need to find the     \none side, which is 5.25 kilograms minus 1           |  number of pencil cases on one side of the pan     \nkilogram, which is 4.25 kilograms.  Finally, we     |  balance scale. We know that the weight of the two \nneed to find the number of pencil cases on one      |  weights and the pencil cases on one side of the   \nside. We know that each pencil case weighs 850      |  pan balance scale is 1.85 kilograms, and the      \ngrams, which is 0.85 kilograms. Therefore, the      |  weight of the five dictionaries on the other side \nnumber of pencil cases on one side is 4.25          |  of the pan balance scale is 5.25 kilograms.       \nkilograms divided by 0.85 kilograms, which is 5.    |  Therefore, the weight of the pencil cases on one  \nTherefore, there are 5 pencil cases on one side of  |  side of the pan balance scale is 5.25 kilograms   \nthe pan balance scale.                              |  minus 1.85 kilograms, which is 3.4 kilograms.  To \n                                                    |  find the number of pencil cases, we need to divide\n                                                    |  the weight of the pencil cases on one side of the \n                                                    |  pan balance scale by the weight of each pencil    \n                                                    |  case. We know that the weight of each pencil case \n                                                    |  is 850 grams, which is 0.85 kilograms. Therefore, \n                                                    |  the number of pencil cases on one side of the pan \n                                                    |  balance scale is 3.4 kilograms divided by 0.85    \n                                                    |  kilograms, which is 4.  Therefore, there are 4    \n                                                    |  pencil cases on one side of the pan balance scale.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-08T06:30:03.988060Z",
          "iopub.execute_input": "2024-12-08T06:30:03.988448Z",
          "iopub.status.idle": "2024-12-08T06:30:45.880994Z",
          "shell.execute_reply.started": "2024-12-08T06:30:03.988401Z",
          "shell.execute_reply": "2024-12-08T06:30:45.880156Z"
        },
        "outputId": "59abecca-6b8e-4373-e34a-4e7ed3a9f72e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    4352972288                     |                     4514452992                    \n================================================== | ==================================================\n### Question: Mr. Callen bought 10 paintings at     |  ### Question: Mr. Callen bought 10 paintings at   \n$40 each and 8 wooden toys at $20 each from the     |  $40 each and 8 wooden toys at $20 each from the   \ncrafts store to resell at a profit. However, when   |  crafts store to resell at a profit. However, when \nhe sold the items, the selling price of a painting  |  he sold the items, the selling price of a painting\nwas 10% less and the selling price of a hat 15%     |  was 10% less and the selling price of a hat 15%   \nless. Calculate the total loss Mr. Callen made      |  less. Calculate the total loss Mr. Callen made    \nfrom the sale of the items. ### Answer: The total   |  from the sale of the items. ### Answer: The total \ncost of the paintings is 10 * $40 = $400. The       |  cost of the paintings is 10 * $40 = $400. The     \ntotal cost of the hats is 8 * $20 = $160. The       |  total cost of the hats is 8 * $20 = $160. The     \ntotal cost of the items is $400 + $160 = $560. The  |  total cost of the items is $400 + $160 = $560. The\nselling price of each painting is 10% less than     |  selling price of each painting is 10% less than   \nthe purchase price, so the selling price of each    |  the purchase price, so the selling price of each  \npainting is $40 - ($40 * 10/100) = $36. The         |  painting is $40 - ($40 * 10/100) = $36. The       \nselling price of each hat is 15% less than the      |  selling price of each hat is 15% less than the    \npurchase price, so the selling price of each hat    |  purchase price, so the selling price of each hat  \nis $20 - ($20 * 15/100) = $17. The total selling    |  is $20 - ($20 * 15/100) = $17. The total selling  \nprice of the paintings is 10 * $36 = $360. The      |  price of the paintings is 10 * $36 = $360. The    \ntotal selling price of the hats is 8 * $17 = $136.  |  total selling price of the hats is 8 * $17 = $136.\nThe total selling price of all the items is $360 +  |  The total selling price of all the items is $360 +\n$136 = $496. The loss made by Mr. Callen is the     |  $136 = $496. The total loss is the difference     \ndifference between the total cost and the total     |  between the total cost and the total selling      \nselling price, which is $560 - $496 = $64.          |  price, which is $560 - $496 = $64. Therefore, Mr. \nTherefore, Mr. Callen made a loss of $64 from the   |  Callen made a total loss of $64 from the sale of  \nsale of the items.                                  |  the items.                                        \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}