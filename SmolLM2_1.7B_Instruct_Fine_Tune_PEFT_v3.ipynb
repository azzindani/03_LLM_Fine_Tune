{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-08T11:05:08.680904Z","iopub.execute_input":"2024-12-08T11:05:08.681242Z","iopub.status.idle":"2024-12-08T11:06:12.011149Z","shell.execute_reply.started":"2024-12-08T11:05:08.681208Z","shell.execute_reply":"2024-12-08T11:06:12.010142Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-08T11:06:12.013408Z","iopub.execute_input":"2024-12-08T11:06:12.013787Z","iopub.status.idle":"2024-12-08T11:06:30.837758Z","shell.execute_reply.started":"2024-12-08T11:06:12.013750Z","shell.execute_reply":"2024-12-08T11:06:30.836939Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-08T11:06:30.839151Z","iopub.execute_input":"2024-12-08T11:06:30.839495Z","iopub.status.idle":"2024-12-08T11:06:30.846139Z","shell.execute_reply.started":"2024-12-08T11:06:30.839456Z","shell.execute_reply":"2024-12-08T11:06:30.845329Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'HuggingFaceTB/SmolLM2-1.7B-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-08T11:06:30.848821Z","iopub.execute_input":"2024-12-08T11:06:30.849196Z","iopub.status.idle":"2024-12-08T11:06:30.876298Z","shell.execute_reply.started":"2024-12-08T11:06:30.849167Z","shell.execute_reply":"2024-12-08T11:06:30.875592Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-08T11:06:30.877340Z","iopub.execute_input":"2024-12-08T11:06:30.877703Z","iopub.status.idle":"2024-12-08T11:06:30.890091Z","shell.execute_reply.started":"2024-12-08T11:06:30.877667Z","shell.execute_reply":"2024-12-08T11:06:30.889358Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:06:30.890983Z","iopub.execute_input":"2024-12-08T11:06:30.891194Z","iopub.status.idle":"2024-12-08T11:07:57.967680Z","shell.execute_reply.started":"2024-12-08T11:06:30.891172Z","shell.execute_reply":"2024-12-08T11:07:57.966852Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0192e4228d0d4cab92dd25dfbf814175"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b23001e5c524343b923b5644447270d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c619e46d0be4e6ebc58fc8fb115b477"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:07:57.968700Z","iopub.execute_input":"2024-12-08T11:07:57.968993Z","iopub.status.idle":"2024-12-08T11:07:57.976341Z","shell.execute_reply.started":"2024-12-08T11:07:57.968965Z","shell.execute_reply":"2024-12-08T11:07:57.975522Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 906070016\nTrainable parameters : 100763648\nTrainable percentage: 11.12%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:07:57.977416Z","iopub.execute_input":"2024-12-08T11:07:57.977742Z","iopub.status.idle":"2024-12-08T11:08:03.303019Z","shell.execute_reply.started":"2024-12-08T11:07:57.977705Z","shell.execute_reply":"2024-12-08T11:08:03.302028Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fcd2f2a10c499186eedd45ca1ae7d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f42bc0608df4109afac907594be5055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e0e1e3e6684ae093350445efd2709d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aea324bc61ff4ea782c1a31bb7150047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"989ba00b091442ed8b87e992d319c627"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:03.304458Z","iopub.execute_input":"2024-12-08T11:08:03.305205Z","iopub.status.idle":"2024-12-08T11:08:03.309391Z","shell.execute_reply.started":"2024-12-08T11:08:03.305165Z","shell.execute_reply":"2024-12-08T11:08:03.308370Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:03.313245Z","iopub.execute_input":"2024-12-08T11:08:03.313555Z","iopub.status.idle":"2024-12-08T11:08:03.321642Z","shell.execute_reply.started":"2024-12-08T11:08:03.313520Z","shell.execute_reply":"2024-12-08T11:08:03.320942Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:03.322621Z","iopub.execute_input":"2024-12-08T11:08:03.322877Z","iopub.status.idle":"2024-12-08T11:08:08.579086Z","shell.execute_reply.started":"2024-12-08T11:08:03.322837Z","shell.execute_reply":"2024-12-08T11:08:08.578246Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27fe484ab22b49f6baa79d4782b72d83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/84.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ba798d7aadf496899e1f87b28b9c7fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/200035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0e390f138848f8ba4441b1b44eec18"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:08.580207Z","iopub.execute_input":"2024-12-08T11:08:08.580472Z","iopub.status.idle":"2024-12-08T11:08:08.586582Z","shell.execute_reply.started":"2024-12-08T11:08:08.580446Z","shell.execute_reply":"2024-12-08T11:08:08.585909Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:08.587532Z","iopub.execute_input":"2024-12-08T11:08:08.587823Z","iopub.status.idle":"2024-12-08T11:08:08.617516Z","shell.execute_reply.started":"2024-12-08T11:08:08.587783Z","shell.execute_reply":"2024-12-08T11:08:08.616792Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:08.618433Z","iopub.execute_input":"2024-12-08T11:08:08.618692Z","iopub.status.idle":"2024-12-08T11:08:08.626264Z","shell.execute_reply.started":"2024-12-08T11:08:08.618666Z","shell.execute_reply":"2024-12-08T11:08:08.625400Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:08.627421Z","iopub.execute_input":"2024-12-08T11:08:08.627674Z","iopub.status.idle":"2024-12-08T11:08:08.635963Z","shell.execute_reply.started":"2024-12-08T11:08:08.627650Z","shell.execute_reply":"2024-12-08T11:08:08.635101Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:08.636941Z","iopub.execute_input":"2024-12-08T11:08:08.637184Z","iopub.status.idle":"2024-12-08T11:08:08.645224Z","shell.execute_reply.started":"2024-12-08T11:08:08.637160Z","shell.execute_reply":"2024-12-08T11:08:08.644352Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:08.646251Z","iopub.execute_input":"2024-12-08T11:08:08.646539Z","iopub.status.idle":"2024-12-08T11:08:08.655038Z","shell.execute_reply.started":"2024-12-08T11:08:08.646514Z","shell.execute_reply":"2024-12-08T11:08:08.654274Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:08.656004Z","iopub.execute_input":"2024-12-08T11:08:08.656254Z","iopub.status.idle":"2024-12-08T11:08:09.119229Z","shell.execute_reply.started":"2024-12-08T11:08:08.656229Z","shell.execute_reply":"2024-12-08T11:08:09.118100Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf546d739094717bc94c71411dd5b5b"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:09.120580Z","iopub.execute_input":"2024-12-08T11:08:09.121224Z","iopub.status.idle":"2024-12-08T11:08:09.125729Z","shell.execute_reply.started":"2024-12-08T11:08:09.121179Z","shell.execute_reply":"2024-12-08T11:08:09.124900Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:09.126920Z","iopub.execute_input":"2024-12-08T11:08:09.127423Z","iopub.status.idle":"2024-12-08T11:08:09.146914Z","shell.execute_reply.started":"2024-12-08T11:08:09.127392Z","shell.execute_reply":"2024-12-08T11:08:09.146184Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:09.147951Z","iopub.execute_input":"2024-12-08T11:08:09.148190Z","iopub.status.idle":"2024-12-08T11:08:18.173904Z","shell.execute_reply.started":"2024-12-08T11:08:09.148167Z","shell.execute_reply":"2024-12-08T11:08:18.173070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd9ce5cfc55745c0b049e18aadc3a513"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.175020Z","iopub.execute_input":"2024-12-08T11:08:18.175309Z","iopub.status.idle":"2024-12-08T11:08:18.181967Z","shell.execute_reply.started":"2024-12-08T11:08:18.175270Z","shell.execute_reply":"2024-12-08T11:08:18.181053Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.184830Z","iopub.execute_input":"2024-12-08T11:08:18.185115Z","iopub.status.idle":"2024-12-08T11:08:18.238014Z","shell.execute_reply.started":"2024-12-08T11:08:18.185084Z","shell.execute_reply":"2024-12-08T11:08:18.237085Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.239090Z","iopub.execute_input":"2024-12-08T11:08:18.239361Z","iopub.status.idle":"2024-12-08T11:08:18.249589Z","shell.execute_reply.started":"2024-12-08T11:08:18.239334Z","shell.execute_reply":"2024-12-08T11:08:18.248845Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.250606Z","iopub.execute_input":"2024-12-08T11:08:18.250848Z","iopub.status.idle":"2024-12-08T11:08:18.277074Z","shell.execute_reply.started":"2024-12-08T11:08:18.250824Z","shell.execute_reply":"2024-12-08T11:08:18.276186Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [3757, 15232, 42, 198, 2122, 314, 253, 827, 29...   \n1  [3757, 15232, 42, 198, 788, 253, 2066, 3985, 2...   \n2  [3757, 15232, 42, 198, 31019, 3935, 288, 253, ...   \n3  [3757, 15232, 42, 198, 60, 14765, 314, 3012, 2...   \n4  [3757, 15232, 42, 198, 2122, 314, 253, 18896, ...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[3757, 15232, 42, 198, 2122, 314, 253, 827, 29...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[3757, 15232, 42, 198, 788, 253, 2066, 3985, 2...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[3757, 15232, 42, 198, 31019, 3935, 288, 253, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[3757, 15232, 42, 198, 60, 14765, 314, 3012, 2...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[3757, 15232, 42, 198, 2122, 314, 253, 18896, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.278155Z","iopub.execute_input":"2024-12-08T11:08:18.278556Z","iopub.status.idle":"2024-12-08T11:08:18.283946Z","shell.execute_reply.started":"2024-12-08T11:08:18.278526Z","shell.execute_reply":"2024-12-08T11:08:18.283104Z"}},"outputs":[{"name":"stdout","text":"### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|im_end|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.285210Z","iopub.execute_input":"2024-12-08T11:08:18.285890Z","iopub.status.idle":"2024-12-08T11:08:18.294075Z","shell.execute_reply.started":"2024-12-08T11:08:18.285828Z","shell.execute_reply":"2024-12-08T11:08:18.293194Z"}},"outputs":[{"name":"stdout","text":"[3757, 15232, 42, 198, 2122, 314, 253, 827, 29, 23141, 1782, 1230, 3449, 12281, 1379, 314, 216, 35, 30, 2959, 330, 284, 389, 325, 260, 14498, 1010, 282, 451, 1230, 411, 216, 33, 32, 284, 260, 17867, 282, 7573, 411, 216, 33, 32, 28, 7827, 30, 1094, 389, 25319, 411, 216, 33, 32, 8055, 330, 314, 216, 41, 1181, 670, 330, 25319, 411, 216, 33, 32, 8055, 389, 28, 732, 314, 260, 808, 1230, 47, 198, 3757, 19842, 42, 198, 4239, 506, 25832, 260, 827, 29, 23141, 1230, 347, 3814, 24, 33739, 3814, 643, 837, 3814, 24, 2273, 3814, 25, 314, 260, 11403, 281, 260, 12281, 1379, 284, 3814, 24, 718, 3814, 25, 314, 260, 11403, 281, 260, 2911, 1379, 30, 4311, 260, 12281, 1379, 314, 216, 35, 28, 392, 457, 3814, 24, 2273, 446, 216, 35, 3814, 595, 198, 198, 5449, 288, 260, 1732, 28, 3814, 24, 330, 3814, 25, 314, 260, 14498, 1010, 282, 260, 1230, 411, 216, 33, 32, 28, 284, 3814, 24, 389, 3814, 25, 314, 260, 17867, 282, 260, 7573, 411, 216, 33, 32, 30, 4882, 28, 3814, 24, 330, 446, 2273, 446, 216, 35, 3814, 25, 284, 3814, 24, 389, 446, 718, 3814, 595, 198, 198, 504, 1732, 2496, 338, 3814, 24, 389, 3814, 14602, 216, 33, 32, 1232, 330, 3814, 25, 314, 216, 41, 1181, 670, 3814, 24, 330, 3814, 14602, 216, 33, 32, 1232, 389, 3814, 595, 669, 416, 325, 2855, 347, 354, 8345, 42, 198, 198, 76, 75, 389, 3814, 14602, 216, 33, 32, 1232, 330, 446, 330, 3814, 14602, 216, 33, 32, 1232, 389, 731, 216, 41, 3814, 77, 198, 198, 40810, 30053, 3814, 24, 330, 3814, 25, 284, 3814, 24, 389, 3814, 25, 351, 3814, 24, 216, 35, 3814, 25, 284, 3814, 24, 718, 3814, 643, 7827, 28, 392, 820, 42, 198, 198, 76, 75, 718, 3814, 14602, 216, 33, 32, 1232, 216, 35, 446, 216, 35, 3814, 14602, 216, 33, 32, 1232, 718, 731, 216, 41, 3814, 77, 198, 198, 8152, 439, 4318, 260, 8345, 42, 198, 198, 76, 75, 216, 33, 32, 73, 1232, 216, 35, 446, 216, 35, 32, 1232, 718, 731, 216, 41, 3814, 77, 198, 198, 76, 75, 216, 33, 32, 73, 1232, 216, 35, 446, 718, 1232, 216, 34, 33, 3814, 77, 198, 198, 7871, 38519, 3814]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.301152Z","iopub.execute_input":"2024-12-08T11:08:18.302182Z","iopub.status.idle":"2024-12-08T11:08:18.306890Z","shell.execute_reply.started":"2024-12-08T11:08:18.302151Z","shell.execute_reply":"2024-12-08T11:08:18.305942Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.308002Z","iopub.execute_input":"2024-12-08T11:08:18.308258Z","iopub.status.idle":"2024-12-08T11:08:18.316705Z","shell.execute_reply.started":"2024-12-08T11:08:18.308232Z","shell.execute_reply":"2024-12-08T11:08:18.315906Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.317579Z","iopub.execute_input":"2024-12-08T11:08:18.317823Z","iopub.status.idle":"2024-12-08T11:08:18.329876Z","shell.execute_reply.started":"2024-12-08T11:08:18.317798Z","shell.execute_reply":"2024-12-08T11:08:18.328850Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.330946Z","iopub.execute_input":"2024-12-08T11:08:18.331278Z","iopub.status.idle":"2024-12-08T11:08:18.339470Z","shell.execute_reply.started":"2024-12-08T11:08:18.331240Z","shell.execute_reply":"2024-12-08T11:08:18.338758Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.340564Z","iopub.execute_input":"2024-12-08T11:08:18.341079Z","iopub.status.idle":"2024-12-08T11:08:18.350099Z","shell.execute_reply.started":"2024-12-08T11:08:18.341042Z","shell.execute_reply":"2024-12-08T11:08:18.349456Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:18.350992Z","iopub.execute_input":"2024-12-08T11:08:18.351223Z","iopub.status.idle":"2024-12-08T11:08:19.322232Z","shell.execute_reply.started":"2024-12-08T11:08:18.351200Z","shell.execute_reply":"2024-12-08T11:08:19.321289Z"}},"outputs":[{"name":"stdout","text":"trainable params: 72,351,744 || all params: 1,783,728,128 || trainable%: 4.0562\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:19.323506Z","iopub.execute_input":"2024-12-08T11:08:19.323884Z","iopub.status.idle":"2024-12-08T11:08:19.337184Z","shell.execute_reply.started":"2024-12-08T11:08:19.323826Z","shell.execute_reply":"2024-12-08T11:08:19.336335Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 978421760\nTrainable parameters : 72351744\nTrainable percentage: 7.39%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:19.338380Z","iopub.execute_input":"2024-12-08T11:08:19.338640Z","iopub.status.idle":"2024-12-08T11:08:19.348855Z","shell.execute_reply.started":"2024-12-08T11:08:19.338614Z","shell.execute_reply":"2024-12-08T11:08:19.347932Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 2\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:19.350106Z","iopub.execute_input":"2024-12-08T11:08:19.350422Z","iopub.status.idle":"2024-12-08T11:08:19.391618Z","shell.execute_reply.started":"2024-12-08T11:08:19.350389Z","shell.execute_reply":"2024-12-08T11:08:19.390747Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec08_11-08-19_a2c7c129f410,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:19.392828Z","iopub.execute_input":"2024-12-08T11:08:19.393212Z","iopub.status.idle":"2024-12-08T11:08:21.029555Z","shell.execute_reply.started":"2024-12-08T11:08:19.393169Z","shell.execute_reply":"2024-12-08T11:08:21.028723Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7925ec0b56f0>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:08:21.030718Z","iopub.execute_input":"2024-12-08T11:08:21.031007Z","iopub.status.idle":"2024-12-08T11:49:21.562705Z","shell.execute_reply.started":"2024-12-08T11:08:21.030980Z","shell.execute_reply":"2024-12-08T11:49:21.562015Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 2\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 72,351,744\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113679844442635, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108b5454ffc74f75838dac85224e4eb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241208_112444-8kriaqch</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/8kriaqch' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/8kriaqch' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/8kriaqch</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 24:28, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.824300</td>\n      <td>0.824495</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.841700</td>\n      <td>0.791060</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.785700</td>\n      <td>0.749588</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.756000</td>\n      <td>0.712707</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.703800</td>\n      <td>0.695804</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.667700</td>\n      <td>0.688196</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.678100</td>\n      <td>0.683665</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.674800</td>\n      <td>0.681043</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.679500</td>\n      <td>0.680017</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.687700</td>\n      <td>0.679762</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.7299389839172363, metrics={'train_runtime': 2460.1057, 'train_samples_per_second': 0.65, 'train_steps_per_second': 0.081, 'total_flos': 6471167665766400.0, 'train_loss': 0.7299389839172363, 'epoch': 0.17777777777777778})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:49:21.563766Z","iopub.execute_input":"2024-12-08T11:49:21.564042Z","iopub.status.idle":"2024-12-08T11:50:13.206882Z","shell.execute_reply.started":"2024-12-08T11:49:21.564016Z","shell.execute_reply":"2024-12-08T11:50:13.206048Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:50]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.6797617077827454, 'eval_runtime': 51.6304, 'eval_samples_per_second': 3.874, 'eval_steps_per_second': 0.968, 'epoch': 0.17777777777777778}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:13.207851Z","iopub.execute_input":"2024-12-08T11:50:13.208140Z","iopub.status.idle":"2024-12-08T11:50:14.563032Z","shell.execute_reply.started":"2024-12-08T11:50:13.208114Z","shell.execute_reply":"2024-12-08T11:50:14.562299Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/1dfd32118ef11b4998e591ed60f2a0e978e15411/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 130000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/1dfd32118ef11b4998e591ed60f2a0e978e15411/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 130000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:14.564023Z","iopub.execute_input":"2024-12-08T11:50:14.564282Z","iopub.status.idle":"2024-12-08T11:50:14.761341Z","shell.execute_reply.started":"2024-12-08T11:50:14.564256Z","shell.execute_reply":"2024-12-08T11:50:14.760563Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:14.762521Z","iopub.execute_input":"2024-12-08T11:50:14.762793Z","iopub.status.idle":"2024-12-08T11:50:14.772523Z","shell.execute_reply.started":"2024-12-08T11:50:14.762764Z","shell.execute_reply":"2024-12-08T11:50:14.771896Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:14.773454Z","iopub.execute_input":"2024-12-08T11:50:14.773665Z","iopub.status.idle":"2024-12-08T11:50:15.878703Z","shell.execute_reply.started":"2024-12-08T11:50:14.773643Z","shell.execute_reply":"2024-12-08T11:50:15.878025Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:15.880253Z","iopub.execute_input":"2024-12-08T11:50:15.880606Z","iopub.status.idle":"2024-12-08T11:50:20.462268Z","shell.execute_reply.started":"2024-12-08T11:50:15.880570Z","shell.execute_reply":"2024-12-08T11:50:20.461387Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/1dfd32118ef11b4998e591ed60f2a0e978e15411/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 32,\n  \"pad_token_id\": 2,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 130000,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers.js_config\": {\n    \"kv_cache_dtype\": {\n      \"fp16\": \"float16\",\n      \"q4f16\": \"float16\"\n    }\n  },\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 49152\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/1dfd32118ef11b4998e591ed60f2a0e978e15411/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-1.7B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-1.7B-Instruct/snapshots/1dfd32118ef11b4998e591ed60f2a0e978e15411/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 2\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:20.463512Z","iopub.execute_input":"2024-12-08T11:50:20.464134Z","iopub.status.idle":"2024-12-08T11:50:20.473412Z","shell.execute_reply.started":"2024-12-08T11:50:20.464104Z","shell.execute_reply":"2024-12-08T11:50:20.472563Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 906070016\nTrainable parameters : 100763648\nTrainable percentage: 11.12%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:20.474463Z","iopub.execute_input":"2024-12-08T11:50:20.474738Z","iopub.status.idle":"2024-12-08T11:50:20.493680Z","shell.execute_reply.started":"2024-12-08T11:50:20.474708Z","shell.execute_reply":"2024-12-08T11:50:20.492784Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(49152, 2048, padding_idx=2)\n        (layers): ModuleList(\n          (0-23): 24 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=8192, out_features=64, bias=False)\n                  (default): Linear(in_features=8192, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:20.494917Z","iopub.execute_input":"2024-12-08T11:50:20.495512Z","iopub.status.idle":"2024-12-08T11:50:20.517404Z","shell.execute_reply.started":"2024-12-08T11:50:20.495485Z","shell.execute_reply":"2024-12-08T11:50:20.516619Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1050773504\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:20.518284Z","iopub.execute_input":"2024-12-08T11:50:20.518520Z","iopub.status.idle":"2024-12-08T11:50:20.528853Z","shell.execute_reply.started":"2024-12-08T11:50:20.518496Z","shell.execute_reply":"2024-12-08T11:50:20.528002Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:20.530034Z","iopub.execute_input":"2024-12-08T11:50:20.530530Z","iopub.status.idle":"2024-12-08T11:50:20.540567Z","shell.execute_reply.started":"2024-12-08T11:50:20.530503Z","shell.execute_reply":"2024-12-08T11:50:20.539766Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:50:20.541614Z","iopub.execute_input":"2024-12-08T11:50:20.541874Z","iopub.status.idle":"2024-12-08T11:50:20.549662Z","shell.execute_reply.started":"2024-12-08T11:50:20.541832Z","shell.execute_reply":"2024-12-08T11:50:20.548927Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:23:27.841487Z","iopub.execute_input":"2024-12-08T12:23:27.841834Z","iopub.status.idle":"2024-12-08T12:25:18.222822Z","shell.execute_reply.started":"2024-12-08T12:23:27.841801Z","shell.execute_reply":"2024-12-08T12:25:18.222063Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n### Question: There are 40 students in a class. If  |  ### Question: There are 40 students in a class. If\na certain fraction of students are absent, 3/4 of   |  a certain fraction of students are absent, 3/4 of \nthe students who are present are in the classroom,  |  the students who are present are in the classroom,\nand the rest are in the canteen. There are 9        |  and the rest are in the canteen. There are 9      \nstudents in the canteen. What fraction of students  |  students in the canteen. What fraction of students\nare absent? ### Answer: To find the fraction of     |  are absent? ### Answer: To find the fraction of   \nstudents absent, we first need to determine the     |  students absent, we first need to determine the   \nnumber of students present and then subtract that   |  number of students present and then subtract that \nfrom the total number of students.  The number of   |  from the total number of students.  The number of \nstudents present is 40 - 9 = 31.  Since 31          |  students present is 40 - 9 = 31.  Since 31        \nstudents are present and 3/4 of them are in the     |  students are present and 3/4 of them are in the   \nclassroom, we can set up the following equation to  |  classroom, we can set up the following proportion \nfind the fraction of students absent:  (3/4) * 40   |  to find the fraction of students absent:  (3/4) * \n= (3/4) * 31  Now, we solve for the fraction of     |  40 = (3/4) * 31  Now, we solve for the fraction of\nstudents absent:  (3/4) * 31 = 21.25  So, 21.25     |  students absent:  3/4 * 40 = 3/4 * 31 30 = 31     \nstudents are absent. To find the fraction of        |  This equation is not true, which means there is a \nstudents absent, we divide the number of absent     |  mistake in the setup. Let's correct the setup:    \nstudents by the total number of students:  (21.25   |  (3/4) * 40 = (3/4) * (31 - 9)  Now, we solve for  \n/ 40) = 0.53125  Therefore, the fraction of         |  the fraction of students absent:  30 = (3/4) *    \nstudents absent is approximately 0.53125, or        |  (22)  This equation is also not true, which means \n53.125%.  ### Question: A snail is at the bottom    |  there is a mistake in the setup. Let's correct the\nof a 20-foot well. Each day, it climbs up 3 feet,   |  setup again:  (3/4) * 40 = (3/4) * (31 - 9)  Now, \nbut at night, it slips back 2 feet. How many days   |  we solve for the fraction of students absent:  30 \nwill it take for the snail to reach the top of the  |  = (3/4) * (22)  This equation is also not true,   \nwell? ### Answer: To solve this problem, we need    |  which means there is a mistake in the setup. Let's\nto understand the pattern of the snail's movement.  |  correct the setup one more time:  (3/4) * 40 =    \nThe snail climbs 3 feet during the day and slips    |  (3/4) * (31 - 9)  Now, we solve for the fraction  \nback 2 feet at night, so it effectively moves 1     |  of students absent:  30 = (3/4) * (22)  This      \nfoot up each day.  However, on the day the snail    |  equation is also not true, which means there is a \nreaches the top, it will not slip back at night     |  mistake in the setup. Let's correct the setup one \nbecause it will have already escaped the well.      |  more time:  (3/4) * 40 = (3/4) * (31 - 9)  Now, we\nThe well is 20 feet deep, and the snail moves 1     |  solve for the fraction of students absent:  30 =  \nfoot up each day. So, after 17 days, the snail      |  (3/4) * (22)  This equation is also not true,     \nwill be at 17 feet. On the 18th day, it will climb  |  which means there is a mistake in the setup. Let's\n3 feet and reach 20 feet, which is the top of the   |  correct the setup one more time:  (3/4) * 40 =    \nwell.  Therefore, it will take the snail 18 days    |  (3/4) * (31 - 9)  Now, we solve for the fraction  \nto reach the top of the well.  ### Question: A bat  |  of students absent:  30 = (3/4) * (22)  This      \nand a ball together cost $1.10. The bat costs       |  equation is also not true, which means there is a \n$1.00 more than the ball. How much does the ball    |  mistake in the setup. Let's correct the setup one \ncost? ### Answer: Let's denote the cost of the      |  more time:  (3/4) * 40 = (3/4) * (31 - 9)  Now, we\nball as x. Since the bat costs $1.00 more than the  |  solve for the fraction of students absent:  30 =  \nball, the cost of the bat is x + $1.00.  We know    |  (3/4) * (22)  This equation is also not true,     \nthat the total cost of the bat and the ball is      |  which means there is a mistake in the setup. Let's\n$1.10. So, we can set up the following equation:    |  correct the setup one more time:  (3/4) * 40 =    \nx + (x + $1.00) = $1.10  Combine like terms:  2x +  |  (3/4) * (31 - 9)  Now, we solve for the fraction  \n$1.00 = $1.10  Subtract $1.00 from both sides:  2x  |  of students absent:  30 = (3/4) * (22)  This      \n= $0.10  Divide both sides by 2:  x = $0.05         |  equation is also not true, which means there is a \nTherefore, the ball costs $0.05.  ### Question: A   |  mistake in the setup. Let's correct the setup one \nwoman has two coins that add up to 30 cents. One    |  more time:  (3/4) * 40 = (3/4) * (31 - 9)  Now, we\ncoin is not a nickel. What are the two coins? ###   |  solve for the fraction of students absent:  30 =  \nAnswer: The statement \"One coin is not a nickel\"    |  (3/4) * (22)  This equation is also not true,     \nis often misinterpreted as \"Neither of the coins    |  which means there is a mistake in the setup. Let's\nis a nickel.\" However, it only says that one of     |  correct the setup one more time:  (3/4) * 40 =    \nthe coins is not a nickel, leaving the possibility  |  (3/4) * (31 - 9)  Now, we solve for the fraction  \nthat the other coin is a nickel.  Since the total   |  of students absent:  30 = (3/4) * (22)  This      \namount is 30 cents, and a nickel is 5 cents, we     |  equation is also not true, which means there is a \ncan try different combinations of coins to reach    |  mistake in the setup. Let's correct the setup one \nthis amount.  A quarter (25 cents) and a nickel (5  |  more time:  (3/4) * 40 = (3/4) * (31 - 9)  Now, we\ncents) add up to 30 cents. This combination         |  solve for the fraction of students absent:  30 =  \nsatisfies the condition that one coin is not a      |  (3/4) * (22)  This equation is also not true,     \nnickel (the quarter), while the other coin is a     |  which means there is a mistake in the setup. Let's\nnickel.  Therefore, the two coins are a quarter     |  correct the setup one more time:  (3/4) * 40 =    \nand a nickel.                                       |  (3/4) * (31 - 9)  Now, we solve for the fraction  \n                                                    |  of students absent:  30 = (3/4) * (22)  This      \n                                                    |  equation is also not true                         \n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:11:57.513905Z","iopub.execute_input":"2024-12-08T12:11:57.514186Z","iopub.status.idle":"2024-12-08T12:12:31.546706Z","shell.execute_reply.started":"2024-12-08T12:11:57.514158Z","shell.execute_reply":"2024-12-08T12:12:31.545859Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n### Question: Sehun wants to make a three-digit     |  ### Question: Sehun wants to make a three-digit   \nnumber by using the single-digit natural numbers    |  number by using the single-digit natural numbers  \n0, 2, and 9 once each. How many total can he make?  |  0, 2, and 9 once each. How many total can he make?\n### Answer: Sehun can make 10 different three-      |  ### Answer: Sehun can make 10 different three-    \ndigit numbers.  Explanation: 1. First, we need to   |  digit numbers.  Explanation: 1. First, we need to \nfind the total number of ways to arrange the        |  find the total number of ways to arrange the      \ndigits 0, 2, and 9. Since there are three digits,   |  digits 0, 2, and 9. Since there are three digits, \nthere are 3! (3 factorial) ways to arrange them,    |  there are 3! (3 factorial) ways to arrange them,  \nwhich is 3 * 2 * 1 = 6. 2. Next, we need to find    |  which is 3 * 2 * 1 = 6. 2. Next, we need to find  \nthe number of ways to arrange the digits 0, 2, and  |  the number of ways to arrange the digits 0, 2, and\n9 to form a three-digit number. There are 3         |  9 to form a three-digit number. There are three   \nchoices for the first digit (0, 2, or 9), 2         |  possible three-digit numbers: 029, 092, and 290.  \nchoices for the second digit (any of the remaining  |  3. Finally, we need to find the number of ways to \ntwo digits), and 1 choice for the third digit (the  |  arrange the digits 0, 2, and 9 to form a three-   \nremaining digit). So, there are 3 * 2 * 1 = 6 ways  |  digit number that is divisible by 3. A number is  \nto arrange the digits to form a three-digit         |  divisible by 3 if the sum of its digits is        \nnumber. 3. Finally, we need to find the number of   |  divisible by 3. The sum of the digits 0, 2, and 9 \nthree-digit numbers that can be formed using the    |  is 0 + 2 + 9 = 11, which is not divisible by 3.   \ndigits 0, 2, and 9. Since the digits are distinct,  |  Therefore, Sehun can make 10 different three-digit\nthere are 3 choices for the first digit, 2 choices  |  numbers.  The final answer is: $\\boxed{10}$       \nfor the second digit, and 1 choice for the third    |                                                    \ndigit. So, there are 3 * 2 * 1 = 6 three-digit      |                                                    \nnumbers that can be formed using the digits 0, 2,   |                                                    \nand 9.  Therefore, Sehun can make 10 different      |                                                    \nthree-digit numbers by using the single-digit       |                                                    \nnatural numbers 0, 2, and 9 once each.  The final   |                                                    \nanswer is: $\\boxed{10}$                             |                                                    \n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:07:43.505530Z","iopub.execute_input":"2024-12-08T12:07:43.505916Z","iopub.status.idle":"2024-12-08T12:08:00.387744Z","shell.execute_reply.started":"2024-12-08T12:07:43.505879Z","shell.execute_reply":"2024-12-08T12:08:00.386853Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n### Question: If A is the number of diagonals in a  |  ### Question: If A is the number of diagonals in a\nheptagon and B is the number of diagonals in an     |  heptagon and B is the number of diagonals in an   \noctagon, find the value of B-A. ### Answer: To      |  octagon, find the value of B-A. ### Answer: The   \nfind the value of B-A, we first need to determine   |  number of diagonals in a heptagon (7 sides) is (7 \nthe number of diagonals in each polygon.  For a     |  * 6) / 2 = 21. The number of diagonals in an      \nheptagon (7 sides): - Number of diagonals =         |  octagon (8 sides) is (8 * 7) / 2 = 28. Therefore, \nn(n-3)/2 - Number of diagonals = 7(7-3)/2 - Number  |  B - A = 28 - 21 = 7. The value of B - A is 7. The \nof diagonals = 7(4)/2 - Number of diagonals = 28/2  |  answer is: 7                                      \n- Number of diagonals = 14  For an octagon (8       |                                                    \nsides): - Number of diagonals = n(n-3)/2 - Number   |                                                    \nof diagonals = 8(8-3)/2 - Number of diagonals =     |                                                    \n8(5)/2 - Number of diagonals = 40/2 - Number of     |                                                    \ndiagonals = 20  Now, we find the difference         |                                                    \nbetween the number of diagonals in an octagon and   |                                                    \na heptagon: - B-A = 20 - 14 - B-A = 6  Therefore,   |                                                    \nthe value of B-A is $\\boxed{6}$.                    |                                                    \n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:58:49.295281Z","iopub.execute_input":"2024-12-08T11:58:49.295617Z","iopub.status.idle":"2024-12-08T12:00:31.364050Z","shell.execute_reply.started":"2024-12-08T11:58:49.295589Z","shell.execute_reply":"2024-12-08T12:00:31.363132Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n### Question: How many faces does an octahedron     |  ### Question: How many faces does an octahedron   \nhave? ### Answer: 8 ### Reason: An octahedron is a  |  have? ### Answer: 8 ### Reason: An octahedron is a\npolyhedron with eight faces, each of which is a     |  polyhedron with eight faces, each of which is a   \ntriangle.  Question: How many edges does a          |  triangle.  Question: What is the sum of the       \ntetrahedron have? ### Question: How many edges      |  interior angles of a triangle? ### Question: The  \ndoes a tetrahedron have? ### Answer: 4 ### Reason:  |  sum of the interior angles of a triangle is always\nA tetrahedron is a polyhedron with four edges.      |  180 degrees. ### Answer: 180 ### Reason: This is a\nQuestion: How many edges does a dodecahedron have?  |  fundamental property of triangles in geometry.    \n### Question: How many edges does a dodecahedron    |  Question: What is the formula for the area of a   \nhave? ### Answer: 12 ### Reason: A dodecahedron is  |  triangle? ### Question: The area of a triangle can\na polyhedron with twelve edges.  Question: How      |  be calculated using the formula: area = (base *   \nmany edges does a icosahedron have? ### Question:   |  height) / 2. ### Answer: (base * height) / 2 ###  \nHow many edges does a icosahedron have? ###         |  Reason: This formula is widely used in geometry to\nAnswer: 30 ### Reason: An icosahedron is a          |  calculate the area of a triangle.  Question: What \npolyhedron with thirty edges.  Question: How many   |  is the formula for the volume of a cube? ###      \nedges does a cube have? ### Question: How many      |  Question: The volume of a cube can be calculated  \nedges does a cube have? ### Answer: 12 ### Reason:  |  using the formula: volume = side^3. ### Answer:   \nA cube is a polyhedron with twelve edges.           |  side^3 ### Reason: This formula is used to        \nQuestion: How many edges does a sphere have? ###    |  calculate the volume of a cube, where the side    \nQuestion: How many edges does a sphere have? ###    |  length is the length of the side of the cube.     \nAnswer: 0 ### Reason: A sphere is a three-          |  Question: What is the formula for the surface area\ndimensional shape with no edges.  Question: How     |  of a sphere? ### Question: The surface area of a  \nmany edges does a cylinder have? ### Question: How  |  sphere can be calculated using the formula:       \nmany edges does a cylinder have? ### Answer: 2 ###  |  surface area = 4 * pi * radius^2. ### Answer: 4 * \nReason: A cylinder is a three-dimensional shape     |  pi * radius^2 ### Reason: This formula is used to \nwith two parallel faces and two edges.  Question:   |  calculate the surface area of a sphere, where the \nHow many edges does a cone have? ### Question: How  |  radius is the radius of the sphere.  Question:    \nmany edges does a cone have? ### Answer: 1 ###      |  What is the formula for the area of a circle? ### \nReason: A cone is a three-dimensional shape with    |  Question: The area of a circle can be calculated  \none curved surface and one edge.  Question: How     |  using the formula: area = pi * radius^2. ###      \nmany edges does a tetrahedron have? ### Question:   |  Answer: pi * radius^2 ### Reason: This formula is \nHow many edges does a tetrahedron have? ###         |  widely used in geometry to calculate the area of a\nAnswer: 4 ### Reason: A tetrahedron is a            |  circle.  Question: What is the formula for the    \npolyhedron with four edges.  Question: How many     |  volume of a cylinder? ### Question: The volume of \nedges does a dodecahedron have? ### Question: How   |  a cylinder can be calculated using the formula:   \nmany edges does a dodecahedron have? ### Answer:    |  volume = pi * radius^2 * height. ### Answer: pi * \n12 ### Reason: A dodecahedron is a polyhedron with  |  radius^2 * height ### Reason: This formula is used\ntwelve edges.  Question: How many edges does a      |  to calculate the volume of a cylinder, where the  \nsphere have? ### Question: How many edges does a    |  radius and height are the radius and height of the\nsphere have? ### Answer: 0 ### Reason: A sphere is  |  cylinder.  Question: What is the formula for the  \na three-dimensional shape with no edges.            |  surface area of a cone? ### Question: The surface \nQuestion: How many edges does a cylinder have? ###  |  area of a cone can be calculated using the        \nQuestion: How many edges does a cylinder have? ###  |  formula: surface area = pi * radius * slant height\nAnswer: 2 ### Reason: A cylinder is a three-        |  + pi * radius^2. ### Answer: pi * radius * slant  \ndimensional shape with two parallel faces and two   |  height + pi * radius^2 ### Reason: This formula is\nedges.  Question: How many edges does a cone have?  |  used to calculate the surface area of a cone,     \n### Question: How many edges does a cone have? ###  |  where the radius and slant height are the radius  \nAnswer: 1 ### Reason: A cone is a three-            |  and slant height of the cone.  Question: What is  \ndimensional shape with one curved surface and one   |  the formula for the volume of a pyramid? ###      \nedge.                                               |  Question: The volume of a pyramid can be          \n                                                    |  calculated using the formula: volume = (1/3) *    \n                                                    |  base area * height. ### Answer: (1/3) * base area \n                                                    |  * height ### Reason: This formula is used to      \n                                                    |  calculate the volume of a pyramid, where the base \n                                                    |  area and height are the base area and height of   \n                                                    |  the pyramid.  Question: What is the formula for   \n                                                    |  the surface area of a rectangular prism? ###      \n                                                    |  Question: The surface area of a rectangular prism \n                                                    |  can be calculated using the formula: surface area \n                                                    |  = 2 * (length * width + width * height + length * \n                                                    |  height). ### Answer: 2 * (length * width + width *\n                                                    |  height + length * height) ### Reason: This formula\n                                                    |  is used to calculate the surface area of a        \n                                                    |  rectangular prism, where the length, width, and   \n                                                    |  height are the length, width, and height of the   \n                                                    |  prism.  Question: What is the formula for the     \n                                                    |  volume of a sphere? ### Question: The volume of a \n                                                    |  sphere can be calculated using the formula: volume\n                                                    |  = (4/3) * pi * radius^3. ### Answer: (4/3) * pi * \n                                                    |  radius^3 ### Reason: This formula is used to      \n                                                    |  calculate the volume of a sphere, where the radius\n                                                    |  is the radius of the sphere.  Question: What is   \n                                                    |  the formula for the surface area of a cone? ###   \n                                                    |  Question: The surface area of a cone can be       \n                                                    |  calculated using the formula: surface area = pi * \n                                                    |  radius * slant height + pi * radius^2. ### Answer:\n                                                    |  pi * radius * slant height + pi * radius^2 ###    \n                                                    |  Reason: This formula is used to calculate the     \n                                                    |  surface area of a cone, where the radius and slant\n                                                    |  height are the radius and slant height of the     \n                                                    |  cone.  Question: What is the formula for the      \n                                                    |  volume of a cylinder? ### Question: The volume of \n                                                    |  a cylinder can be calculated using the formula:   \n                                                    |  volume = pi * radius^                             \n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.idle":"2024-12-08T12:15:55.979017Z","shell.execute_reply.started":"2024-12-08T12:14:41.983623Z","shell.execute_reply":"2024-12-08T12:15:55.978129Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    906070016                      |                     1050773504                    \n================================================== | ==================================================\n### Question: Josh has 18 yards of ribbon that is   |  ### Question: Josh has 18 yards of ribbon that is \nto be used equally to some gifts. Each gift will    |  to be used equally to some gifts. Each gift will  \nuse 2 yards of ribbon. There will be 6 yards of     |  use 2 yards of ribbon. There will be 6 yards of   \nribbon left. How many gifts will receive ribbon?    |  ribbon left. How many gifts will receive ribbon?  \n### Answer: To find out how many gifts will         |  ### Answer: To find out how many gifts will       \nreceive ribbon, we first need to determine how      |  receive ribbon, we first need to determine how    \nmany yards of ribbon are left after giving some to  |  many yards of ribbon are left after giving some to\nthe gifts.  Josh has 18 yards of ribbon. Each gift  |  the gifts.  Josh has 18 yards of ribbon. Each gift\nwill use 2 yards of ribbon. There will be 6 yards   |  will use 2 yards of ribbon. There will be 6 yards \nof ribbon left.  To find out how many gifts will    |  of ribbon left.  To find out how many gifts will  \nreceive ribbon, we divide the remaining ribbon by   |  receive ribbon, we divide the remaining ribbon by \nthe amount each gift uses:  (18 yards - 6 yards) /  |  the amount each gift uses:  (18 yards - 6 yards) /\n2 yards per gift = 6 / 2 = 3 gifts.  Therefore, 3   |  2 yards per gift = 6 / 2 = 3 gifts.  Therefore, 3 \ngifts will receive ribbon.  ### Question: A snail   |  gifts will receive ribbon.  ### Question: A snail \nis at the bottom of a 20-foot well. Each day, it    |  is at the bottom of a 20-foot well. Each day, it  \nclimbs up 3 feet, but at night, it slips back 2     |  climbs up 3 feet, but at night, it slips back 2   \nfeet. How many days will it take for the snail to   |  feet. How many days will it take for the snail to \nreach the top of the well? ### Answer: To find out  |  reach the top of the well? ### Answer: To find out\nhow many days it will take for the snail to reach   |  how many days it will take for the snail to reach \nthe top of the well, we need to calculate the net   |  the top of the well, we need to calculate the net \nprogress the snail makes each day.  The snail       |  progress the snail makes each day.  The snail     \nclimbs 3 feet during the day and slips back 2 feet  |  climbs 3 feet during the day and slips back 2 feet\nat night, so the net progress is 3 feet - 2 feet =  |  at night, so the net progress is 3 feet - 2 feet =\n1 foot per day.  Since the well is 20 feet deep,    |  1 foot per day.  Since the well is 20 feet deep,  \nthe snail needs to make 20 feet of net progress to  |  the snail needs to make 20 feet of net progress to\nreach the top.  To find the number of days, we      |  reach the top.  To find the number of days, we    \ndivide the total depth by the net progress:  (20    |  divide the total depth by the net progress:  (20  \nfeet - 3 feet) / 1 foot per day = 17 days.          |  feet - 3 feet) / 1 foot per day = 17 days.        \nHowever, on the 18th day, the snail will climb the  |  However, on the 18th day, the snail will climb the\nfinal 3 feet and reach the top of the well.         |  final 3 feet and reach the top of the well.       \nTherefore, it will take the snail 18 days to reach  |  Therefore, it will take the snail 18 days to reach\nthe top of the well.  ### Question: A bat and a     |  the top of the well.  ### Question: A bat and a   \nball together cost $1.10. The bat costs $1.00 more  |  ball together cost $1.10. The bat costs $1.00 more\nthan the ball. How much does the ball cost? ###     |  than the ball. How much does the ball cost? ###   \nAnswer: To find out how much the ball costs, we     |  Answer:                                           \nneed to set up an equation based on the given       |                                                    \ninformation.  Let's denote the cost of the ball as  |                                                    \nx. Since the bat costs $1.00 more than the ball,    |                                                    \nthe cost of the bat is x + $1.00.  We know that     |                                                    \nthe total cost of the bat and the ball is $1.10,    |                                                    \nso we can set up the equation:  x + (x + $1.00) =   |                                                    \n$1.10  Combine like terms:  2x + $1.00 = $1.10      |                                                    \nSubtract $1.00 from both sides:  2x = $0.10         |                                                    \nDivide both sides by 2:  x = $0.05  Therefore, the  |                                                    \nball costs $0.05.  ### Question: A woman has two    |                                                    \ncoins that add up to 30 cents. One coin is not a    |                                                    \nnickel. What are the two coins? ### Answer: The     |                                                    \nstatement \"One coin is not a nickel\" is often       |                                                    \nmisinterpreted as \"Neither of the coins is a        |                                                    \nnickel.\" However, it only says that one of the      |                                                    \ncoins is not a nickel, leaving the possibility      |                                                    \nthat the other coin is a nickel.  Since the total   |                                                    \namount is 30 cents, and we know that one of the     |                                                    \ncoins is a nickel (5 cents), we can try to find a   |                                                    \ncombination of coins that adds up to 30 cents.  A   |                                                    \nquarter (25 cents) and a nickel (5 cents) add up    |                                                    \nto 30 cents.  Therefore, the two coins are a        |                                                    \nquarter and a nickel.  ### Question: A man is       |                                                    \nlooking at a photograph of someone. His friend      |                                                    \nasks him, \"Who is in the picture?\" The man          |                                                    \nreplies, \"Brothers and sisters, I have none. But    |                                                    \nthat man's father is my father's son.\" Who is in    |                                                    \nthe picture? ### Answer: To solve this puzzle, we   |                                                    \nneed to break down the information given in the     |                                                    \nman's statement.  The man says, \"Brothers and       |                                                    \nsisters, I have none.\" This means he has no         |                                                    \nsiblings.  Then he says, \"But that man's father is  |                                                    \nmy father's son.\"  Since the man has no siblings,   |                                                    \nthe only son of his father is the man himself.      |                                                    \nSo, the statement \"my father's son\" refers to the   |                                                    \nman himself.  Therefore, the man in the picture is  |                                                    \nthe son of the man who is speaking.  The man in     |                                                    \nthe picture is the speaker's son.  ### Question:    |                                                    \nThere are three switches, but they are not          |                                                    \nlabelled. Each switch corresponds to one of three   |                                                    \nlight bulbs in a room. Each light bulb is either    |                                                    \non or off. You can turn                             |                                                    \n","output_type":"stream"}],"execution_count":68}]}