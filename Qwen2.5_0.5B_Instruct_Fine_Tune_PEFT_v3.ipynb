{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-07T03:10:48.704227Z","iopub.execute_input":"2024-12-07T03:10:48.704612Z","iopub.status.idle":"2024-12-07T03:11:45.310379Z","shell.execute_reply.started":"2024-12-07T03:10:48.704578Z","shell.execute_reply":"2024-12-07T03:11:45.309054Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.12.2 requires transformers<4.47.0, but you have transformers 4.47.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-07T03:11:45.312414Z","iopub.execute_input":"2024-12-07T03:11:45.312824Z","iopub.status.idle":"2024-12-07T03:11:52.495161Z","shell.execute_reply.started":"2024-12-07T03:11:45.312778Z","shell.execute_reply":"2024-12-07T03:11:52.494195Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-07T03:11:52.496454Z","iopub.execute_input":"2024-12-07T03:11:52.496818Z","iopub.status.idle":"2024-12-07T03:11:52.814747Z","shell.execute_reply.started":"2024-12-07T03:11:52.496776Z","shell.execute_reply":"2024-12-07T03:11:52.813729Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'Qwen/Qwen2.5-0.5B-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-07T03:11:52.816550Z","iopub.execute_input":"2024-12-07T03:11:52.816953Z","iopub.status.idle":"2024-12-07T03:11:52.825802Z","shell.execute_reply.started":"2024-12-07T03:11:52.816911Z","shell.execute_reply":"2024-12-07T03:11:52.825074Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-07T03:11:52.826685Z","iopub.execute_input":"2024-12-07T03:11:52.826910Z","iopub.status.idle":"2024-12-07T03:11:52.838099Z","shell.execute_reply.started":"2024-12-07T03:11:52.826886Z","shell.execute_reply":"2024-12-07T03:11:52.837340Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:52.839015Z","iopub.execute_input":"2024-12-07T03:11:52.839263Z","iopub.status.idle":"2024-12-07T03:11:54.918245Z","shell.execute_reply.started":"2024-12-07T03:11:52.839238Z","shell.execute_reply":"2024-12-07T03:11:54.917461Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:54.919483Z","iopub.execute_input":"2024-12-07T03:11:54.920226Z","iopub.status.idle":"2024-12-07T03:11:54.927277Z","shell.execute_reply.started":"2024-12-07T03:11:54.920182Z","shell.execute_reply":"2024-12-07T03:11:54.926316Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 315119488\nTrainable parameters : 136178560\nTrainable percentage: 43.21%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:54.928448Z","iopub.execute_input":"2024-12-07T03:11:54.929044Z","iopub.status.idle":"2024-12-07T03:11:55.369921Z","shell.execute_reply.started":"2024-12-07T03:11:54.928999Z","shell.execute_reply":"2024-12-07T03:11:55.369112Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:55.371250Z","iopub.execute_input":"2024-12-07T03:11:55.371701Z","iopub.status.idle":"2024-12-07T03:11:55.376236Z","shell.execute_reply.started":"2024-12-07T03:11:55.371651Z","shell.execute_reply":"2024-12-07T03:11:55.375216Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:55.380516Z","iopub.execute_input":"2024-12-07T03:11:55.380947Z","iopub.status.idle":"2024-12-07T03:11:55.386384Z","shell.execute_reply.started":"2024-12-07T03:11:55.380919Z","shell.execute_reply":"2024-12-07T03:11:55.385554Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:55.387282Z","iopub.execute_input":"2024-12-07T03:11:55.387535Z","iopub.status.idle":"2024-12-07T03:11:57.189147Z","shell.execute_reply.started":"2024-12-07T03:11:55.387511Z","shell.execute_reply":"2024-12-07T03:11:57.188361Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(20000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.190086Z","iopub.execute_input":"2024-12-07T03:11:57.190361Z","iopub.status.idle":"2024-12-07T03:11:57.196548Z","shell.execute_reply.started":"2024-12-07T03:11:57.190325Z","shell.execute_reply":"2024-12-07T03:11:57.195732Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.197802Z","iopub.execute_input":"2024-12-07T03:11:57.198416Z","iopub.status.idle":"2024-12-07T03:11:57.217457Z","shell.execute_reply.started":"2024-12-07T03:11:57.198373Z","shell.execute_reply":"2024-12-07T03:11:57.216665Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.218503Z","iopub.execute_input":"2024-12-07T03:11:57.219040Z","iopub.status.idle":"2024-12-07T03:11:57.224874Z","shell.execute_reply.started":"2024-12-07T03:11:57.219000Z","shell.execute_reply":"2024-12-07T03:11:57.224029Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.226187Z","iopub.execute_input":"2024-12-07T03:11:57.226808Z","iopub.status.idle":"2024-12-07T03:11:57.236475Z","shell.execute_reply.started":"2024-12-07T03:11:57.226779Z","shell.execute_reply":"2024-12-07T03:11:57.235563Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.237509Z","iopub.execute_input":"2024-12-07T03:11:57.237999Z","iopub.status.idle":"2024-12-07T03:11:57.245982Z","shell.execute_reply.started":"2024-12-07T03:11:57.237971Z","shell.execute_reply":"2024-12-07T03:11:57.245190Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.246924Z","iopub.execute_input":"2024-12-07T03:11:57.247175Z","iopub.status.idle":"2024-12-07T03:11:57.258199Z","shell.execute_reply.started":"2024-12-07T03:11:57.247150Z","shell.execute_reply":"2024-12-07T03:11:57.257477Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.259112Z","iopub.execute_input":"2024-12-07T03:11:57.259468Z","iopub.status.idle":"2024-12-07T03:11:57.271684Z","shell.execute_reply.started":"2024-12-07T03:11:57.259429Z","shell.execute_reply":"2024-12-07T03:11:57.270943Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 20000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.272511Z","iopub.execute_input":"2024-12-07T03:11:57.272743Z","iopub.status.idle":"2024-12-07T03:11:57.279858Z","shell.execute_reply.started":"2024-12-07T03:11:57.272718Z","shell.execute_reply":"2024-12-07T03:11:57.279157Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.280724Z","iopub.execute_input":"2024-12-07T03:11:57.280969Z","iopub.status.idle":"2024-12-07T03:11:57.290947Z","shell.execute_reply.started":"2024-12-07T03:11:57.280944Z","shell.execute_reply":"2024-12-07T03:11:57.290090Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.291807Z","iopub.execute_input":"2024-12-07T03:11:57.292085Z","iopub.status.idle":"2024-12-07T03:11:57.400939Z","shell.execute_reply.started":"2024-12-07T03:11:57.292058Z","shell.execute_reply":"2024-12-07T03:11:57.400093Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 20000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.402107Z","iopub.execute_input":"2024-12-07T03:11:57.402953Z","iopub.status.idle":"2024-12-07T03:11:57.410102Z","shell.execute_reply.started":"2024-12-07T03:11:57.402907Z","shell.execute_reply":"2024-12-07T03:11:57.409222Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.411161Z","iopub.execute_input":"2024-12-07T03:11:57.411429Z","iopub.status.idle":"2024-12-07T03:11:57.425501Z","shell.execute_reply.started":"2024-12-07T03:11:57.411403Z","shell.execute_reply":"2024-12-07T03:11:57.424712Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 18000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 2000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.426569Z","iopub.execute_input":"2024-12-07T03:11:57.427187Z","iopub.status.idle":"2024-12-07T03:11:57.436263Z","shell.execute_reply.started":"2024-12-07T03:11:57.427147Z","shell.execute_reply":"2024-12-07T03:11:57.435600Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 18000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.437164Z","iopub.execute_input":"2024-12-07T03:11:57.437426Z","iopub.status.idle":"2024-12-07T03:11:57.462786Z","shell.execute_reply.started":"2024-12-07T03:11:57.437400Z","shell.execute_reply":"2024-12-07T03:11:57.462039Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nCompared to the amount of water...   \n1  ### Question:\\nEunseol divided 28 candies equa...   \n2  ### Question:\\nNancy and Rose are making brace...   \n3  ### Question:\\nWithout factoring in the cost o...   \n4  ### Question:\\nThere are 250 books inside a li...   \n\n                                           input_ids  \\\n0  [14374, 15846, 510, 1092, 7212, 311, 279, 3311...   \n1  [14374, 15846, 510, 36, 359, 325, 337, 17779, ...   \n2  [14374, 15846, 510, 45, 6572, 323, 15964, 525,...   \n3  [14374, 15846, 510, 26040, 2097, 5503, 304, 27...   \n4  [14374, 15846, 510, 3862, 525, 220, 17, 20, 15...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nCompared to the amount of water...</td>\n      <td>[14374, 15846, 510, 1092, 7212, 311, 279, 3311...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nEunseol divided 28 candies equa...</td>\n      <td>[14374, 15846, 510, 36, 359, 325, 337, 17779, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nNancy and Rose are making brace...</td>\n      <td>[14374, 15846, 510, 45, 6572, 323, 15964, 525,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nWithout factoring in the cost o...</td>\n      <td>[14374, 15846, 510, 26040, 2097, 5503, 304, 27...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere are 250 books inside a li...</td>\n      <td>[14374, 15846, 510, 3862, 525, 220, 17, 20, 15...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.463705Z","iopub.execute_input":"2024-12-07T03:11:57.463936Z","iopub.status.idle":"2024-12-07T03:11:57.468779Z","shell.execute_reply.started":"2024-12-07T03:11:57.463911Z","shell.execute_reply":"2024-12-07T03:11:57.467864Z"}},"outputs":[{"name":"stdout","text":"### Question:\nCompared to the amount of water she drank, Carla drank three times as much soda minus a certain number of ounces. She drank 54 ounces of liquid total. If Carla drank 15 ounces of water, how many ounces were subtracted from the amount of soda she drank?\n### Answer:\nLet's call the number of ounces subtracted from the soda \"x.\"\n\nCarla drank 15 ounces of water. Since she drank three times as much soda as water, she would have drunk 3 * 15 = 45 ounces of soda without any subtraction.\n\nHowever, we know that she drank a total of 54 ounces of liquid. So, the water and the actual amount of soda she drank (after subtracting x ounces) should add up to 54 ounces.\n\nSo, we have:\n15 ounces (water) + (45 ounces - x) (soda) = 54 ounces (total)\n\nNow, let's solve for x:\n15 + 45 - x = 54\n60 - x = 54\nx = 60 - 54\nx = 6 ounces\n\nTherefore, 6 ounces were subtracted from the amount of soda she would have drunk.<|im_end|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.469968Z","iopub.execute_input":"2024-12-07T03:11:57.470188Z","iopub.status.idle":"2024-12-07T03:11:57.478669Z","shell.execute_reply.started":"2024-12-07T03:11:57.470165Z","shell.execute_reply":"2024-12-07T03:11:57.477934Z"}},"outputs":[{"name":"stdout","text":"[14374, 15846, 510, 1092, 7212, 311, 279, 3311, 315, 3015, 1340, 53144, 11, 93770, 53144, 2326, 3039, 438, 1753, 38862, 27283, 264, 3654, 1372, 315, 48038, 13, 2932, 53144, 220, 20, 19, 48038, 315, 14473, 2790, 13, 1416, 93770, 53144, 220, 16, 20, 48038, 315, 3015, 11, 1246, 1657, 48038, 1033, 32256, 291, 504, 279, 3311, 315, 38862, 1340, 53144, 5267, 14374, 21806, 510, 10061, 594, 1618, 279, 1372, 315, 48038, 32256, 291, 504, 279, 38862, 330, 87, 2217, 8852, 4260, 53144, 220, 16, 20, 48038, 315, 3015, 13, 8704, 1340, 53144, 2326, 3039, 438, 1753, 38862, 438, 3015, 11, 1340, 1035, 614, 28750, 220, 18, 353, 220, 16, 20, 284, 220, 19, 20, 48038, 315, 38862, 2041, 894, 75240, 382, 11209, 11, 582, 1414, 429, 1340, 53144, 264, 2790, 315, 220, 20, 19, 48038, 315, 14473, 13, 2055, 11, 279, 3015, 323, 279, 5042, 3311, 315, 38862, 1340, 53144, 320, 10694, 32256, 287, 856, 48038, 8, 1265, 912, 705, 311, 220, 20, 19, 48038, 382, 4416, 11, 582, 614, 510, 16, 20, 48038, 320, 12987, 8, 488, 320, 19, 20, 48038, 481, 856, 8, 320, 82, 13993, 8, 284, 220, 20, 19, 48038, 320, 5035, 692, 7039, 11, 1077, 594, 11625, 369, 856, 510, 16, 20, 488, 220, 19, 20, 481, 856, 284, 220, 20, 19, 198, 21, 15, 481, 856, 284, 220, 20, 19, 198, 87, 284, 220, 21, 15, 481, 220, 20, 19, 198, 87, 284, 220, 21, 48038, 271, 54815, 11, 220, 21, 48038, 1033, 32256, 291, 504, 279, 3311, 315, 38862, 1340, 1035, 614, 28750, 13, 151645, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.483887Z","iopub.execute_input":"2024-12-07T03:11:57.484413Z","iopub.status.idle":"2024-12-07T03:11:57.489124Z","shell.execute_reply.started":"2024-12-07T03:11:57.484385Z","shell.execute_reply":"2024-12-07T03:11:57.488333Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.490153Z","iopub.execute_input":"2024-12-07T03:11:57.490451Z","iopub.status.idle":"2024-12-07T03:11:57.500010Z","shell.execute_reply.started":"2024-12-07T03:11:57.490422Z","shell.execute_reply":"2024-12-07T03:11:57.499356Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.501055Z","iopub.execute_input":"2024-12-07T03:11:57.501674Z","iopub.status.idle":"2024-12-07T03:11:57.513806Z","shell.execute_reply.started":"2024-12-07T03:11:57.501645Z","shell.execute_reply":"2024-12-07T03:11:57.513025Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.514758Z","iopub.execute_input":"2024-12-07T03:11:57.515459Z","iopub.status.idle":"2024-12-07T03:11:57.524843Z","shell.execute_reply.started":"2024-12-07T03:11:57.515432Z","shell.execute_reply":"2024-12-07T03:11:57.523975Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.525820Z","iopub.execute_input":"2024-12-07T03:11:57.526432Z","iopub.status.idle":"2024-12-07T03:11:57.534271Z","shell.execute_reply.started":"2024-12-07T03:11:57.526392Z","shell.execute_reply":"2024-12-07T03:11:57.533582Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:57.535126Z","iopub.execute_input":"2024-12-07T03:11:57.535398Z","iopub.status.idle":"2024-12-07T03:11:58.099185Z","shell.execute_reply.started":"2024-12-07T03:11:57.535372Z","shell.execute_reply":"2024-12-07T03:11:58.098279Z"}},"outputs":[{"name":"stdout","text":"trainable params: 35,192,832 || all params: 529,225,600 || trainable%: 6.6499\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:58.100312Z","iopub.execute_input":"2024-12-07T03:11:58.100610Z","iopub.status.idle":"2024-12-07T03:11:58.113210Z","shell.execute_reply.started":"2024-12-07T03:11:58.100581Z","shell.execute_reply":"2024-12-07T03:11:58.112320Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 350312320\nTrainable parameters : 35192832\nTrainable percentage: 10.05%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:58.114280Z","iopub.execute_input":"2024-12-07T03:11:58.114587Z","iopub.status.idle":"2024-12-07T03:11:58.121282Z","shell.execute_reply.started":"2024-12-07T03:11:58.114542Z","shell.execute_reply":"2024-12-07T03:11:58.120567Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 4\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:58.122163Z","iopub.execute_input":"2024-12-07T03:11:58.122426Z","iopub.status.idle":"2024-12-07T03:11:58.161206Z","shell.execute_reply.started":"2024-12-07T03:11:58.122400Z","shell.execute_reply":"2024-12-07T03:11:58.160443Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec07_03-11-58_483922eafc63,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:58.162267Z","iopub.execute_input":"2024-12-07T03:11:58.162555Z","iopub.status.idle":"2024-12-07T03:11:59.202083Z","shell.execute_reply.started":"2024-12-07T03:11:58.162529Z","shell.execute_reply":"2024-12-07T03:11:59.201268Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x79e0145ad7b0>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:11:59.203132Z","iopub.execute_input":"2024-12-07T03:11:59.203434Z","iopub.status.idle":"2024-12-07T03:25:21.229652Z","shell.execute_reply.started":"2024-12-07T03:11:59.203405Z","shell.execute_reply":"2024-12-07T03:25:21.228776Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 4\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 18,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 35,192,832\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113771433338115, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"173ed75feb8e47a98acb781fcaa124ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241207_031200-zgo6s66z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/zgo6s66z' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/zgo6s66z' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/zgo6s66z</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 13:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.879800</td>\n      <td>0.826254</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.789000</td>\n      <td>0.740061</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.719800</td>\n      <td>0.673777</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.676900</td>\n      <td>0.620947</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.614600</td>\n      <td>0.602810</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.601800</td>\n      <td>0.596286</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.597000</td>\n      <td>0.592829</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.595100</td>\n      <td>0.591062</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.594800</td>\n      <td>0.590348</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.608100</td>\n      <td>0.590278</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.6676997137069702, metrics={'train_runtime': 801.6168, 'train_samples_per_second': 3.992, 'train_steps_per_second': 0.249, 'total_flos': 3157650623692800.0, 'train_loss': 0.6676997137069702, 'epoch': 0.17777777777777778})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:21.230808Z","iopub.execute_input":"2024-12-07T03:25:21.231080Z","iopub.status.idle":"2024-12-07T03:25:38.714461Z","shell.execute_reply.started":"2024-12-07T03:25:21.231047Z","shell.execute_reply":"2024-12-07T03:25:38.713609Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.5902778506278992, 'eval_runtime': 17.4708, 'eval_samples_per_second': 11.448, 'eval_steps_per_second': 2.862, 'epoch': 0.17777777777777778}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:38.715816Z","iopub.execute_input":"2024-12-07T03:25:38.716182Z","iopub.status.idle":"2024-12-07T03:25:39.833730Z","shell.execute_reply.started":"2024-12-07T03:25:38.716142Z","shell.execute_reply":"2024-12-07T03:25:39.832629Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:39.835040Z","iopub.execute_input":"2024-12-07T03:25:39.835402Z","iopub.status.idle":"2024-12-07T03:25:40.139478Z","shell.execute_reply.started":"2024-12-07T03:25:39.835367Z","shell.execute_reply":"2024-12-07T03:25:40.138335Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:40.141086Z","iopub.execute_input":"2024-12-07T03:25:40.141503Z","iopub.status.idle":"2024-12-07T03:25:40.151632Z","shell.execute_reply.started":"2024-12-07T03:25:40.141456Z","shell.execute_reply":"2024-12-07T03:25:40.151005Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:40.152419Z","iopub.execute_input":"2024-12-07T03:25:40.152757Z","iopub.status.idle":"2024-12-07T03:25:41.137347Z","shell.execute_reply.started":"2024-12-07T03:25:40.152715Z","shell.execute_reply":"2024-12-07T03:25:41.136468Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:41.139086Z","iopub.execute_input":"2024-12-07T03:25:41.139460Z","iopub.status.idle":"2024-12-07T03:25:43.092657Z","shell.execute_reply.started":"2024-12-07T03:25:41.139417Z","shell.execute_reply":"2024-12-07T03:25:43.091828Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:43.093658Z","iopub.execute_input":"2024-12-07T03:25:43.093933Z","iopub.status.idle":"2024-12-07T03:25:43.103998Z","shell.execute_reply.started":"2024-12-07T03:25:43.093904Z","shell.execute_reply":"2024-12-07T03:25:43.103326Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 315119488\nTrainable parameters : 136178560\nTrainable percentage: 43.21%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:43.105014Z","iopub.execute_input":"2024-12-07T03:25:43.105429Z","iopub.status.idle":"2024-12-07T03:25:43.127572Z","shell.execute_reply.started":"2024-12-07T03:25:43.105392Z","shell.execute_reply":"2024-12-07T03:25:43.126782Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 896)\n        (layers): ModuleList(\n          (0-23): 24 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=896, bias=False)\n                  (default): Linear(in_features=64, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=128, bias=False)\n                  (default): Linear(in_features=64, out_features=128, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=128, bias=False)\n                  (default): Linear(in_features=64, out_features=128, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=896, bias=False)\n                  (default): Linear(in_features=64, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=4864, bias=False)\n                  (default): Linear(in_features=64, out_features=4864, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=4864, bias=False)\n                  (default): Linear(in_features=64, out_features=4864, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4864, out_features=896, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4864, out_features=64, bias=False)\n                  (default): Linear(in_features=4864, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=896, bias=False)\n                  (default): Linear(in_features=64, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:43.128572Z","iopub.execute_input":"2024-12-07T03:25:43.128826Z","iopub.status.idle":"2024-12-07T03:25:43.155995Z","shell.execute_reply.started":"2024-12-07T03:25:43.128801Z","shell.execute_reply":"2024-12-07T03:25:43.155138Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 385505152\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:43.157036Z","iopub.execute_input":"2024-12-07T03:25:43.157271Z","iopub.status.idle":"2024-12-07T03:25:43.171364Z","shell.execute_reply.started":"2024-12-07T03:25:43.157247Z","shell.execute_reply":"2024-12-07T03:25:43.170557Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:43.172338Z","iopub.execute_input":"2024-12-07T03:25:43.172718Z","iopub.status.idle":"2024-12-07T03:25:43.182172Z","shell.execute_reply.started":"2024-12-07T03:25:43.172686Z","shell.execute_reply":"2024-12-07T03:25:43.181357Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T03:25:43.183360Z","iopub.execute_input":"2024-12-07T03:25:43.184144Z","iopub.status.idle":"2024-12-07T03:25:43.194215Z","shell.execute_reply.started":"2024-12-07T03:25:43.184100Z","shell.execute_reply":"2024-12-07T03:25:43.193425Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T04:03:19.405554Z","iopub.execute_input":"2024-12-07T04:03:19.406259Z","iopub.status.idle":"2024-12-07T04:05:26.866483Z","shell.execute_reply.started":"2024-12-07T04:03:19.406223Z","shell.execute_reply":"2024-12-07T04:05:26.865482Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Mary used some gallons of fuel this   |  ### Question: Mary used some gallons of fuel this \nweek. Last week she used 20% less. She used a       |  week. Last week she used 20% less. She used a     \ntotal of 27 gallons of fuel for the two weeks. How  |  total of 27 gallons of fuel for the two weeks. How\nmuch fuel did she use this week? ### Answer: Mary   |  much fuel did she use this week? ### Answer: Mary \nused 27 gallons of fuel this week. She used 20%     |  used 27 gallons of fuel this week. She used 20%   \nless fuel than last week, which means she used 20%  |  less fuel than last week, which means she used 20%\nof the fuel she used last week. To find out how     |  of the fuel she used last week. To find out how   \nmuch fuel she used last week, we can use the        |  much she used last week, we can use the formula:  \nformula:  Fuel used last week = Total fuel used     |  Total fuel used = Total fuel used this week +     \nthis week / (1 - 20%)  Let's calculate the fuel     |  (Total fuel used this week * 20%)  So, we can set \nused last week:  Fuel used last week = 27 gallons   |  up the equation:  27 gallons = Total fuel used    \n/ (1 - 0.20) Fuel used last week = 27 gallons /     |  this week + (27 gallons * 20%)  Now, we can solve \n0.80 Fuel used last week = 33.75 gallons  Now, to   |  for the total fuel used this week:  Total fuel    \nfind out how much fuel she used this week:  Fuel    |  used this week = 27 gallons - 27 gallons * 20%    \nused this week = Total fuel used this week - Fuel   |  Total fuel used this week = 27 gallons - 54       \nused last week Fuel used this week = 27 gallons -   |  gallons Total fuel used this week = -27 gallons   \n33.75 gallons Fuel used this week = -6.75 gallons   |  Since the total fuel used this week cannot be     \nIt seems there was a mistake in the calculation.    |  negative, it means that there is an error in the  \nLet's correct it:  Fuel used last week = 27         |  calculation. Let's correct it:  Total fuel used   \ngallons / (1 - 0.20) Fuel used last week = 27       |  this week = 27 gallons - 27 gallons * 20% Total   \ngallons / 0.80 Fuel used last week = 33.75 gallons  |  fuel used this week = 27 gallons - 54 gallons     \nNow, to find out how much fuel she used this week:  |  Total fuel used this week = -27 gallons  This     \nFuel used this week = Total fuel used this week -   |  means that Mary used 27 gallons of fuel this week,\nFuel used last week Fuel used this week = 27        |  which is the same as the fuel she used last week. \ngallons - 33.75 gallons Fuel used this week =       |  However, since she used 20% less fuel than last   \n-6.75 gallons  It seems there was a mistake in the  |  week, she used 27 gallons of fuel this week.  ### \ncalculation. Let's correct it:  Fuel used last      |  Question: Mary used 27 gallons of fuel this week. \nweek = 27 gallons / (1 - 0.20) Fuel used last week  |  Last week she used 20% less fuel. She used a total\n= 27 gallons / 0.80 Fuel used last week = 33.75     |  of 27 gallons of fuel for the two weeks. How much \ngallons  Now, to find out how much fuel she used    |  fuel did she use this week? ### Answer: Mary used \nthis week:  Fuel used this week = Total fuel used   |  27 gallons of fuel this week. She used 20% less   \nthis week - Fuel used last week Fuel used this      |  fuel than last week, which means she used 20% of  \nweek = 27 gallons - 33.75 gallons Fuel used this    |  the fuel she used last week. To find out how much \nweek = -6.75 gallons  It seems there was a mistake  |  she used last week, we can use the formula:  Total\nin the calculation. Let's correct it:  Fuel used    |  fuel used = Total fuel used this week + (Total    \nlast week = 27 gallons / (1 - 0.20) Fuel used last  |  fuel used this week * 20%)  So, we can set up the \nweek = 27 gallons / 0.80 Fuel used last week =      |  equation:  27 gallons = Total fuel used this week \n33.75 gallons  Now, to find out how much fuel she   |  + (27 gallons * 20%)  Now, we can solve for the   \nused this week:  Fuel used this week = Total fuel   |  total fuel used this week:  Total fuel used this  \nused this week - Fuel used last week Fuel used      |  week = 27 gallons - 27 gallons * 20% Total fuel   \nthis week = 27 gallons - 33.75 gallons Fuel used    |  used this week = 27 gallons - 54 gallons Total    \nthis week = -6.75 gallons  It seems there was a     |  fuel used this week = -27 gallons  Since the total\nmistake in the calculation. Let's correct it:       |  fuel used this week cannot be negative, it means  \nFuel used last week = 27 gallons / (1 - 0.20) Fuel  |  that there is an error in the calculation. Let's  \nused last week = 27 gallons / 0.80 Fuel used last   |  correct it:  Total fuel used this week = 27       \nweek = 33.75 gallons  Now, to find out how much     |  gallons - 27 gallons * 20% Total fuel used this   \nfuel she used this week:  Fuel used this week =     |  week = 27 gallons - 54 gallons Total fuel used    \nTotal fuel used this week - Fuel used last week     |  this week = -27 gallons  This means that Mary used\nFuel used this week = 27 gallons - 33.75 gallons    |  27 gallons of fuel this week, which is the same as\nFuel used this week = -6.75 gallons  It seems       |  the fuel she used last week. However, since she   \nthere was a mistake in the calculation. Let's       |  used 20% less fuel than last week, she used 27    \ncorrect it:  Fuel used last week = 27 gallons / (1  |  gallons of fuel this week.  ### Question: Mary    \n- 0.20) Fuel used last week = 27 gallons / 0.80     |  used 27 gallons of fuel this week. Last week she  \nFuel used last week = 33.75 gallons  Now, to find   |  used 20% less fuel. She used a total of 27 gallons\nout how much fuel she used this week:  Fuel used    |  of fuel for the two weeks. How much fuel did she  \nthis week = Total fuel used this week - Fuel used   |  use this week? ### Answer: Mary used 27 gallons of\nlast week Fuel used this week = 27 gallons - 33.75  |  fuel this week. She used 20% less fuel than last  \ngallons Fuel used this week = -6.75 gallons  It     |  week, which means she used 20% of the fuel she    \nseems there was a mistake in the calculation.       |  used last week. To find out how much she used last\nLet's correct it:  Fuel used last week = 27         |  week, we can use the formula:  Total fuel used =  \ngallons / (1 - 0.20) Fuel used last week = 27       |  Total fuel used this week + (Total fuel used this \ngallons / 0.80 Fuel used last week = 33.75 gallons  |  week * 20%)  So, we can set up the equation:  27  \nNow, to find out how much fuel she used this week:  |  gallons = Total fuel used this week + (27 gallons \nFuel used this week = Total fuel used this week -   |  * 20%)  Now, we can solve for the total fuel used \nFuel used last week Fuel used this week = 27        |  this week:  Total fuel used this week = 27 gallons\ngallons - 33.75 gallons Fuel used this week =       |  - 27 gallons * 20% Total fuel used this week = 27 \n-6.75 gallons  It seems there was a mistake in the  |  gallons - 54 gallons Total fuel used this week =  \ncalculation. Let's correct it:  Fuel used last      |  -27 gallons  Since the total fuel used this week  \nweek = 27 gallons / (1 - 0.20) Fuel used last week  |  cannot be negative, it means that there is an     \n= 27 gallons / 0.80 Fuel used last week = 33.75     |  error in the calculation. Let's correct it:  Total\ngallons  Now, to find out how much fuel she used    |  fuel used this week = 27 gallons - 27 gallons *   \nthis week:  Fuel used this week = Total fuel used   |  20% Total fuel used this week = 27 gallons - 54   \nthis week - Fuel used last week Fuel used this      |  gallons Total fuel used this week = -27 gallons   \nweek = 27 gallons -                                 |  This means that Mary used 27 gallons of fuel this \n                                                    |  week, which is the same as the fuel she used last \n                                                    |  week. However, since she used 20% less fuel than  \n                                                    |  last week, she used 27 gallons of fuel this week. \n                                                    |  ### Question: Mary used 27 gallons of fuel this   \n                                                    |  week. Last week she used 20% less fuel. She used a\n                                                    |  total of 27 gallons of fuel for the two weeks. How\n                                                    |  much fuel did she use this                        \n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T04:26:27.146037Z","iopub.execute_input":"2024-12-07T04:26:27.146736Z","iopub.status.idle":"2024-12-07T04:28:33.709967Z","shell.execute_reply.started":"2024-12-07T04:26:27.146696Z","shell.execute_reply":"2024-12-07T04:28:33.709088Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: To get to work, Ryan bikes one day a  |  ### Question: To get to work, Ryan bikes one day a\nweek, takes the bus three times a week, and gets a  |  week, takes the bus three times a week, and gets a\nride from a friend once a week. It takes him some   |  ride from a friend once a week. It takes him some \nminutes to bike to work. The bus takes ten minutes  |  minutes to bike to work. The bus takes ten minutes\nlonger since it stops at every bus stop for other   |  longer since it stops at every bus stop for other \npassengers. His friend driving him is the fastest   |  passengers. His friend driving him is the fastest \nand cuts two-thirds off his biking time. He spends  |  and cuts two-thirds off his biking time. He spends\n160 minutes every week commuting to work. How many  |  160 minutes every week commuting to work. How many\nminutes does it take for Ryan to bike to work? ###  |  minutes does it take for Ryan to bike to work? ###\nAnswer: Ryan bikes to work for 1 minute because it  |  Answer: Ryan bikes to work for 1 minute because it\ntakes him some minutes to bike to work. The bus     |  takes him some minutes to bike to work. The bus   \ntakes ten minutes longer than biking so it takes 1  |  takes ten minutes longer than biking so it takes  \n+ 10 = 11 minutes to get to work by bus. His        |  10 + 1 = 11 minutes to get to work by bus. His    \nfriend driving him cuts two-thirds off his biking   |  friend driving him cuts two-thirds off his biking \ntime so he gets 1 - (2/3) = 1/3 of his biking       |  time so he gets 1 - (2/3) = 1/3 of his biking     \ntime. That means he gets 1/3 * 1 = 1/9 of his       |  time. That means he gets 1/3 * 1 = 1/9 of his     \nbiking time. So he gets 1/9 * 1 = 1/9 of his        |  biking time. So he gets 1/9 * 11 = 11/90 minutes  \nbiking time. That means he gets 1/9 * 1 = 1/9 of    |  of biking time. That means it takes him 11/90     \nhis biking time. That means he gets 1/9 * 1 = 1/9   |  minutes to bike to work. He bikes to work for 1   \nof his biking time. That means he gets 1/9 * 1 =    |  minute because it takes him some minutes to bike  \n1/9 of his biking time. That means he gets 1/9 * 1  |  to work. The bus takes ten minutes longer since it\n= 1/9 of his biking time. That means he gets 1/9 *  |  stops at every bus stop for other passengers. His \n1 = 1/9 of his biking time. That means he gets 1/9  |  friend driving him is the fastest and cuts two-   \n* 1 = 1/9 of his biking time. That means he gets    |  thirds off his biking time. He spends 160 minutes \n1/9 * 1 = 1/9 of his biking time. That means he     |  every week commuting to work. He bikes to work for\ngets 1/9 * 1 = 1/9 of his biking time. That means   |  1 minute because it takes him some minutes to bike\nhe gets 1/9 * 1 = 1/9 of his biking time. That      |  to work. The bus takes ten minutes longer since it\nmeans he gets 1/9 * 1 = 1/9 of his biking time.     |  stops at every bus stop for other passengers. His \nThat means he gets 1/9 * 1 = 1/9 of his biking      |  friend driving him is the fastest and cuts two-   \ntime. That means he gets 1/9 * 1 = 1/9 of his       |  thirds off his biking time. He spends 160 minutes \nbiking time. That means he gets 1/9 * 1 = 1/9 of    |  every week commuting to work. He bikes to work for\nhis biking time. That means he gets 1/9 * 1 = 1/9   |  1 minute because it takes him some minutes to bike\nof his biking time. That means he gets 1/9 * 1 =    |  to work. The bus takes ten minutes longer since it\n1/9 of his biking time. That means he gets 1/9 * 1  |  stops at every bus stop for other passengers. His \n= 1/9 of his biking time. That means he gets 1/9 *  |  friend driving him is the fastest and cuts two-   \n1 = 1/9 of his biking time. That means he gets 1/9  |  thirds off his biking time. He spends 160 minutes \n* 1 = 1/9 of his biking time. That means he gets    |  every week commuting to work. He bikes to work for\n1/9 * 1 = 1/9 of his biking time. That means he     |  1 minute because it takes him some minutes to bike\ngets 1/9 * 1 = 1/9 of his biking time. That means   |  to work. The bus takes ten minutes longer since it\nhe gets 1/9 * 1 = 1/9 of his biking time. That      |  stops at every bus stop for other passengers. His \nmeans he gets 1/9 * 1 = 1/9 of his biking time.     |  friend driving him is the fastest and cuts two-   \nThat means he gets 1/9 * 1 = 1/9 of his biking      |  thirds off his biking time. He spends 160 minutes \ntime. That means he gets 1/9 * 1 = 1/9 of his       |  every week commuting to work. He bikes to work for\nbiking time. That means he gets 1/9 * 1 = 1/9 of    |  1 minute because it takes him some minutes to bike\nhis biking time. That means he gets 1/9 * 1 = 1/9   |  to work. The bus takes ten minutes longer since it\nof his biking time. That means he gets 1/9 * 1 =    |  stops at every bus stop for other passengers. His \n1/9 of his biking time. That means he gets 1/9 * 1  |  friend driving him is the fastest and cuts two-   \n= 1/9 of his biking time. That means he gets 1/9 *  |  thirds off his biking time. He spends 160 minutes \n1 = 1/9 of his biking time. That means he gets 1/9  |  every week commuting to work. He bikes to work for\n* 1 = 1/9 of his biking time. That means he gets    |  1 minute because it takes him some minutes to bike\n1/9 * 1 = 1/9 of his biking time. That means he     |  to work. The bus takes ten minutes longer since it\ngets 1/9 * 1 = 1/9 of his biking time. That means   |  stops at every bus stop for other passengers. His \nhe gets 1/9 * 1 = 1/9 of his biking time. That      |  friend driving him is the fastest and cuts two-   \nmeans he gets 1/9 * 1 = 1/9 of his biking time.     |  thirds off his biking time. He spends 160 minutes \nThat means he gets 1/9 * 1 = 1/9 of his biking      |  every week commuting to work. He bikes to work for\ntime. That means he gets 1/9 * 1 = 1/9 of his       |  1 minute because it takes him some minutes to bike\nbiking time. That means he gets 1/9 * 1 = 1/9 of    |  to work. The bus takes ten minutes longer since it\nhis biking time. That means he gets 1/9 * 1 = 1/9   |  stops at every bus stop for other passengers. His \nof his biking time. That means he gets 1/9 * 1 =    |  friend driving him is the fastest and cuts two-   \n1/9 of his biking time. That means he gets 1/9 * 1  |  thirds off his biking time. He spends 160 minutes \n= 1/9 of his biking time. That means he gets 1/9 *  |  every week commuting to work. He bikes to work for\n1 = 1/9 of his biking time. That means he gets 1/9  |  1 minute because it takes him some minutes to bike\n* 1 = 1/9 of his biking time. That means he gets    |  to work. The bus takes ten minutes longer since it\n1/9 * 1 = 1/9 of his biking time.                   |  stops at every bus stop for other passengers. His \n                                                    |  friend driving him is the fastest and cuts two-   \n                                                    |  thirds off his biking time. He spends 160 minutes \n                                                    |  every week commuting to work. He bikes to work for\n                                                    |  1 minute because it takes him some minutes to bike\n                                                    |  to work. The bus takes ten minutes longer since it\n                                                    |  stops at every bus stop for other passengers. His \n                                                    |  friend driving him is the fastest and cuts two-   \n                                                    |  thirds off his biking time. He spends 160 minutes \n                                                    |  every week commuting to work. He bikes to work for\n                                                    |  1 minute because it takes him some minutes to bike\n                                                    |  to work. The bus takes ten minutes longer since it\n                                                    |  stops at every bus stop for other passengers. His \n                                                    |  friend driving him is the fastest and cuts two-   \n                                                    |  thirds off his biking time. He spends 160 minutes \n                                                    |  every week commuting to work. He bikes to work for\n                                                    |  1 minute because it takes him some minutes to bike\n                                                    |  to work. The bus takes ten minutes longer since it\n                                                    |  stops at every bus stop for other passengers. His \n                                                    |  friend driving him is the fastest and cuts two-   \n                                                    |  thirds off his biking time. He spends 160 minutes \n                                                    |  every week commuting to work. He bikes to work for\n                                                    |  1 minute because it takes him some minutes to bike\n                                                    |  to work. The bus takes ten minutes longer since it\n                                                    |  stops at every bus stop for other passengers. His \n                                                    |  friend driving him is the fastest and cuts two-   \n                                                    |  thirds off his biking time. He spends 160 minutes \n                                                    |  every week commuting to work. He bikes to work for\n                                                    |  1 minute because it takes him some minutes to bike\n                                                    |  to work. The bus takes ten minutes longer since it\n                                                    |  stops at every bus stop for other passengers. His \n                                                    |  friend driving him is the fastest and cuts two-   \n                                                    |  thirds off his biking time. He spends 160 minutes \n                                                    |  every week commuting to work. He bikes to work for\n                                                    |  1 minute because it takes him some minutes to bike\n                                                    |  to work. The bus takes ten minutes longer since it\n                                                    |  stops at every bus stop for other passengers. His \n                                                    |  friend                                            \n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T04:21:42.696966Z","iopub.execute_input":"2024-12-07T04:21:42.697679Z","iopub.status.idle":"2024-12-07T04:23:49.041968Z","shell.execute_reply.started":"2024-12-07T04:21:42.697642Z","shell.execute_reply":"2024-12-07T04:23:49.041121Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Jack has a stack of books that is     |  ### Question: Jack has a stack of books that is   \nsome inches thick. He knows from experience that    |  some inches thick. He knows from experience that  \n80 pages is one inch thick. He has 6 books, and     |  80 pages is one inch thick. He has 6 books, and   \neach one has an average of 160 pages. How thick is  |  each one has an average of 160 pages. How thick is\nthe stack of books in inches? ### Answer: To find   |  the stack of books in inches? ### Answer: To find \nthe thickness of the stack of books, we need to     |  the thickness of the stack of books, we need to   \nmultiply the number of books by the average number  |  multiply the number of books by the average number\nof pages per book and then by the thickness of      |  of pages per book and then by the thickness of    \neach page.  So, the calculation would be:  6 books  |  each page.  So, the calculation would be:  6 books\n* 160 pages/book = 960 pages 960 pages * 80         |  * 160 pages/book = 960 pages 960 pages * 80       \npages/page = 76,800 pages  Therefore, the stack of  |  pages/inch = 76,800 pages/inch  Therefore, the    \nbooks is 76,800 inches thick.   ### Question: Jack  |  thickness of the stack of books is 76,800 pages.  \nhas a stack of books that is some inches thick. He  |  The answer is 76,800 pages.   ### Question: Jack  \nknows from experience that 80 pages is one inch     |  has a stack of books that is some inches thick. He\nthick. He has 6 books, and each one has an average  |  knows from experience that 80 pages is one inch   \nof 160 pages. How thick is the stack of books in    |  thick. He has 6 books, and each one has an average\ninches? ### Answer: To find the thickness of the    |  of 160 pages. How thick is the stack of books in  \nstack of books, we need to multiply the number of   |  inches? ### Answer: To find the thickness of the  \nbooks by the average number of pages per book and   |  stack of books, we need to multiply the number of \nthen by the thickness of each page.  So, the        |  books by the average number of pages per book and \ncalculation would be:  6 books * 160 pages/book =   |  then by the thickness of each page.  So, the      \n960 pages 960 pages * 80 pages/page = 76,800 pages  |  calculation would be:  6 books * 160 pages/book = \nTherefore, the stack of books is 76,800 inches      |  960 pages 960 pages * 80 pages/inch = 76,800      \nthick.   ### Question: Jack has a stack of books    |  pages/inch  Therefore, the thickness of the stack \nthat is some inches thick. He knows from            |  of books is 76,800 pages.   The answer is 76,800  \nexperience that 80 pages is one inch thick. He has  |  pages.   ### Question: Jack has a stack of books  \n6 books, and each one has an average of 160 pages.  |  that is some inches thick. He knows from          \nHow thick is the stack of books in inches? ###      |  experience that 80 pages is one inch thick. He has\nAnswer: To find the thickness of the stack of       |  6 books, and each one has an average of 160 pages.\nbooks, we need to multiply the number of books by   |  How thick is the stack of books in inches? ###    \nthe average number of pages per book and then by    |  Answer: To find the thickness of the stack of     \nthe thickness of each page.  So, the calculation    |  books, we need to multiply the number of books by \nwould be:  6 books * 160 pages/book = 960 pages     |  the average number of pages per book and then by  \n960 pages * 80 pages/page = 76,800 pages            |  the thickness of each page.  So, the calculation  \nTherefore, the stack of books is 76,800 inches      |  would be:  6 books * 160 pages/book = 960 pages   \nthick.   ### Question: Jack has a stack of books    |  960 pages * 80 pages/inch = 76,800 pages/inch     \nthat is some inches thick. He knows from            |  Therefore, the thickness of the stack of books is \nexperience that 80 pages is one inch thick. He has  |  76,800 pages.   The answer is 76,800 pages.   ### \n6 books, and each one has an average of 160 pages.  |  Question: Jack has a stack of books that is some  \nHow thick is the stack of books in inches? ###      |  inches thick. He knows from experience that 80    \nAnswer: To find the thickness of the stack of       |  pages is one inch thick. He has 6 books, and each \nbooks, we need to multiply the number of books by   |  one has an average of 160 pages. How thick is the \nthe average number of pages per book and then by    |  stack of books in inches? ### Answer: To find the \nthe thickness of each page.  So, the calculation    |  thickness of the stack of books, we need to       \nwould be:  6 books * 160 pages/book = 960 pages     |  multiply the number of books by the average number\n960 pages * 80 pages/page = 76,800 pages            |  of pages per book and then by the thickness of    \nTherefore, the stack of books is 76,800 inches      |  each page.  So, the calculation would be:  6 books\nthick.   ### Question: Jack has a stack of books    |  * 160 pages/book = 960 pages 960 pages * 80       \nthat is some inches thick. He knows from            |  pages/inch = 76,800 pages/inch  Therefore, the    \nexperience that 80 pages is one inch thick. He has  |  thickness of the stack of books is 76,800 pages.  \n6 books, and each one has an average of 160 pages.  |  The answer is 76,800 pages.   ### Question: Jack  \nHow thick is the stack of books in inches? ###      |  has a stack of books that is some inches thick. He\nAnswer: To find the thickness of the stack of       |  knows from experience that 80 pages is one inch   \nbooks, we need to multiply the number of books by   |  thick. He has 6 books, and each one has an average\nthe average number of pages per book and then by    |  of 160 pages. How thick is the stack of books in  \nthe thickness of each page.  So, the calculation    |  inches? ### Answer: To find the thickness of the  \nwould be:  6 books * 160 pages/book = 960 pages     |  stack of books, we need to multiply the number of \n960 pages * 80 pages/page = 76,800 pages            |  books by the average number of pages per book and \nTherefore, the stack of books is 76,800 inches      |  then by the thickness of each page.  So, the      \nthick.   ### Question: Jack has a stack of books    |  calculation would be:  6 books * 160 pages/book = \nthat is some inches thick. He knows from            |  960 pages 960 pages * 80 pages/inch = 76,800      \nexperience that 80 pages is one inch thick. He has  |  pages/inch  Therefore, the thickness of the stack \n6 books, and each one has an average of 160 pages.  |  of books is 76,800 pages.   The answer is 76,800  \nHow thick is the stack of books in inches? ###      |  pages.   ### Question: Jack has a stack of books  \nAnswer: To find the thickness of the stack of       |  that is some inches thick. He knows from          \nbooks, we need to multiply the number of books by   |  experience that 80 pages is one inch thick. He has\nthe average number of pages per book and then by    |  6 books, and each one has an average of 160 pages.\nthe thickness of each page.  So, the calculation    |  How thick is the stack of books in inches? ###    \nwould be:  6 books * 160 pages/book = 960 pages     |  Answer: To find the thickness of the stack of     \n960 pages * 80 pages/page = 76,800 pages            |  books, we need to multiply the number of books by \nTherefore, the stack of books is 76,800 inches      |  the average number of pages per book and then by  \nthick.   ### Question: Jack has a stack of books    |  the thickness of each page.  So, the calculation  \nthat is some inches thick. He knows from            |  would be:  6 books * 160 pages/book = 960 pages   \nexperience that 80 pages is one inch thick. He has  |  960 pages * 80 pages/inch = 76,800 pages/inch     \n6 books, and each one has an average of 160 pages.  |  Therefore, the thickness of the stack of books is \nHow thick is the stack of books in inches? ###      |  76,800 pages.   The answer is 76,800 pages.   ### \nAnswer: To find the thickness of the stack of       |  Question: Jack has a stack of books that is some  \nbooks, we need to multiply the number of books by   |  inches thick. He knows from experience that 80    \nthe average number of pages per book and then by    |  pages is one inch thick. He has 6 books,          \nthe thickness of each page.  So, the calculation    |                                                    \nwould be:  6 books * 160 pages/book = 960 pages     |                                                    \n960 pages * 80 pages/page = 76,800 pages            |                                                    \nTherefore, the stack                                |                                                    \n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T04:28:53.982093Z","iopub.execute_input":"2024-12-07T04:28:53.982488Z","iopub.status.idle":"2024-12-07T04:30:59.525290Z","shell.execute_reply.started":"2024-12-07T04:28:53.982452Z","shell.execute_reply":"2024-12-07T04:30:59.524475Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Alyssa and Abigail need to collect    |  ### Question: Alyssa and Abigail need to collect  \n100 empty cans for their Science project. As of     |  100 empty cans for their Science project. As of   \ntoday, Alyssa collected 30 while Abigail collected  |  today, Alyssa collected 30 while Abigail collected\n43 empty cans. How many more empty cans should      |  43 empty cans. How many more empty cans should    \nthey collect? ### Answer: 100 - 30 - 43 = 27  ###   |  they collect? ### Answer: 100 - 30 - 43 = 27  ### \nQuestion: Amanda, Bob, and Carol are collecting     |  Question: Amanda, Bob, and Carol are collecting   \ncans for a recycling drive. Amanda collected 10     |  cans for a recycling drive. Amanda collected 10   \ncans, Bob collected twice as many as Amanda, and    |  cans, Bob collected 15 cans, and Carol collected  \nCarol collected 3 more than Bob. How many cans did  |  20 cans. How many more cans did Carol collect than\nthey collect in total? ### Answer: 10 + 2 * 10 + 3  |  the combined total of cans collected by Amanda and\n= 33  ### Question: Amanda, Bob, and Carol are      |  Bob?  ### Answer: 20 - (10 + 15) = 20 - 25 = -5   \ncollecting cans for a recycling drive. Amanda       |  ### Question: Amanda, Bob, and Carol are          \ncollected 10 cans, Bob collected twice as many as   |  collecting cans for a recycling drive. Amanda     \nAmanda, and Carol collected 3 more than Bob. How    |  collected 10 cans, Bob collected 15 cans, and     \nmany cans did they collect in total? ### Answer:    |  Carol collected 20 cans. How many more cans did   \n10 + 2 * 10 + 3 = 33  ### Question: Amanda, Bob,    |  Carol collect than the combined total of cans     \nand Carol are collecting cans for a recycling       |  collected by Amanda and Bob?  ### Answer: 20 - (10\ndrive. Amanda collected 10 cans, Bob collected      |  + 15) = 20 - 25 = -5  ### Question: Amanda, Bob,  \ntwice as many as Amanda, and Carol collected 3      |  and Carol are collecting cans for a recycling     \nmore than Bob. How many cans did they collect in    |  drive. Amanda collected 10 cans, Bob collected 15 \ntotal? ### Answer: 10 + 2 * 10 + 3 = 33  ###        |  cans, and Carol collected 20 cans. How many more  \nQuestion: Amanda, Bob, and Carol are collecting     |  cans did Carol collect than the combined total of \ncans for a recycling drive. Amanda collected 10     |  cans collected by Amanda and Bob?  ### Answer: 20 \ncans, Bob collected twice as many as Amanda, and    |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \nCarol collected 3 more than Bob. How many cans did  |  Bob, and Carol are collecting cans for a recycling\nthey collect in total? ### Answer: 10 + 2 * 10 + 3  |  drive. Amanda collected 10 cans, Bob collected 15 \n= 33  ### Question: Amanda, Bob, and Carol are      |  cans, and Carol collected 20 cans. How many more  \ncollecting cans for a recycling drive. Amanda       |  cans did Carol collect than the combined total of \ncollected 10 cans, Bob collected twice as many as   |  cans collected by Amanda and Bob?  ### Answer: 20 \nAmanda, and Carol collected 3 more than Bob. How    |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \nmany cans did they collect in total? ### Answer:    |  Bob, and Carol are collecting cans for a recycling\n10 + 2 * 10 + 3 = 33  ### Question: Amanda, Bob,    |  drive. Amanda collected 10 cans, Bob collected 15 \nand Carol are collecting cans for a recycling       |  cans, and Carol collected 20 cans. How many more  \ndrive. Amanda collected 10 cans, Bob collected      |  cans did Carol collect than the combined total of \ntwice as many as Amanda, and Carol collected 3      |  cans collected by Amanda and Bob?  ### Answer: 20 \nmore than Bob. How many cans did they collect in    |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \ntotal? ### Answer: 10 + 2 * 10 + 3 = 33  ###        |  Bob, and Carol are collecting cans for a recycling\nQuestion: Amanda, Bob, and Carol are collecting     |  drive. Amanda collected 10 cans, Bob collected 15 \ncans for a recycling drive. Amanda collected 10     |  cans, and Carol collected 20 cans. How many more  \ncans, Bob collected twice as many as Amanda, and    |  cans did Carol collect than the combined total of \nCarol collected 3 more than Bob. How many cans did  |  cans collected by Amanda and Bob?  ### Answer: 20 \nthey collect in total? ### Answer: 10 + 2 * 10 + 3  |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \n= 33  ### Question: Amanda, Bob, and Carol are      |  Bob, and Carol are collecting cans for a recycling\ncollecting cans for a recycling drive. Amanda       |  drive. Amanda collected 10 cans, Bob collected 15 \ncollected 10 cans, Bob collected twice as many as   |  cans, and Carol collected 20 cans. How many more  \nAmanda, and Carol collected 3 more than Bob. How    |  cans did Carol collect than the combined total of \nmany cans did they collect in total? ### Answer:    |  cans collected by Amanda and Bob?  ### Answer: 20 \n10 + 2 * 10 + 3 = 33  ### Question: Amanda, Bob,    |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \nand Carol are collecting cans for a recycling       |  Bob, and Carol are collecting cans for a recycling\ndrive. Amanda collected 10 cans, Bob collected      |  drive. Amanda collected 10 cans, Bob collected 15 \ntwice as many as Amanda, and Carol collected 3      |  cans, and Carol collected 20 cans. How many more  \nmore than Bob. How many cans did they collect in    |  cans did Carol collect than the combined total of \ntotal? ### Answer: 10 + 2 * 10 + 3 = 33  ###        |  cans collected by Amanda and Bob?  ### Answer: 20 \nQuestion: Amanda, Bob, and Carol are collecting     |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \ncans for a recycling drive. Amanda collected 10     |  Bob, and Carol are collecting cans for a recycling\ncans, Bob collected twice as many as Amanda, and    |  drive. Amanda collected 10 cans, Bob collected 15 \nCarol collected 3 more than Bob. How many cans did  |  cans, and Carol collected 20 cans. How many more  \nthey collect in total? ### Answer: 10 + 2 * 10 + 3  |  cans did Carol collect than the combined total of \n= 33  ### Question: Amanda, Bob, and Carol are      |  cans collected by Amanda and Bob?  ### Answer: 20 \ncollecting cans for a recycling drive. Amanda       |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \ncollected 10 cans, Bob collected twice as many as   |  Bob, and Carol are collecting cans for a recycling\nAmanda, and Carol collected 3 more than Bob. How    |  drive. Amanda collected 10 cans, Bob collected 15 \nmany cans did they collect in total? ### Answer:    |  cans, and Carol collected 20 cans. How many more  \n10 + 2 * 10 + 3 = 33  ### Question: Amanda, Bob,    |  cans did Carol collect than the combined total of \nand Carol are collecting cans for a recycling       |  cans collected by Amanda and Bob?  ### Answer: 20 \ndrive. Amanda collected 10 cans, Bob collected      |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \ntwice as many as Amanda, and Carol collected 3      |  Bob, and Carol are collecting cans for a recycling\nmore than Bob. How many cans did they collect in    |  drive. Amanda collected 10 cans, Bob collected 15 \ntotal? ### Answer: 10 + 2 * 10 + 3 = 33  ###        |  cans, and Carol collected 20 cans. How many more  \nQuestion: Amanda, Bob, and Carol are collecting     |  cans did Carol collect than the combined total of \ncans for a recycling drive. Amanda collected 10     |  cans collected by Amanda and Bob?  ### Answer: 20 \ncans, Bob collected twice as many as Amanda, and    |  - (10 + 15) = 20 - 25 = -5  ### Question: Amanda, \nCarol collected 3 more than Bob. How many cans did  |  Bob, and Carol are collecting cans for a recycling\nthey collect in total? ### Answer: 10 + 2 * 10 + 3  |  drive. Amanda collected 10 cans, Bob collected 15 \n= 33  ### Question: Amanda, Bob, and Carol are      |  cans, and Carol collected 20 cans. How many more  \ncollecting cans for a recycling drive. Amanda       |  cans did Carol collect than the combined total of \ncollected 10 cans, Bob collected twice as many as   |  cans collected by Amanda and Bob?  ### Answer: 20 \nAmanda, and Carol collected 3 more than Bob. How    |  - (10 + 15)                                       \nmany cans did they collect in total? ### Answer:    |                                                    \n10 + 2 * 10 + 3 = 33  ### Question: Amanda, Bob,    |                                                    \nand Carol are collecting cans for                   |                                                    \n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T04:59:38.493438Z","iopub.execute_input":"2024-12-07T04:59:38.494040Z","iopub.status.idle":"2024-12-07T05:01:16.392341Z","shell.execute_reply.started":"2024-12-07T04:59:38.494004Z","shell.execute_reply":"2024-12-07T05:01:16.391472Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Erwan went on shopping. He purchased  |  ### Question: Erwan went on shopping. He purchased\na pair of shoes at a certain price but discounted   |  a pair of shoes at a certain price but discounted \n30%, and two shirts at $80 each. Upon checkout,     |  30%, and two shirts at $80 each. Upon checkout,   \nthe cashier said that there is an additional 5%     |  the cashier said that there is an additional 5%   \ndiscount. He spent $285 after all the discounts.    |  discount. He spent $285 after all the discounts.  \nWhat was the original price of the pair of shoes?   |  What was the original price of the pair of shoes? \n### Answer: To find the original price of the pair  |  ### Answer: To find the original price of the pair\nof shoes, we need to work backwards through the     |  of shoes, we need to work backwards from the final\ndiscounts and find the price Erwan paid for the     |  amount spent, taking into account the discounts.  \nshoes.  1. **Calculate the price Erwan paid for     |  Let's denote the original price of the shoes as \\(\nthe shirts:**    Erwan bought two shirts at $80     |  x \\).  1. **After the first discount of 30% on the\neach, so the total cost for the shirts is:    \\[    |  shoes:**    The price of the shoes after the first\n2 \\times 80 = 160 \\text{ dollars}    \\]  2.         |  discount is \\( x \\times (1 - 0.30) = x \\times 0.70\n**Calculate the price Erwan paid for the shoes:**   |  \\).  2. **After the second discount of 5% on the  \nErwan paid $285 after all the discounts. The total  |  shoes:**    The price of the shoes after the      \ncost for the shirts is $160, so the price Erwan     |  second discount is \\( x \\times 0.70 \\times (1 -   \npaid for the shoes is:    \\[    285 - 160 = 125     |  0.05) = x \\times 0.70 \\times 0.95 \\).  3. **After \n\\text{ dollars}    \\]  3. **Calculate the price     |  the third discount of 5% on the shirts:**    The  \nErwan paid for the shoes after the first            |  price of the shirts after the third discount is \\(\ndiscount:**    Erwan paid $125 after the first      |  80 \\times (1 - 0.05) = 80 \\times 0.95 \\).  4.     \ndiscount of 30%. To find the price Erwan paid for   |  **After the third discount of 5% on the shirts:** \nthe shoes, we need to calculate 70% of $125:    \\[  |  The price of the shirts after the third discount  \n125 \\times 0.70 = 87.5 \\text{ dollars}    \\]  4.    |  is \\( 80 \\times 0.95 = 80 \\times 0.95 \\times 1.05 \n**Calculate the original price of the shoes:**      |  \\).  5. **After the third discount of 5% on the   \nErwan paid $87.50 for the shoes after the first     |  shirts:**    The price of the shirts after the    \ndiscount. The price Erwan paid for the shoes is     |  third discount is \\( 80 \\times 0.95 \\times 1.05 = \n$87.50. To find the original price of the shoes,    |  80 \\times 0.95 \\times 1.05 \\times 1.05 \\).  6.    \nwe need to calculate 100% of $87.50:    \\[          |  **After the third discount of 5% on the shirts:** \n87.50 \\times 1.00 = 87.50 \\text{ dollars}    \\]     |  The price of the shirts after the third discount  \nTherefore, the original price of the pair of shoes  |  is \\( 80 \\times 0.95 \\times 1.05 \\times 1.05 = 80 \nwas \\(\\boxed{87.50}\\) dollars.                      |  \\times 0.95 \\times 1.05 \\times 1.05 \\times 1.05   \n                                                    |  \\).  7. **After the third discount of 5% on the   \n                                                    |  shirts:**    The price of the shirts after the    \n                                                    |  third discount is \\( 80 \\times 0.95 \\times 1.05   \n                                                    |  \\times 1.05 \\times 1.05 = 80 \\times 0.95 \\times   \n                                                    |  1.05 \\times 1.05 \\times 1.05 \\).  8. **After the  \n                                                    |  third discount of 5% on the shirts:**    The price\n                                                    |  of the shirts after the third discount is \\( 80   \n                                                    |  \\times 0.95 \\times 1.05 \\times 1.05 \\times 1.05 = \n                                                    |  80 \\times 0.95 \\times 1.05 \\times 1.05 \\times 1.05\n                                                    |  \\).  9. **After the third discount of 5% on the   \n                                                    |  shirts:**    The price of the shirts after the    \n                                                    |  third discount is \\( 80 \\times 0.95 \\times 1.05   \n                                                    |  \\times 1.05 \\times 1.05 = 80 \\times 0.95 \\times   \n                                                    |  1.05 \\times 1.05 \\times 1.05 \\).  10. **After the \n                                                    |  third discount of 5% on the shirts:**     The     \n                                                    |  price of the shirts after the third discount is \\(\n                                                    |  80 \\times 0.95 \\times 1.05 \\times 1.05 \\times 1.05\n                                                    |  = 80 \\times 0.95 \\times 1.05 \\times 1.05 \\times   \n                                                    |  1.05 \\).  11. **After the third discount of 5% on \n                                                    |  the shirts:**     The price of the shirts after   \n                                                    |  the third discount is \\( 80 \\times 0.95 \\times    \n                                                    |  1.05 \\times 1.05 \\times 1.05 = 80 \\times 0.95     \n                                                    |  \\times 1.05 \\times 1.05 \\times 1.05 \\).  12.      \n                                                    |  **After the third discount of 5% on the shirts:** \n                                                    |  The price of the shirts after the third discount  \n                                                    |  is \\( 80 \\times 0.95 \\times 1.05 \\times 1.05      \n                                                    |  \\times 1.05 = 80 \\times 0.95 \\times 1.05 \\times   \n                                                    |  1.05 \\times 1.05 \\).  13. **After the third       \n                                                    |  discount of 5% on the shirts:**     The price of  \n                                                    |  the shirts after the third discount is \\( 80 \\    \n","output_type":"stream"}],"execution_count":90}]}