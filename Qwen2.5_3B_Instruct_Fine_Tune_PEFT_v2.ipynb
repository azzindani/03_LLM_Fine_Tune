{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"#!pip install --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-22T23:57:15.326788Z","iopub.execute_input":"2024-11-22T23:57:15.327771Z","iopub.status.idle":"2024-11-22T23:57:53.159112Z","shell.execute_reply.started":"2024-11-22T23:57:15.327710Z","shell.execute_reply":"2024-11-22T23:57:53.157835Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-11-22T23:57:53.161636Z","iopub.execute_input":"2024-11-22T23:57:53.162029Z","iopub.status.idle":"2024-11-22T23:58:01.442207Z","shell.execute_reply.started":"2024-11-22T23:57:53.161995Z","shell.execute_reply":"2024-11-22T23:58:01.441165Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-11-22T23:58:01.443687Z","iopub.execute_input":"2024-11-22T23:58:01.444024Z","iopub.status.idle":"2024-11-22T23:58:01.451864Z","shell.execute_reply.started":"2024-11-22T23:58:01.443993Z","shell.execute_reply":"2024-11-22T23:58:01.450623Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n#model_name = url.split('.co/')[-1]\n\nmodel_name = 'Qwen/Qwen2.5-3B-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-22T23:58:01.453099Z","iopub.execute_input":"2024-11-22T23:58:01.453387Z","iopub.status.idle":"2024-11-22T23:58:01.466312Z","shell.execute_reply.started":"2024-11-22T23:58:01.453359Z","shell.execute_reply":"2024-11-22T23:58:01.465522Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-11-22T23:58:01.468476Z","iopub.execute_input":"2024-11-22T23:58:01.468799Z","iopub.status.idle":"2024-11-22T23:58:01.478464Z","shell.execute_reply.started":"2024-11-22T23:58:01.468772Z","shell.execute_reply":"2024-11-22T23:58:01.477495Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:01.479608Z","iopub.execute_input":"2024-11-22T23:58:01.479934Z","iopub.status.idle":"2024-11-22T23:58:10.689571Z","shell.execute_reply.started":"2024-11-22T23:58:01.479906Z","shell.execute_reply":"2024-11-22T23:58:10.688580Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"992d056dfb94493faee49ba288c49109"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 2048)\n    (layers): ModuleList(\n      (0-35): 36 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:10.690735Z","iopub.execute_input":"2024-11-22T23:58:10.691047Z","iopub.status.idle":"2024-11-22T23:58:10.700600Z","shell.execute_reply.started":"2024-11-22T23:58:10.691018Z","shell.execute_reply":"2024-11-22T23:58:10.699562Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1698672640\nTrainable parameters : 311314432\nTrainable percentage: 18.33%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:10.702072Z","iopub.execute_input":"2024-11-22T23:58:10.702817Z","iopub.status.idle":"2024-11-22T23:58:11.173403Z","shell.execute_reply.started":"2024-11-22T23:58:10.702770Z","shell.execute_reply":"2024-11-22T23:58:11.172604Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n#dataset_name = url.split('datasets/')[-1]\n\ndataset_name = 'yahma/alpaca-cleaned'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:11.174700Z","iopub.execute_input":"2024-11-22T23:58:11.175078Z","iopub.status.idle":"2024-11-22T23:58:11.179963Z","shell.execute_reply.started":"2024-11-22T23:58:11.175036Z","shell.execute_reply":"2024-11-22T23:58:11.178866Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 512","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:11.181272Z","iopub.execute_input":"2024-11-22T23:58:11.181716Z","iopub.status.idle":"2024-11-22T23:58:11.193186Z","shell.execute_reply.started":"2024-11-22T23:58:11.181645Z","shell.execute_reply":"2024-11-22T23:58:11.192166Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:11.194784Z","iopub.execute_input":"2024-11-22T23:58:11.195601Z","iopub.status.idle":"2024-11-22T23:58:12.405021Z","shell.execute_reply.started":"2024-11-22T23:58:11.195549Z","shell.execute_reply":"2024-11-22T23:58:12.404014Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['output', 'input', 'instruction'],\n    num_rows: 51760\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.406319Z","iopub.execute_input":"2024-11-22T23:58:12.407073Z","iopub.status.idle":"2024-11-22T23:58:12.414709Z","shell.execute_reply.started":"2024-11-22T23:58:12.407028Z","shell.execute_reply":"2024-11-22T23:58:12.413881Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.415931Z","iopub.execute_input":"2024-11-22T23:58:12.416532Z","iopub.status.idle":"2024-11-22T23:58:12.440792Z","shell.execute_reply.started":"2024-11-22T23:58:12.416499Z","shell.execute_reply":"2024-11-22T23:58:12.439842Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                              output input  \\\n0  1. Eat a balanced and nutritious diet: Make su...         \n1  The three primary colors are red, blue, and ye...         \n2  An atom is the basic building block of all mat...         \n3  There are several ways to reduce air pollution...         \n4  I had to make a difficult decision when I was ...         \n\n                                         instruction  \n0               Give three tips for staying healthy.  \n1                 What are the three primary colors?  \n2                 Describe the structure of an atom.  \n3                   How can we reduce air pollution?  \n4  Pretend you are a project manager of a constru...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>output</th>\n      <th>input</th>\n      <th>instruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n      <td></td>\n      <td>Give three tips for staying healthy.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The three primary colors are red, blue, and ye...</td>\n      <td></td>\n      <td>What are the three primary colors?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>An atom is the basic building block of all mat...</td>\n      <td></td>\n      <td>Describe the structure of an atom.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>There are several ways to reduce air pollution...</td>\n      <td></td>\n      <td>How can we reduce air pollution?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I had to make a difficult decision when I was ...</td>\n      <td></td>\n      <td>Pretend you are a project manager of a constru...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.445242Z","iopub.execute_input":"2024-11-22T23:58:12.445902Z","iopub.status.idle":"2024-11-22T23:58:12.452054Z","shell.execute_reply.started":"2024-11-22T23:58:12.445871Z","shell.execute_reply":"2024-11-22T23:58:12.451117Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n 'input': '',\n 'instruction': 'Give three tips for staying healthy.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.453366Z","iopub.execute_input":"2024-11-22T23:58:12.454313Z","iopub.status.idle":"2024-11-22T23:58:12.462002Z","shell.execute_reply.started":"2024-11-22T23:58:12.454283Z","shell.execute_reply":"2024-11-22T23:58:12.461188Z"}},"outputs":[{"name":"stdout","text":"['output', 'input', 'instruction']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.463385Z","iopub.execute_input":"2024-11-22T23:58:12.464138Z","iopub.status.idle":"2024-11-22T23:58:12.475990Z","shell.execute_reply.started":"2024-11-22T23:58:12.464093Z","shell.execute_reply":"2024-11-22T23:58:12.474898Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  instruction = examples['instruction']\n  input = examples['input']\n  output = examples['output']\n  \n  text = prompt_format.format(instruction, input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.477025Z","iopub.execute_input":"2024-11-22T23:58:12.477287Z","iopub.status.idle":"2024-11-22T23:58:12.487643Z","shell.execute_reply.started":"2024-11-22T23:58:12.477262Z","shell.execute_reply":"2024-11-22T23:58:12.486817Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.488996Z","iopub.execute_input":"2024-11-22T23:58:12.489359Z","iopub.status.idle":"2024-11-22T23:58:12.500952Z","shell.execute_reply.started":"2024-11-22T23:58:12.489321Z","shell.execute_reply":"2024-11-22T23:58:12.500060Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"cZya4tPEWUc9","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.501876Z","iopub.execute_input":"2024-11-22T23:58:12.502172Z","iopub.status.idle":"2024-11-22T23:58:12.509380Z","shell.execute_reply.started":"2024-11-22T23:58:12.502148Z","shell.execute_reply":"2024-11-22T23:58:12.508416Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.510427Z","iopub.execute_input":"2024-11-22T23:58:12.510733Z","iopub.status.idle":"2024-11-22T23:58:12.521602Z","shell.execute_reply.started":"2024-11-22T23:58:12.510693Z","shell.execute_reply":"2024-11-22T23:58:12.520573Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.522650Z","iopub.execute_input":"2024-11-22T23:58:12.522950Z","iopub.status.idle":"2024-11-22T23:58:12.648489Z","shell.execute_reply.started":"2024-11-22T23:58:12.522924Z","shell.execute_reply":"2024-11-22T23:58:12.647575Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.649638Z","iopub.execute_input":"2024-11-22T23:58:12.649963Z","iopub.status.idle":"2024-11-22T23:58:12.659169Z","shell.execute_reply.started":"2024-11-22T23:58:12.649935Z","shell.execute_reply":"2024-11-22T23:58:12.658352Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.660223Z","iopub.execute_input":"2024-11-22T23:58:12.660552Z","iopub.status.idle":"2024-11-22T23:58:12.676586Z","shell.execute_reply.started":"2024-11-22T23:58:12.660526Z","shell.execute_reply":"2024-11-22T23:58:12.675739Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.677749Z","iopub.execute_input":"2024-11-22T23:58:12.678027Z","iopub.status.idle":"2024-11-22T23:58:12.689404Z","shell.execute_reply.started":"2024-11-22T23:58:12.678000Z","shell.execute_reply":"2024-11-22T23:58:12.688378Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.690574Z","iopub.execute_input":"2024-11-22T23:58:12.690897Z","iopub.status.idle":"2024-11-22T23:58:12.722282Z","shell.execute_reply.started":"2024-11-22T23:58:12.690870Z","shell.execute_reply":"2024-11-22T23:58:12.721497Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Below is an instruction that describes a task,...   \n1  Below is an instruction that describes a task,...   \n2  Below is an instruction that describes a task,...   \n3  Below is an instruction that describes a task,...   \n4  Below is an instruction that describes a task,...   \n\n                                           input_ids  \\\n0  [38214, 374, 458, 7600, 429, 16555, 264, 3383,...   \n1  [38214, 374, 458, 7600, 429, 16555, 264, 3383,...   \n2  [38214, 374, 458, 7600, 429, 16555, 264, 3383,...   \n3  [38214, 374, 458, 7600, 429, 16555, 264, 3383,...   \n4  [38214, 374, 458, 7600, 429, 16555, 264, 3383,...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[38214, 374, 458, 7600, 429, 16555, 264, 3383,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[38214, 374, 458, 7600, 429, 16555, 264, 3383,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[38214, 374, 458, 7600, 429, 16555, 264, 3383,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[38214, 374, 458, 7600, 429, 16555, 264, 3383,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[38214, 374, 458, 7600, 429, 16555, 264, 3383,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.723251Z","iopub.execute_input":"2024-11-22T23:58:12.723524Z","iopub.status.idle":"2024-11-22T23:58:12.728832Z","shell.execute_reply.started":"2024-11-22T23:58:12.723499Z","shell.execute_reply":"2024-11-22T23:58:12.727906Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the given poem and identify its main theme.\n\n### Input:\nTwo roads diverged in a yellow wood,\\nAnd sorry I could not travel both\\nAnd be one traveler, long I stood\\nAnd looked down one as far as I could\\nTo where it bent in the undergrowth;\\n\\nThen took the other, as just as fair,\\nAnd having perhaps the better claim,\\nBecause it was grassy and wanted wear;\\nThough as for that the passing there\\nHad worn them really about the same,\\nThe roads that morning equally lay\\nIn leaves no step had trodden black.\\nOh, I left the first for another day!\\nYet knowing how way leads on to way,\\nI doubted if I should ever come back.\\n\\nI shall be telling this with a sigh\\nSomewhere ages and ages hence:\\nTwo roads diverged in a wood, and I—\\nI took the one less traveled by,\\nAnd that has made all the difference.\n\n### Response:\nThe main theme of the poem is the importance of making choices and the impact of those choices on one's life. The speaker is faced with a decision between two paths and ultimately chooses the one less traveled, which ultimately shapes their life experience.<|im_end|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.730106Z","iopub.execute_input":"2024-11-22T23:58:12.730492Z","iopub.status.idle":"2024-11-22T23:58:12.741321Z","shell.execute_reply.started":"2024-11-22T23:58:12.730450Z","shell.execute_reply":"2024-11-22T23:58:12.740449Z"}},"outputs":[{"name":"stdout","text":"[38214, 374, 458, 7600, 429, 16555, 264, 3383, 11, 34426, 448, 458, 1946, 429, 5707, 4623, 2266, 13, 9645, 264, 2033, 429, 34901, 44595, 279, 1681, 382, 14374, 29051, 510, 2082, 55856, 279, 2661, 32794, 323, 10542, 1181, 1887, 6912, 382, 14374, 5571, 510, 11613, 19241, 36341, 3556, 304, 264, 13753, 7579, 26266, 77, 3036, 14589, 358, 1410, 537, 5821, 2176, 1699, 3036, 387, 825, 62765, 11, 1293, 358, 14638, 1699, 3036, 6966, 1495, 825, 438, 3041, 438, 358, 1410, 1699, 1249, 1380, 432, 29180, 304, 279, 1212, 73089, 17882, 77, 1699, 12209, 3867, 279, 1008, 11, 438, 1101, 438, 6624, 26266, 77, 3036, 3432, 8365, 279, 2664, 3717, 26266, 77, 17949, 432, 572, 16359, 88, 323, 4829, 9850, 17882, 77, 26733, 438, 369, 429, 279, 12299, 1052, 1699, 55468, 23704, 1105, 2167, 911, 279, 1852, 26266, 88230, 19241, 429, 6556, 18308, 10962, 1699, 641, 10901, 902, 3019, 1030, 8185, 10792, 3691, 7110, 77, 11908, 11, 358, 2115, 279, 1156, 369, 2441, 1899, 14771, 77, 28074, 14063, 1246, 1616, 11508, 389, 311, 1616, 26266, 77, 40, 92662, 421, 358, 1265, 3512, 2525, 1182, 7110, 77, 1699, 40, 4880, 387, 11629, 419, 448, 264, 30138, 1699, 49882, 60652, 16639, 323, 16639, 16085, 7190, 77, 11613, 19241, 36341, 3556, 304, 264, 7579, 11, 323, 358, 2293, 59, 77, 40, 3867, 279, 825, 2686, 30696, 553, 26266, 77, 3036, 429, 702, 1865, 678, 279, 6672, 382, 14374, 5949, 510, 785, 1887, 6912, 315, 279, 32794, 374, 279, 12650, 315, 3259, 11454, 323, 279, 5421, 315, 1846, 11454, 389, 825, 594, 2272, 13, 576, 18601, 374, 16601, 448, 264, 5480, 1948, 1378, 12716, 323, 13653, 39911, 279, 825, 2686, 30696, 11, 892, 13653, 20816, 862, 2272, 3139, 13, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.742289Z","iopub.execute_input":"2024-11-22T23:58:12.742601Z","iopub.status.idle":"2024-11-22T23:58:12.751944Z","shell.execute_reply.started":"2024-11-22T23:58:12.742573Z","shell.execute_reply":"2024-11-22T23:58:12.751114Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.752947Z","iopub.execute_input":"2024-11-22T23:58:12.753243Z","iopub.status.idle":"2024-11-22T23:58:12.760470Z","shell.execute_reply.started":"2024-11-22T23:58:12.753217Z","shell.execute_reply":"2024-11-22T23:58:12.759740Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.761479Z","iopub.execute_input":"2024-11-22T23:58:12.761765Z","iopub.status.idle":"2024-11-22T23:58:12.770586Z","shell.execute_reply.started":"2024-11-22T23:58:12.761735Z","shell.execute_reply":"2024-11-22T23:58:12.769887Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.771767Z","iopub.execute_input":"2024-11-22T23:58:12.772088Z","iopub.status.idle":"2024-11-22T23:58:12.779887Z","shell.execute_reply.started":"2024-11-22T23:58:12.772051Z","shell.execute_reply":"2024-11-22T23:58:12.779138Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.780942Z","iopub.execute_input":"2024-11-22T23:58:12.781273Z","iopub.status.idle":"2024-11-22T23:58:12.789208Z","shell.execute_reply.started":"2024-11-22T23:58:12.781245Z","shell.execute_reply":"2024-11-22T23:58:12.788297Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:12.790394Z","iopub.execute_input":"2024-11-22T23:58:12.790817Z","iopub.status.idle":"2024-11-22T23:58:14.457638Z","shell.execute_reply.started":"2024-11-22T23:58:12.790777Z","shell.execute_reply":"2024-11-22T23:58:14.456600Z"}},"outputs":[{"name":"stdout","text":"trainable params: 119,734,272 || all params: 3,205,672,960 || trainable%: 3.7351\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"model","metadata":{"id":"ikF6Yfkz1myd","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:14.458918Z","iopub.execute_input":"2024-11-22T23:58:14.459303Z","iopub.status.idle":"2024-11-22T23:58:14.476874Z","shell.execute_reply.started":"2024-11-22T23:58:14.459261Z","shell.execute_reply":"2024-11-22T23:58:14.475838Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 2048)\n    (layers): ModuleList(\n      (0-35): 36 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=256, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=256, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=11008, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=11008, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=11008, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:14.478106Z","iopub.execute_input":"2024-11-22T23:58:14.478464Z","iopub.status.idle":"2024-11-22T23:58:14.502022Z","shell.execute_reply.started":"2024-11-22T23:58:14.478433Z","shell.execute_reply":"2024-11-22T23:58:14.500881Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1818406912\nTrainable parameters : 119734272\nTrainable percentage: 6.58%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:14.503032Z","iopub.execute_input":"2024-11-22T23:58:14.503320Z","iopub.status.idle":"2024-11-22T23:58:14.510862Z","shell.execute_reply.started":"2024-11-22T23:58:14.503290Z","shell.execute_reply":"2024-11-22T23:58:14.509776Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 1\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:14.512120Z","iopub.execute_input":"2024-11-22T23:58:14.512426Z","iopub.status.idle":"2024-11-22T23:58:14.554394Z","shell.execute_reply.started":"2024-11-22T23:58:14.512395Z","shell.execute_reply":"2024-11-22T23:58:14.553407Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Nov22_23-58-14_41736bf96ed5,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:14.555572Z","iopub.execute_input":"2024-11-22T23:58:14.555907Z","iopub.status.idle":"2024-11-22T23:58:16.901900Z","shell.execute_reply.started":"2024-11-22T23:58:14.555874Z","shell.execute_reply":"2024-11-22T23:58:16.900938Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7eb574dd64a0>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T23:58:16.902981Z","iopub.execute_input":"2024-11-22T23:58:16.903284Z","iopub.status.idle":"2024-11-23T00:41:24.144537Z","shell.execute_reply.started":"2024-11-22T23:58:16.903253Z","shell.execute_reply":"2024-11-23T00:41:24.143719Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 1\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 119,734,272\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111303508888947, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d05f8c6de3443cbc9fdafac215e938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241122_235818-hejaghkd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/hejaghkd' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/hejaghkd' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/hejaghkd</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 42:55, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.993700</td>\n      <td>1.496681</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.338500</td>\n      <td>1.196130</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.153200</td>\n      <td>1.113486</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.094100</td>\n      <td>1.073941</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.011100</td>\n      <td>1.048630</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.002700</td>\n      <td>1.031544</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.007100</td>\n      <td>1.024387</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.963600</td>\n      <td>1.022241</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.006000</td>\n      <td>1.021526</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.963600</td>\n      <td>1.021452</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.1533556938171388, metrics={'train_runtime': 2586.755, 'train_samples_per_second': 0.309, 'train_steps_per_second': 0.077, 'total_flos': 7407801886310400.0, 'train_loss': 1.1533556938171388, 'epoch': 0.08888888888888889})"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:41:24.145715Z","iopub.execute_input":"2024-11-23T00:41:24.146018Z","iopub.status.idle":"2024-11-23T00:43:29.335057Z","shell.execute_reply.started":"2024-11-23T00:41:24.145989Z","shell.execute_reply":"2024-11-23T00:43:29.334072Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 02:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 1.0214524269104004, 'eval_runtime': 125.1746, 'eval_samples_per_second': 1.598, 'eval_steps_per_second': 0.399, 'epoch': 0.08888888888888889}\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:29.344315Z","iopub.execute_input":"2024-11-23T00:43:29.344694Z","iopub.status.idle":"2024-11-23T00:43:31.747440Z","shell.execute_reply.started":"2024-11-23T00:43:29.344643Z","shell.execute_reply":"2024-11-23T00:43:31.746584Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:31.748611Z","iopub.execute_input":"2024-11-23T00:43:31.748958Z","iopub.status.idle":"2024-11-23T00:43:32.107341Z","shell.execute_reply.started":"2024-11-23T00:43:31.748911Z","shell.execute_reply":"2024-11-23T00:43:32.106254Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:32.108680Z","iopub.execute_input":"2024-11-23T00:43:32.109087Z","iopub.status.idle":"2024-11-23T00:43:32.124266Z","shell.execute_reply.started":"2024-11-23T00:43:32.109032Z","shell.execute_reply":"2024-11-23T00:43:32.123399Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:32.125358Z","iopub.execute_input":"2024-11-23T00:43:32.125633Z","iopub.status.idle":"2024-11-23T00:43:34.234064Z","shell.execute_reply.started":"2024-11-23T00:43:32.125606Z","shell.execute_reply":"2024-11-23T00:43:34.233196Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:34.235262Z","iopub.execute_input":"2024-11-23T00:43:34.235548Z","iopub.status.idle":"2024-11-23T00:43:43.163315Z","shell.execute_reply.started":"2024-11-23T00:43:34.235521Z","shell.execute_reply":"2024-11-23T00:43:43.162264Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-3B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors.index.json\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bab3bcc4033437fb5b315619b32b675"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-3B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.05,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 2048)\n    (layers): ModuleList(\n      (0-35): 36 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:43.164679Z","iopub.execute_input":"2024-11-23T00:43:43.164998Z","iopub.status.idle":"2024-11-23T00:43:43.177798Z","shell.execute_reply.started":"2024-11-23T00:43:43.164969Z","shell.execute_reply":"2024-11-23T00:43:43.176808Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1698672640\nTrainable parameters : 311314432\nTrainable percentage: 18.33%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:43.179290Z","iopub.execute_input":"2024-11-23T00:43:43.179722Z","iopub.status.idle":"2024-11-23T00:43:43.210596Z","shell.execute_reply.started":"2024-11-23T00:43:43.179647Z","shell.execute_reply":"2024-11-23T00:43:43.209363Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 2048)\n        (layers): ModuleList(\n          (0-35): 36 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=11008, bias=False)\n                  (default): Linear(in_features=64, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=11008, bias=False)\n                  (default): Linear(in_features=64, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=11008, out_features=64, bias=False)\n                  (default): Linear(in_features=11008, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T00:43:43.212002Z","iopub.execute_input":"2024-11-23T00:43:43.212808Z","iopub.status.idle":"2024-11-23T00:43:43.245572Z","shell.execute_reply.started":"2024-11-23T00:43:43.212763Z","shell.execute_reply":"2024-11-23T00:43:43.244554Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1938141184\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## 13 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt, inputs):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      inputs,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:01:12.195239Z","iopub.execute_input":"2024-11-23T01:01:12.195613Z","iopub.status.idle":"2024-11-23T01:01:12.202898Z","shell.execute_reply.started":"2024-11-23T01:01:12.195580Z","shell.execute_reply":"2024-11-23T01:01:12.201988Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def post_assistant(prompt, inputs):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      inputs,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:01:14.771147Z","iopub.execute_input":"2024-11-23T01:01:14.772070Z","iopub.status.idle":"2024-11-23T01:01:14.779655Z","shell.execute_reply.started":"2024-11-23T01:01:14.772018Z","shell.execute_reply":"2024-11-23T01:01:14.778520Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:01:17.989553Z","iopub.execute_input":"2024-11-23T01:01:17.989983Z","iopub.status.idle":"2024-11-23T01:01:17.997607Z","shell.execute_reply.started":"2024-11-23T01:01:17.989950Z","shell.execute_reply":"2024-11-23T01:01:17.996581Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:05:57.724409Z","iopub.execute_input":"2024-11-23T01:05:57.724836Z","iopub.status.idle":"2024-11-23T01:09:52.579914Z","shell.execute_reply.started":"2024-11-23T01:05:57.724804Z","shell.execute_reply":"2024-11-23T01:09:52.578601Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Think of   |  completes the request.  ### Instruction: Think of \nan example of a moral dilemma and explain why it    |  an example of a moral dilemma and explain why it  \nis difficult to make a decision.  ### Input:   ###  |  is difficult to make a decision.  ### Input:   ###\nResponse: Certainly! Let's consider the moral       |  Response: Certainly! Let's consider the moral     \ndilemma of euthanasia. Euthanasia is the act of     |  dilemma of euthanasia. Euthanasia is the act of   \nintentionally ending a person's life to relieve     |  intentionally ending a person's life to relieve   \nthem from suffering. This can be particularly       |  them from suffering. This can be particularly     \nchallenging in cases where a terminally ill         |  challenging in cases where a terminally ill       \npatient is experiencing unbearable pain and has a   |  patient is experiencing unbearable pain and there \nvery short time left to live.  The difficulty in    |  is no hope for a cure.  The difficulty in making a\nmaking a decision about euthanasia stems from the   |  decision about euthanasia stems from the complex  \ncomplex interplay of ethical, legal, and personal   |  interplay of ethical, legal, and personal         \nconsiderations. On one hand, the patient's          |  considerations. On one hand, the patient's        \nsuffering and the desire to alleviate it can be     |  suffering and the desire to alleviate it can be   \ncompelling. On the other hand, there are concerns   |  compelling. On the other hand, the principle of   \nabout the potential for abuse, the slippery slope   |  \"do no harm\" and the potential for abuse of the   \nof making euthanasia more accessible, and the       |  system are significant concerns.   Legally,       \nethical implications of ending a life.  Legally,    |  euthanasia is often illegal in many countries, and\nmany countries have strict regulations around       |  the practice can be heavily regulated or even     \neuthanasia, making it difficult to implement.       |  criminalized. This makes it difficult to provide a\nEthically, there are debates about the sanctity of  |  clear and safe path for those in need. Ethically, \nlife and the moral responsibility to help someone   |  the decision to end a life raises profound        \nin extreme pain. Personal beliefs also play a       |  questions about the value of human life, the      \nsignificant role, as some people may find it        |  sanctity of death, and the limits of medical      \nmorally acceptable to end a life to relieve         |  intervention.  Moreover, the personal aspect of   \nsuffering, while others believe that all life,      |  the dilemma can be deeply personal and subjective.\neven in its final moments, deserves respect and     |  Each patient's situation is unique, and the       \ndignity.  In summary, the moral dilemma of          |  decision to allow or prevent euthanasia can be    \neuthanasia is difficult to resolve because it       |  influenced by a wide range of factors, including  \ntouches on multiple aspects of human life,          |  cultural beliefs, religious convictions, and      \nincluding suffering, autonomy, and the sanctity of  |  individual circumstances.  In summary, the moral  \nlife, making it a complex and emotionally charged   |  dilemma of euthanasia is difficult to resolve     \nissue.  The decision is further complicated by      |  because it involves balancing the patient's right \nlegal and ethical frameworks that can limit or      |  to relief from suffering against the potential for\nrestrict certain actions.  Ultimately, the          |  harm, legal restrictions, and personal beliefs.   \ndifficulty lies in balancing these various factors  |  The complexity of these factors makes it a        \nto arrive at a decision that is both just and       |  challenging decision for both healthcare providers\ncompassionate.  This is why it is a challenging     |  and patients.  Response: Certainly! Let's consider\nmoral dilemma.  To make a decision, one must        |  the moral dilemma of euthanasia. Euthanasia is the\ncarefully weigh these various factors and consider  |  act of intentionally ending a person's life to    \nthe potential consequences of their actions.  It    |  relieve them from suffering. This can be          \nis a decision that requires deep reflection and     |  particularly challenging in cases where a         \ncareful consideration.  It is a decision that can   |  terminally ill patient is experiencing unbearable \nhave far-reaching implications for both the         |  pain and there is no hope for a cure.  The        \nindividual and society at large.  It is a decision  |  difficulty in making a decision about euthanasia  \nthat requires a nuanced understanding of the        |  stems from the complex interplay of ethical,      \nethical, legal, and personal dimensions of the      |  legal, and personal considerations. On one hand,  \nissue.  It is a decision that requires a balance    |  the patient's suffering and the desire to         \nbetween compassion and justice.  It is a decision   |  alleviate it can be compelling. On the other hand,\nthat requires a deep understanding of the           |  the principle of \"do no harm\" and the potential   \ncomplexities of human suffering and the ethical     |  for abuse of the system are significant concerns. \nand legal frameworks that govern it.  It is a       |  Legally, euthanasia is often illegal in many      \ndecision that requires a careful consideration of   |  countries, and the practice can be heavily        \nthe potential consequences of one's actions.  It    |  regulated or even criminalized. This makes it     \nis a decision that requires a balance between       |  difficult to provide a clear and safe path for    \ncompassion and justice.  It is a decision that      |  those in need. Ethically, the decision to end a   \nrequires a deep understanding of the complexities   |  life raises profound questions about the value of \nof human suffering and the ethical and legal        |  human life, the sanctity of death, and the limits \nframeworks that govern it.  It is a decision that   |  of medical intervention.  Moreover, the personal  \nrequires a careful consideration of the potential   |  aspect of the dilemma can be deeply personal and  \nconsequences of one's actions.  It is a decision    |  subjective. Each patient's situation is unique,   \nthat requires a balance between compassion and      |  and the decision to allow or prevent euthanasia   \njustice.  It is a decision that requires a deep     |  can be influenced by a wide range forget cultural \nunderstanding of the complexities of human          |  beliefs, religious convictions, and individual    \nsuffering and the ethical and legal frameworks      |  circumstances.  In summary, the moral dilemma of  \nthat govern it.  It is a decision that requires a   |  euthanasia is difficult to resolve because it     \ncareful consideration of the potential              |  involves balancing the patient's right to relief  \nconsequences of one's actions.  It is a decision    |  from suffering against the potential for harm,    \nthat requires a balance between compassion and      |  legal restrictions, and personal beliefs. The     \njustice.  It is a decision that requires a deep     |  complexity of these factors makes it a challenging\nunderstanding of the complexities of human          |  decision for both healthcare providers and        \nsuffering and the ethical and legal frameworks      |  patients.  Response: Certainly! Let's consider the\nthat govern it.  It is a decision that requires a   |  moral dilemma of euthanasia. Euthanasia is the act\ncareful consideration of the potential              |  of intentionally ending a person's life to relieve\nconsequences of one's actions.  It is a decision    |  them from suffering. This can be particularly     \nthat requires a balance between compassion and      |  challenging in cases where a terminally ill       \njustice.  It is a decision that requires a deep     |  patient is experiencing unbearable pain and there \nunderstanding of the complexities of human          |  is no hope for a cure.  The difficulty in making a\nsuffering and the ethical and legal frameworks      |  decision about euthanasia stems from the complex  \nthat govern it.  It is a decision that requires a   |  interplay of ethical, legal, and personal         \ncareful consideration of the potential              |  considerations. On one hand, the patient's        \nconsequences of one's actions.  It is a decision    |  suffering and the desire to alleviate it can be   \nthat requires a balance between compassion and      |  compelling. On the other hand, the principle of   \njustice.  It is a decision that requires a deep     |  \"do no harm\" and the potential for abuse of the   \nunderstanding of the complexities of human          |  system are significant concerns.  Legally,        \nsuffering and the ethical and legal frameworks      |  euthanasia is often illegal in many countries, and\nthat govern it.  It is a decision that requires a   |  the practice can be heavily regulated or even     \ncareful consideration of the potential              |  criminalized. This makes it difficult to provide a\nconsequences of one's actions.  It is a decision    |  clear and safe path for those in need. Ethically, \nthat requires a balance between compassion and      |  the decision to end a life raises profound        \njustice.  It is a decision that requires a deep     |  questions about the value of human life, the      \nunderstanding of the complexities of human          |  sanctity of death, and the limits of medical      \nsuffering and the ethical and legal frameworks      |  intervention.  Moreover, the personal aspect of   \nthat govern it.  It is a decision that requires a   |  the dilemma can be deeply personal and subjective.\ncareful consideration of the potential              |  Each patient's situation is unique, and the       \nconsequences of one's actions.  It is a decision    |  decision to allow or prevent euthanasia can be    \nthat requires a balance between compassion and      |  influenced by a wide range of factors, including  \njustice.  It is a decision that requires a deep     |  cultural beliefs, religious convictions, and      \nunderstanding of the complexities of human          |  individual circumstances.  In summary, the moral  \nsuffering and the ethical and legal frameworks      |  dilemma of euthanasia is difficult to resolve     \nthat govern it.  It is a decision that requires a   |  because it involves balancing the patient's right \ncareful consideration of the potential              |  to relief from suffering against the potential for\nconsequences of one's actions.  It is a decision    |  harm, legal restrictions, and personal beliefs.   \nthat requires a balance between compassion and      |  The complexity of these factors makes it a        \njustice.  It is a decision that requires a deep     |  challenging decision for both healthcare providers\nunderstanding of the complexities of human          |  and patients.  Response: Certainly! Let's consider\nsuffering and the ethical and legal frameworks      |  the moral dilemma of euthanasia. Euthanasia is the\nthat govern it.  It is a decision that requires a   |  act of intentionally ending a person's life to    \ncareful consideration of the potential              |  relieve them from suffering. This can be          \nconsequences of one's actions.  It is a decision    |  particularly challenging in cases where a         \nthat requires a balance between compassion and      |  terminally ill patient is experiencing unbearable \njustice.  It is a decision that requires a deep     |  pain and there is no hope for a cure.  The        \nunderstanding of the complexities of human          |  difficulty in making a decision about euthanasia  \nsuffering and the ethical and legal frameworks      |  stems from the complex interplay of ethical,      \nthat govern it.  It is a decision that requires a   |  legal, and personal considerations. On one hand,  \ncareful consideration of the potential              |  the patient                                       \nconsequences of one's actions.  It is a decision    |                                                    \nthat requires a balance between compassion and      |                                                    \njustice.  It is a decision that requires a deep     |                                                    \nunderstanding of the complexities of human          |                                                    \nsuffering and the ethical and legal frameworks      |                                                    \nthat                                                |                                                    \n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:28:25.720730Z","iopub.execute_input":"2024-11-23T01:28:25.721482Z","iopub.status.idle":"2024-11-23T01:32:31.909740Z","shell.execute_reply.started":"2024-11-23T01:28:25.721444Z","shell.execute_reply":"2024-11-23T01:32:31.907546Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Rewrite    |  completes the request.  ### Instruction: Rewrite  \nthese sentences using different words  ### Input:   |  these sentences using different words  ### Input: \nThis tech device will make your life more           |  This tech device will make your life more         \nefficient and enjoyable.  ### Response: This        |  efficient and enjoyable.  ### Response: This      \ntechnological gadget will enhance your life and     |  technological gadget will enhance your life and   \nbring more pleasure. To complete the task as        |  bring more pleasure. To complete the task as      \nrequested, I have rewritten the input sentence      |  requested, I have rewritten the input sentence    \nusing different words as follows:  This             |  using different words as follows:  This           \ntechnological gadget will enhance your life and     |  technological gadget will enhance your life and   \nbring more pleasure.   This response uses synonyms  |  bring more pleasure.   This response uses synonyms\nand alternative phrasing to convey the same         |  and alternative phrasing to convey the same       \nmeaning as the original sentence. The key ideas of  |  meaning as the original sentence. The key ideas of\nmaking life more efficient and enjoyable are        |  making life more efficient and enjoyable are      \npreserved, but the wording is altered to use        |  preserved, but the wording is altered to use      \ndifferent words.  The phrase \"make your life more   |  different words.  The phrase \"make your life more \nefficient\" is replaced with \"enhance your life,\"    |  efficient\" is replaced with \"enhance your life,\"  \nand \"enjoyable\" is changed to \"pleasure\" to         |  and \"enjoyable\" is changed to \"pleasure\" to       \nmaintain the overall sentiment.  The sentence       |  maintain the overall sentiment.  The sentence     \nstructure is also slightly modified to ensure it    |  structure is also slightly modified to ensure it  \nreads naturally.  This approach ensures that the    |  reads naturally.  This approach ensures that the  \nmeaning remains intact while using distinct         |  meaning remains intact while using distinct       \nlanguage.  The response adheres to the instruction  |  language.  The response adheres to the instruction\nof using different words.  The sentence is          |  of using different words.  The sentence is        \nrephrased to maintain the original meaning while    |  rephrased to maintain the original meaning while  \nusing different words.  The core message of the     |  using different words.  The core message about the\noriginal sentence is preserved.  The response uses  |  device making life more efficient and enjoyable is\nsynonyms and alternative phrasing to convey the     |  preserved.  The response uses synonyms and        \nsame meaning as the original sentence.  The key     |  alternative phrasing to convey the same meaning as\nideas of making life more efficient and enjoyable   |  the original sentence.  The key ideas of making   \nare preserved, but the wording is altered to use    |  life more efficient and enjoyable are preserved,  \ndifferent words.  The phrase \"make your life more   |  but the wording is altered to use different words.\nefficient\" is replaced with \"enhance your life,\"    |  The phrase \"make your life more efficient\" is     \nand \"enjoyable\" is changed to \"pleasure\" to         |  replaced with \"enhance your life,\" and \"enjoyable\"\nmaintain the overall sentiment.  The sentence       |  is changed to \"pleasure\" to maintain the overall  \nstructure is also slightly modified to ensure it    |  sentiment.  The sentence structure is also        \nreads naturally.  This approach ensures that the    |  slightly modified to ensure it reads naturally.   \nmeaning remains intact while using distinct         |  This approach ensures that the meaning remains    \nlanguage.  The response adheres to the instruction  |  intact while using distinct language.  The        \nof using different words.  The sentence is          |  response adheres to the instruction of using      \nrephrased to maintain the original meaning while    |  different words.  The sentence is rephrased to    \nusing different words.  The core message of the     |  maintain the original meaning while using         \noriginal sentence is preserved.  The response uses  |  different words.  The core message about the      \nsynonyms and alternative phrasing to convey the     |  device making life more efficient and enjoyable is\nsame meaning as the original sentence.  The key     |  preserved.  The response uses synonyms and        \nideas of making life more efficient and enjoyable   |  alternative phrasing to convey the same meaning as\nare preserved, but the wording is altered to use    |  the original sentence.  The key ideas of making   \ndifferent words.  The phrase \"make your life more   |  life more efficient and enjoyable are preserved,  \nefficient\" is replaced with \"enhance your life,\"    |  but the wording is altered to use different words.\nand \"enjoyable\" is changed to \"pleasure\" to         |  The phrase \"make your life more efficient\" is     \nmaintain the overall sentiment.  The sentence       |  replaced with \"enhance your life,\" and \"enjoyable\"\nstructure is also slightly modified to ensure it    |  is changed to \"pleasure\" to maintain the overall  \nreads naturally.  This approach ensures that the    |  sentiment.  The sentence structure is also        \nmeaning remains intact while using distinct         |  slightly modified to ensure it reads naturally.   \nlanguage.  The response adheres to the instruction  |  This approach ensures that the meaning remains    \nof using different words.  The sentence is          |  intact while using distinct language.  The        \nrephrased to maintain the original meaning while    |  response adheres to the instruction of using      \nusing different words.  The core message of the     |  different words.  The sentence is rephrased to    \noriginal sentence is preserved.  The response uses  |  maintain the original meaning while using         \nsynonyms and alternative phrasing to convey the     |  different words.  The core message about the      \nsame meaning as the original sentence.  The key     |  device making life more efficient and enjoyable is\nideas of making life more efficient and enjoyable   |  preserved.  The response uses synonyms and        \nare preserved, but the wording is altered to use    |  alternative phrasing to convey the same meaning as\ndifferent words.  The phrase \"make your life more   |  the original sentence.  The key ideas of making   \nefficient\" is replaced with \"enhance your life,\"    |  life more efficient and enjoyable are preserved,  \nand \"enjoyable\" is changed to \"pleasure\" to         |  but the wording is altered to use different words.\nmaintain the overall sentiment.  The sentence       |  The phrase \"make your life more efficient\" is     \nstructure is also slightly modified to ensure it    |  replaced with \"enhance your life,\" and \"enjoyable\"\nreads naturally.  This approach ensures that the    |  is changed to \"pleasure\" to maintain the overall  \nmeaning remains intact while using distinct         |  sentiment.  The sentence structure is also        \nlanguage.  The response adheres to the instruction  |  slightly modified to ensure it reads naturally.   \nof using different words.  The sentence is          |  This approach ensures that the meaning remains    \nrephrased to maintain the original meaning while    |  intact while using distinct language.  The        \nusing different words.  The core message of the     |  response adheres to the instruction of using      \noriginal sentence is preserved.  The response uses  |  different words.  The sentence is rephrased to    \nsynonyms and alternative phrasing to convey the     |  maintain the original meaning while using         \nsame meaning as the original sentence.  The key     |  different words.  The core message about the      \nideas of making life more efficient and enjoyable   |  device making life more efficient and enjoyable is\nare preserved, but the wording is altered to use    |  preserved.  The response uses synonyms and        \ndifferent words.  The phrase \"make your life more   |  alternative phrasing to convey the same meaning as\nefficient\" is replaced with \"enhance your life,\"    |  the original sentence.  The key ideas of making   \nand \"enjoyable\" is changed to \"pleasure\" to         |  life more efficient and enjoyable are preserved,  \nmaintain the overall sentiment.  The sentence       |  but the wording is altered to use different words.\nstructure is also slightly modified to ensure it    |  The phrase \"make your life more efficient\" is     \nreads naturally.  This approach ensures that the    |  replaced with \"enhance your life,\" and \"enjoyable\"\nmeaning remains intact while using distinct         |  is changed to \"pleasure\" to maintain the overall  \nlanguage.  The response adheres to the instruction  |  sentiment.  The sentence structure is also        \nof using different words.  The sentence is          |  slightly modified to ensure it reads naturally.   \nrephrased to maintain the original meaning while    |  This approach ensures that the meaning remains    \nusing different words.  The core message of the     |  intact while using distinct language.  The        \noriginal sentence is preserved.  The response uses  |  response adheres to the instruction of using      \nsynonyms and alternative phrasing to convey the     |  different words.  The sentence is rephrased to    \nsame meaning as the original sentence.  The key     |  maintain the original meaning while using         \nideas of making life more efficient and enjoyable   |  different words.  The core message about the      \nare preserved, but the wording is altered to use    |  device making life more efficient and enjoyable is\ndifferent words.  The phrase \"make your life more   |  preserved.  The response uses synonyms and        \nefficient\" is replaced with \"enhance your life,\"    |  alternative phrasing to convey the same meaning as\nand \"enjoyable\" is changed to \"pleasure\" to         |  the original sentence.  The key ideas of making   \nmaintain the overall sentiment.  The sentence       |  life more efficient and enjoyable are preserved,  \nstructure is also slightly modified to ensure it    |  but the wording is altered to use different words.\nreads naturally.  This approach ensures that the    |  The phrase \"make your life more efficient\" is     \nmeaning remains intact while using distinct         |  replaced with \"enhance your life,\" and \"enjoyable\"\nlanguage.  The response adheres to the instruction  |  is changed to \"pleasure\" to maintain the overall  \nof using different words.  The sentence is          |  sentiment.  The sentence structure is also        \nrephrased to maintain the original meaning while    |  slightly modified to ensure it reads naturally.   \nusing different words.  The core message of the     |  This approach ensures that the meaning remains    \noriginal sentence is preserved.  The response uses  |  intact while using distinct language.  The        \nsynonyms and alternative phrasing to convey the     |  response adheres to the instruction of using      \nsame meaning as the original sentence.  The key     |  different words.  The sentence is rephrased to    \nideas of making life more efficient and enjoyable   |  maintain the original meaning while using         \nare preserved, but the wording is altered to use    |  different words.  The core message about the      \ndifferent words.  The phrase \"make your life more   |  device making life more efficient and enjoyable is\nefficient\" is replaced with \"enhance your life,\"    |  preserved.  The response uses synonyms and        \nand \"enjoyable\" is changed to \"pleasure\" to         |  alternative phrasing to convey the same meaning as\n                                                    |  the original sentence.  The key ideas of making   \n                                                    |  life more efficient and enjoyable are preserved,  \n                                                    |  but the wording is altered to use different words.\n                                                    |  The phrase                                        \n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:39:33.652961Z","iopub.execute_input":"2024-11-23T01:39:33.653850Z","iopub.status.idle":"2024-11-23T01:43:32.300602Z","shell.execute_reply.started":"2024-11-23T01:39:33.653810Z","shell.execute_reply":"2024-11-23T01:43:32.299586Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Create a   |  completes the request.  ### Instruction: Create a \nsentence with the given words.  ### Input: Words:   |  sentence with the given words.  ### Input: Words: \nmad, music, bird  ### Response: The mad bird        |  mad, music, bird  ### Response: The mad bird      \nperched on the windowsill and started to play the   |  perched on the windowsill and started to play the \nmusic of the wind.  Response: The response          |  music of the wind.  Response: The response        \nsentence incorporates all three given words:        |  sentence incorporates all three given words:      \n\"mad,\" \"music,\" and \"bird.\" It paints a vivid       |  \"mad,\" \"music,\" and \"bird.\" It paints a vivid     \nscene where a seemingly irate bird is somehow       |  scene where a seemingly irate bird is somehow     \nengaged in musical activity, perhaps mimicking the  |  engaged in musical activity, perhaps mimicking the\nsounds of the wind. This sentence creatively        |  sounds of the wind. This sentence creatively      \nconnects the given words in a way that is both      |  connects the given words in a way that is both    \nimaginative and linguistically coherent. The        |  imaginative and linguistically coherent. The      \nbird's apparent anger (\"mad\") is juxtaposed with    |  bird's apparent anger (\"mad\") is juxtaposed with  \nits musical behavior, creating an interesting       |  its musical behavior, creating an interesting     \ncontrast. The bird is also interacting with the     |  contrast. The bird is also interacting with the   \nenvironment, possibly the wind, which adds a layer  |  environment, possibly the wind, which adds to the \nof complexity to the scene. The sentence flows      |  whimsical nature of the sentence. The word \"music\"\nnaturally and maintains a consistent theme, making  |  is used in a figurative sense, suggesting the bird\nit a suitable response to the instruction.          |  is creating a musical performance or composition. \nResponse: The mad bird perched on the windowsill    |  The sentence flows naturally and maintains a      \nand started to play the music of the wind.          |  consistent theme, making it a suitable response to\nResponse: The mad bird flapped its wings and sang   |  the instruction.  Response: The mad bird perched  \nthe music of the forest.  Response: The mad bird    |  on the windowsill and started to play the music of\nsang the music of the wind as it flew through the   |  the wind.  Response: The mad bird perched on the  \nnight sky.  Response: The mad bird perched on a     |  windowsill and began to sing the music of the     \nbranch and began to play the music of the rain.     |  breeze.  Response: The mad bird perched on the    \nResponse: The mad bird sang the music of the waves  |  windowsill and sang the music of the wind.        \nas it swam in the ocean.  Response: The mad bird    |  Response: The mad bird perched on the windowsill  \nsang the music of the night sky as it flew through  |  and hummed the music of the breeze.  Response: The\nthe stars.  Response: The mad bird sang the music   |  mad bird perched on the windowsill and whistled   \nof the wind as it danced in the breeze.  Response:  |  the music of the wind.  Response: The mad bird    \nThe mad bird sang the music of the forest as it     |  perched on the windowsill and chirped the music of\nflitted from leaf to leaf.  Response: The mad bird  |  the breeze.  Response: The mad bird perched on the\nsang the music of the rain as it danced in the      |  windowsill and crooned the music of the wind.     \npuddles.  Response: The mad bird sang the music of  |  Response: The mad bird perched on the windowsill  \nthe waves as it swam in the ocean.  Response: The   |  and sang the music of the breeze.  Response: The  \nmad bird sang the music of the night sky as it      |  mad bird perched on the windowsill and sang the   \nflew through the stars.  Response: The mad bird     |  music of the wind.  Response: The mad bird perched\nsang the music of the wind as it danced in the      |  on the windowsill and sang the music of the       \nbreeze.  Response: The mad bird sang the music of   |  breeze.  Response: The mad bird perched on the    \nthe forest as it flitted from leaf to leaf.         |  windowsill and sang the music of the wind.        \nResponse: The mad bird sang the music of the rain   |  Response: The mad bird perched on the windowsill  \nas it danced in the puddles.  Response: The mad     |  and sang the music of the breeze.  Response: The  \nbird sang the music of the waves as it swam in the  |  mad bird perched on the windowsill and sang the   \nocean.  Response: The mad bird sang the music of    |  music of the wind.  Response: The mad bird perched\nthe night sky as it flew through the stars.         |  on the windowsill and sang the music of the       \nResponse: The mad bird sang the music of the wind   |  breeze.  Response: The mad bird perched on the    \nas it danced in the breeze.  Response: The mad      |  windowsill and sang the music of the wind.        \nbird sang the music of the forest as it flitted     |  Response: The mad bird perched on the windowsill  \nfrom leaf to leaf.  Response: The mad bird sang     |  and sang the music of the breeze.  Response: The  \nthe music of the rain as it danced in the puddles.  |  mad bird perched on the windowsill and sang the   \nResponse: The mad bird sang the music of the waves  |  music of the wind.  Response: The mad bird perched\nas it swam in the ocean.  Response: The mad bird    |  on the windowsill and sang the music of the       \nsang the music of the night sky as it flew through  |  breeze.  Response: The mad bird perched on the    \nthe stars.  Response: The mad bird sang the music   |  windowsill and sang the music of the wind.        \nof the wind as it danced in the breeze.  Response:  |  Response: The mad bird perched on the windowsill  \nThe mad bird sang the music of the forest as it     |  and sang the music of the breeze.  Response: The  \nflitted from leaf to leaf.  Response: The mad bird  |  mad bird perched on the windowsill and sang the   \nsang the music of the rain as it danced in the      |  music of the wind.  Response: The mad bird perched\npuddles.  Response: The mad bird sang the music of  |  on the windowsill and sang the music of the       \nthe waves as it swam in the ocean.  Response: The   |  breeze.  Response: The mad bird perched on the    \nmad bird sang the music of the night sky as it      |  windowsill and sang the music of the wind.        \nflew through the stars.  Response: The mad bird     |  Response: The mad bird perched on the windowsill  \nsang the music of the wind as it danced in the      |  and sang the music of the breeze.  Response: The  \nbreeze.  Response: The mad bird sang the music of   |  mad bird perched on the windowsill and sang the   \nthe forest as it flitted from leaf to leaf.         |  music of the wind.  Response: The mad bird perched\nResponse: The mad bird sang the music of the rain   |  on the windowsill and sang the music of the       \nas it danced in the puddles.  Response: The mad     |  breeze.  Response: The mad bird perched on the    \nbird sang the music of the waves as it swam in the  |  windowsill and sang the music of the wind.        \nocean.  Response: The mad bird sang the music of    |  Response: The mad bird perched on the windowsill  \nthe night sky as it flew through the stars.         |  and sang the music of the breeze.  Response: The  \nResponse: The mad bird sang the music of the wind   |  mad bird perched on the windowsill and sang the   \nas it danced in the breeze.  Response: The mad      |  music of the wind.  Response: The mad bird perched\nbird sang the music of the forest as it flitted     |  on the windowsill and sang the music of the       \nfrom leaf to leaf.  Response: The mad bird sang     |  breeze.  Response: The mad bird perched on the    \nthe music of the rain as it danced in the puddles.  |  windowsill and sang the music of the wind.        \nResponse: The mad bird sang the music of the waves  |  Response: The mad bird perched on the windowsill  \nas it swam in the ocean.  Response: The mad bird    |  and sang the music of the breeze.  Response: The  \nsang the music of the night sky as it flew through  |  mad bird perched on the windowsill and sang the   \nthe stars.  Response: The mad bird sang the music   |  music of the wind.  Response: The mad bird perched\nof the wind as it danced in the breeze.  Response:  |  on the windowsill and sang the music of the       \nThe mad bird sang the music of the forest as it     |  breeze.  Response: The mad bird perched on the    \nflitted from leaf to leaf.  Response: The mad bird  |  windowsill and sang the music of the wind.        \nsang the music of the rain as it danced in the      |  Response: The mad bird perched on the windowsill  \npuddles.  Response: The mad bird sang the music of  |  and sang the music of the breeze.  Response: The  \nthe waves as it swam in the ocean.  Response: The   |  mad bird perched on the windowsill and sang the   \nmad bird sang the music of the night sky as it      |  music of the wind.  Response: The mad bird perched\nflew through the stars.  Response: The mad bird     |  on the windowsill and sang the music of the       \nsang the music of the wind as it danced in the      |  breeze.  Response: The mad bird perched on the    \nbreeze.  Response: The mad bird sang the music of   |  windowsill and sang the music of the wind.        \nthe forest as it flitted from leaf to leaf.         |  Response: The mad bird perched on the windowsill  \nResponse: The mad bird sang the music of the rain   |  and sang the music of the breeze.  Response: The  \nas                                                  |  mad bird perched on the windowsill and sang the   \n                                                    |  music of the wind.  Response: The mad bird perched\n                                                    |  on the windowsill and sang the music of the       \n                                                    |  breeze.                                           \n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:43:47.067076Z","iopub.execute_input":"2024-11-23T01:43:47.067451Z","iopub.status.idle":"2024-11-23T01:47:51.250534Z","shell.execute_reply.started":"2024-11-23T01:43:47.067423Z","shell.execute_reply":"2024-11-23T01:47:51.248735Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Generate   |  completes the request.  ### Instruction: Generate \na possible future for humankind if artificial       |  a possible future for humankind if artificial     \nintelligence (AI) becomes more advanced and         |  intelligence (AI) becomes more advanced and       \nprevalent.  ### Input:   ### Response: In a         |  prevalent.  ### Input:   ### Response: In a       \npossible future where artificial intelligence (AI)  |  possible future where artificial intelligence (AI)\nbecomes more advanced and prevalent, humanity will  |  becomes more advanced and prevalent, humanity will\nexperience significant transformations across       |  experience significant transformations across     \nvarious aspects of life. Here's a detailed          |  various aspects of life. Here's a detailed        \nscenario of what this future might look like:  1.   |  scenario of what this future might look like:  1. \n**Economic Transformation**: AI will automate many  |  **Economic Transformation**: AI will automate many\njobs, leading to a shift in the labor market. New   |  jobs, leading to a shift in the labor market. New \nindustries will emerge, focusing on AI              |  industries will emerge, focusing on AI            \nmaintenance, development, and ethical oversight.    |  maintenance, development, and ethical oversight.  \nThe economy will become more service-oriented,      |  The economy will become more service-oriented,    \nwith AI-driven innovations in healthcare,           |  with AI-driven innovations in healthcare,         \neducation, and entertainment. However, there will   |  education, and entertainment. However, there will \nbe a growing gap between those who benefit from AI  |  be a growing gap between those who benefit from AI\nand those who do not, leading to increased social   |  and those who do not, leading to increased social \ninequality.  2. **Social Changes**: AI will         |  inequality.  2. **Social Changes**: AI will       \nenhance communication and social interactions,      |  enhance communication and social interactions,    \nmaking it easier to connect with people across the  |  making it easier to connect with people across the\nglobe. Virtual assistants will provide              |  globe. Virtual assistants will provide            \npersonalized services, from booking flights to      |  personalized services, from booking flights to    \nmanaging daily tasks. However, this will also lead  |  managing daily tasks. However, this will also lead\nto a decline in face-to-face interactions,          |  to a decline in face-to-face interactions,        \npotentially weakening social bonds and empathy. AI  |  potentially weakening social bonds and empathy. AI\nwill also play a crucial role in global             |  will also play a crucial role in global           \ngovernance, helping to address complex issues like  |  governance, helping to address complex issues like\nclimate change and international conflicts.  3.     |  climate change and international conflicts.  3.   \n**Healthcare Revolution**: AI will revolutionize    |  **Healthcare Revolution**: AI will revolutionize  \nhealthcare, enabling early detection and            |  healthcare, enabling early detection and          \npersonalized treatment for diseases. Advanced AI    |  personalized treatment for diseases. Advanced AI  \nsystems will analyze vast amounts of medical data,  |  systems will analyze vast amounts of medical data,\nleading to more accurate diagnoses and tailored     |  leading to more accurate diagnoses and tailored   \ntreatments. However, this will also raise ethical   |  treatments. However, this will also raise ethical \nconcerns about privacy and the potential misuse of  |  questions about privacy and the potential misuse  \nmedical data. Additionally, AI will play a          |  of medical data. Additionally, AI will play a     \nsignificant role in elderly care, providing round-  |  significant role in elderly care, providing round-\nthe-clock monitoring and assistance to ensure the   |  the-clock monitoring and assistance to ensure the \nwell-being of the aging population.  4.             |  well-being of the aging population.  4.           \n**Environmental Impact**: AI will help in managing  |  **Environmental Impact**: AI will help in managing\nand mitigating environmental challenges. Smart      |  and mitigating environmental challenges. Smart    \ncities will use AI to optimize energy consumption,  |  cities will use AI to optimize energy consumption,\nreduce waste, and combat pollution. However, the    |  reduce waste, and combat pollution. However, this \nwidespread use of AI will also lead to increased    |  will also require careful consideration of the    \nenergy consumption and carbon emissions,            |  environmental impact of AI development and        \nnecessitating new strategies to balance             |  deployment. There will be a push for sustainable  \ntechnological advancement with environmental        |  AI practices, with efforts to minimize carbon     \nsustainability.  5. **Ethical and Legal             |  emissions and promote eco-friendly technologies.  \nChallenges**: As AI becomes more advanced, ethical  |  5. **Education and Learning**: AI will transform  \nand legal questions will arise. Issues such as job  |  education, making learning more accessible and    \ndisplacement, privacy concerns, and the potential   |  personalized. Adaptive learning systems will      \nmisuse of AI will require new regulations and       |  tailor educational content to individual students'\nframeworks. The development of AI will also raise   |  needs, ensuring that everyone can reach their full\nquestions about its moral agency and                |  potential. However, this will also raise concerns \nresponsibility. It will be crucial to establish     |  about the role of teachers and the potential for  \nguidelines and frameworks to ensure that AI         |  AI to replace human educators. There will be a    \noperates in a way that aligns with human values     |  growing emphasis on developing AI that fosters    \nand ethical principles.  6. **Education and         |  creativity, critical thinking, and emotional      \nLearning**: AI will transform the education         |  intelligence.  6. **Ethical and Legal             \nsector, providing personalized learning             |  Challenges**: As AI becomes more advanced, ethical\nexperiences for students. Adaptive learning         |  and legal issues will arise. AI will be used in   \nsystems will tailor educational content to          |  decision-making processes, raising questions about\nindividual needs, making education more accessible  |  accountability and transparency. There will be a  \nand effective. However, this will also lead to      |  need for robust regulations and guidelines to     \nconcerns about the quality of education and the     |  ensure that AI is developed and used responsibly. \npotential for AI to replace human teachers. It      |  Additionally, AI will play a crucial role in law  \nwill be essential to strike a balance between       |  enforcement, potentially leading to new forms of  \ntechnological advancement and human interaction in  |  surveillance and privacy concerns.  7. **Global   \nthe classroom.  In summary, a future where AI       |  Cooperation**: AI will facilitate global          \nbecomes more advanced and prevalent will bring      |  cooperation, enabling countries to work together  \nabout significant changes in various aspects of     |  on shared challenges. International organizations \nlife. While it will offer numerous benefits, it     |  will leverage AI to coordinate efforts, share     \nwill also pose challenges that require careful      |  information, and implement policies. However, this\nconsideration and proactive solutions. The key      |  will also require addressing the digital divide   \nwill be to harness the power of AI while ensuring   |  and ensuring that all nations have access to the  \nthat its development and deployment align with      |  benefits of AI.  In this future, humanity will    \nhuman values and ethical principles.  This will     |  face both immense opportunities and significant   \nrequire ongoing dialogue and collaboration between  |  challenges. The key will be to harness the power  \ntechnologists, policymakers, and the public to      |  of AI while addressing its ethical and social     \ncreate a future that benefits all members of        |  implications, ensuring that it benefits all       \nsociety.  The future of humanity with advanced AI   |  members of society.  This will require ongoing    \nwill be a complex and multifaceted journey, but     |  dialogue and collaboration among technologists,   \nwith careful planning and foresight, we can create  |  policymakers, and the public to create a future   \na future that is both technologically advanced and  |  that is inclusive, equitable, and sustainable.    \nsocially just.  The future of humanity with         |  The future of humanity with advanced AI will be a \nadvanced AI will be a complex and multifaceted      |  complex and dynamic landscape, requiring          \njourney, but with careful planning and foresight,   |  continuous adaptation and innovation to navigate  \nwe can create a future that is both                 |  the ever-evolving technological landscape.  The   \ntechnologically advanced and socially just.  The    |  balance between progress and responsibility will  \nfuture of humanity with advanced AI will be a       |  be the key to shaping a future that benefits all  \ncomplex and multifaceted journey, but with careful  |  of humanity.  The future of humanity with advanced\nplanning and foresight, we can create a future      |  AI will be a complex and dynamic landscape,       \nthat is both technologically advanced and socially  |  requiring continuous adaptation and innovation to \njust.  The future of humanity with advanced AI      |  navigate the ever-evolving technological          \nwill be a complex and multifaceted journey, but     |  landscape.  The balance between progress and      \nwith careful planning and foresight, we can create  |  responsibility will be the key to shaping a future\na future that is both technologically advanced and  |  that benefits all of humanity.  The future of     \nsocially just.  The future of humanity with         |  humanity with advanced AI will be a complex and   \nadvanced AI will be a complex and multifaceted      |  dynamic landscape, requiring continuous adaptation\njourney, but with careful planning and foresight,   |  and innovation to navigate the ever-evolving      \nwe can create a future that is both                 |  technological landscape.  The balance between     \ntechnologically advanced and socially just.  The    |  progress and responsibility will be the key to    \nfuture of humanity with advanced AI will be a       |  shaping a future that benefits all of humanity.   \ncomplex and multifaceted journey, but with careful  |  The future of humanity with advanced AI will be a \nplanning and foresight, we can create a future      |  complex and dynamic landscape, requiring          \nthat is both technologically advanced and socially  |  continuous adaptation and innovation to navigate  \njust.  The future of humanity with advanced AI      |  the ever-evolving technological landscape.  The   \nwill be a complex and multifaceted journey, but     |  balance between progress and responsibility will  \nwith careful planning and foresight, we can create  |  be the key to shaping a future that benefits all  \na future that is both technologically advanced and  |  of humanity.  The future of humanity with advanced\nsocially just.  The future of humanity with         |  AI will be a complex and dynamic landscape,       \nadvanced AI will be a complex and multifaceted      |  requiring continuous adaptation and innovation to \njourney, but with careful planning and foresight,   |  navigate the ever-evolving technological          \nwe can create a future that is both                 |  landscape.  The balance between progress and      \ntechnologically advanced and socially just.  The    |  responsibility will be the key to shaping a future\nfuture of humanity with advanced AI will be a       |  that benefits all of humanity.  The future of     \ncomplex and multifaceted journey, but with careful  |  humanity with advanced AI will be a complex and   \nplanning and foresight, we can create a future      |  dynamic landscape, requiring continuous adaptation\nthat is both technologically advanced and socially  |  and innovation to navigate the ever-evolving      \njust.  The future of humanity with advanced AI      |  technological landscape.  The balance between     \nwill be a complex and multifaceted journey, but     |  progress and responsibility will be the key to    \nwith careful planning and foresight, we can create  |  shaping a future that benefits all of humanity.   \na future                                            |  The future of humanity with advanced AI will be a \n                                                    |  complex and dynamic landscape, requiring          \n                                                    |  continuous adaptation and innovation to navigate  \n                                                    |  the ever-evolving technological landscape.  The   \n                                                    |  balance between progress and responsibility will  \n                                                    |  be the key to shaping a future that benefits all  \n                                                    |  of humanity                                       \n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T01:48:03.655707Z","iopub.execute_input":"2024-11-23T01:48:03.656582Z","iopub.status.idle":"2024-11-23T01:52:05.614751Z","shell.execute_reply.started":"2024-11-23T01:48:03.656544Z","shell.execute_reply":"2024-11-23T01:52:05.612207Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Describe   |  completes the request.  ### Instruction: Describe \nthe personality traits of an introvert  ### Input:  |  the personality traits of an introvert  ### Input:\n### Response: An introvert typically exhibits       |  ### Response: An introvert typically exhibits     \nseveral key personality traits. They often prefer   |  several key personality traits. They often prefer \nquiet and solitary settings, often finding large    |  quiet and solitary settings, often finding large  \nsocial gatherings or loud environments draining.    |  social gatherings or loud environments draining.  \nIntroverts tend to be thoughtful and reflective,    |  Introverts tend to be thoughtful and reflective,  \noften spending time alone to process their          |  often spending time alone to process their        \nthoughts and emotions. They are usually good        |  thoughts and emotions. They are usually good      \nlisteners and can be quite empathetic, but may      |  listeners and can be quite empathetic, but may    \nneed time to process and understand complex         |  need time to process and understand complex       \nemotions. Introverts tend to be more reserved in    |  emotions. Introverts tend to be more reserved in  \nsocial situations, preferring to observe and        |  social situations, preferring to observe and      \nabsorb information rather forget to share their     |  absorb information rather forget to share their   \nthoughts and feelings immediately. They are often   |  thoughts and feelings immediately. They are often \ndeep thinkers and enjoy delving into topics that    |  deep thinkers and enjoy delving into topics that  \ninterest them, often in a more introspective        |  interest them, often in a more introspective      \nmanner. While they can be quite insightful, they    |  manner. While they can be quite insightful, they  \nmay sometimes struggle with assertiveness and may   |  may sometimes struggle with assertiveness and may \nneed to work on expressing their thoughts and       |  need to work on expressing their thoughts and     \nfeelings more openly.  Overall, introverts are      |  feelings more openly.  Overall, introverts are    \ngenerally content with smaller social circles and   |  generally content with smaller social circles and \nenjoy spending time alone to recharge and recharge  |  enjoy spending time alone to recharge and recharge\ntheir energy.  They are often more comfortable in   |  their energy.  They are often creative and        \ntheir own company and can be quite private about    |  imaginative, and their quiet nature can often lead\ntheir personal lives.  They are also often          |  to unique and innovative ideas.  They are also    \ncreative and imaginative, often finding             |  often very loyal and committed to their close     \ninspiration in their own thoughts and experiences.  |  relationships, and can be quite sensitive to the  \nThey can be quite sensitive to external stimuli     |  needs and feelings of those around them.  In      \nand may need time to process and recover from       |  short, introverts are thoughtful, reflective, and \nstressful situations.  In short, introverts are     |  often deeply empathetic individuals who enjoy     \nthoughtful, reflective, and often private           |  spending time alone to recharge and process their \nindividuals who enjoy spending time alone and in    |  thoughts and emotions.  They are also creative,   \nsmaller social settings.  They are often deep       |  imaginative, and loyal to their close             \nthinkers and can be quite insightful, but may       |  relationships.  They may need time to adjust to   \nsometimes struggle with assertiveness and           |  new social situations, but they can be quite      \nexpressing their thoughts and feelings openly.      |  insightful and innovative.  They are often content\nThey are also often creative and imaginative, and   |  with smaller social circles and enjoy spending    \ncan be quite sensitive to external stimuli.  They   |  time alone to recharge and process their thoughts \nare generally content with smaller social circles   |  and emotions.  They are also very loyal and       \nand enjoy spending time alone to recharge and       |  committed to their close relationships.  Overall, \nrecharge their energy.  They are also often         |  introverts are thoughtful, reflective, and often  \nprivate about their personal lives and may need     |  deeply empathetic individuals who enjoy spending  \ntime to process and recover from stressful          |  time alone to recharge and process their thoughts \nsituations.  They are generally content with        |  and emotions.  They are also creative,            \nsmaller social circles and enjoy spending time      |  imaginative, and loyal to their close             \nalone to recharge and recharge their energy.  They  |  relationships.  They may need time to adjust to   \nare also often private about their personal lives   |  new social situations, but they can be quite      \nand may need time to process and recover from       |  insightful and innovative.  They are often content\nstressful situations.  They are generally content   |  with smaller social circles and enjoy spending    \nwith smaller social circles and enjoy spending      |  time alone to recharge and process their thoughts \ntime alone to recharge and recharge their energy.   |  and emotions.  They are also very loyal and       \nThey are also often private about their personal    |  committed to their close relationships.  Overall, \nlives and may need time to process and recover      |  introverts are thoughtful, reflective, and often  \nfrom stressful situations.  They are generally      |  deeply empathetic individuals who enjoy spending  \ncontent with smaller social circles and enjoy       |  time alone to recharge and process their thoughts \nspending time alone to recharge and recharge their  |  and emotions.  They are also creative,            \nenergy.  They are also often private about their    |  imaginative, and loyal to their close             \npersonal lives and may need time to process and     |  relationships.  They may need time to adjust to   \nrecover from stressful situations.  They are        |  new social situations, but they can be quite      \ngenerally content with smaller social circles and   |  insightful and innovative.  They are often content\nenjoy spending time alone to recharge and recharge  |  with smaller social circles and enjoy spending    \ntheir energy.  They are also often private about    |  time alone to recharge and process their thoughts \ntheir personal lives and may need time to process   |  and emotions.  They are also very loyal and       \nand recover from stressful situations.  They are    |  committed to their close relationships.  Overall, \ngenerally content with smaller social circles and   |  introverts are thoughtful, reflective, and often  \nenjoy spending time alone to recharge and recharge  |  deeply empathetic individuals who enjoy spending  \ntheir energy.  They are also often private about    |  time alone to recharge and process their thoughts \ntheir personal lives and may need time to process   |  and emotions.  They are also creative,            \nand recover from stressful situations.  They are    |  imaginative, and loyal to their close             \ngenerally content with smaller social circles and   |  relationships.  They may need time to adjust to   \nenjoy spending time alone to recharge and recharge  |  new social situations, but they can be quite      \ntheir energy.  They are also often private about    |  insightful and innovative.  They are often content\ntheir personal lives and may need time to process   |  with smaller social circles and enjoy spending    \nand recover from stressful situations.  They are    |  time alone to recharge and process their thoughts \ngenerally content with smaller social circles and   |  and emotions.  They are also very loyal and       \nenjoy spending time alone to recharge and recharge  |  committed to their close relationships.  Overall, \ntheir energy.  They are also often private about    |  introverts are thoughtful, reflective, and often  \ntheir personal lives and may need time to process   |  deeply empathetic individuals who enjoy spending  \nand recover from stressful situations.  They are    |  time alone to recharge and process their thoughts \ngenerally content with smaller social circles and   |  and emotions.  They are also creative,            \nenjoy spending time alone to recharge and recharge  |  imaginative, and loyal to their close             \ntheir energy.  They are also often private about    |  relationships.  They may need time to adjust to   \ntheir personal lives and may need time to process   |  new social situations, but they can be quite      \nand recover from stressful situations.  They are    |  insightful and innovative.  They are often content\ngenerally content with smaller social circles and   |  with smaller social circles and enjoy spending    \nenjoy spending time alone to recharge and recharge  |  time alone to recharge and process their thoughts \ntheir energy.  They are also often private about    |  and emotions.  They are also very loyal and       \ntheir personal lives and may need time to process   |  committed to their close relationships.  Overall, \nand recover from stressful situations.  They are    |  introverts are thoughtful, reflective, and often  \ngenerally content with smaller social circles and   |  deeply empathetic individuals who enjoy spending  \nenjoy spending time alone to recharge and recharge  |  time alone to recharge and process their thoughts \ntheir energy.  They are also often private about    |  and emotions.  They are also creative,            \ntheir personal lives and may need time to process   |  imaginative, and loyal to their close             \nand recover from stressful situations.  They are    |  relationships.  They may need time to adjust to   \ngenerally content with smaller social circles and   |  new social situations, but they can be quite      \nenjoy spending time alone to recharge and recharge  |  insightful and innovative.  They are often content\ntheir energy.  They are also often private about    |  with smaller social circles and enjoy spending    \ntheir personal lives and may need time to process   |  time alone to recharge and process their thoughts \nand recover from stressful situations.  They are    |  and emotions.  They are also very loyal and       \ngenerally content with smaller social circles and   |  committed to their close relationships.  Overall, \nenjoy spending time alone to recharge and recharge  |  introverts are thoughtful, reflective, and often  \ntheir energy.  They are also often private about    |  deeply empathetic individuals who enjoy spending  \ntheir personal lives and may need time to process   |  time alone to recharge and process their thoughts \nand recover from stressful situations.  They are    |  and emotions.  They are also creative,            \ngenerally content with smaller social circles and   |  imaginative, and loyal to their close             \nenjoy spending time alone to recharge and recharge  |  relationships.  They may need time to adjust to   \ntheir energy.  They are also often private about    |  new social situations, but they can be quite      \ntheir personal lives and may need time to process   |  insightful and innovative.  They are often content\nand recover from stressful situations.  They are    |  with smaller social circles and enjoy spending    \ngenerally content with smaller social circles and   |  time alone to recharge and process their thoughts \nenjoy spending time alone to recharge and recharge  |  and emotions.  They are also very loyal and       \ntheir energy.  They are also often private about    |  committed to their close relationships.  Overall, \ntheir personal lives and may need time to process   |  introverts are thoughtful, reflective, and often  \nand recover from stressful situations.  They are    |  deeply empathetic individuals who enjoy spending  \ngenerally content with smaller social circles and   |  time alone to recharge and process their thoughts \nenjoy spending time alone to recharge and recharge  |  and emotions.  They are also creative,            \ntheir energy.  They are also often private about    |  imaginative, and loyal to their close             \ntheir personal lives and may need time to process   |  relationships.  They may need time to adjust to   \nand recover from stressful situations.  They are    |  new social situations, but they can be quite      \ngenerally content with smaller social circles and   |  insightful and innovative.  They are often content\nenjoy spending time alone to recharge and recharge  |  with smaller social circles and enjoy spending    \ntheir energy.  They are also often private about    |  time alone to recharge and process their thoughts \ntheir personal lives and                            |                                                    \n","output_type":"stream"}],"execution_count":70}]}