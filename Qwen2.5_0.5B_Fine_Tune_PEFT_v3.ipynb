{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM_Fine_Tune/blob/main/Qwen2.5_0.5B_Fine_Tune_PEFT_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00 Import Modules"
      ],
      "metadata": {
        "id": "iNW_MCROx_hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers\n",
        "!pip install -q peft\n",
        "!pip install -U -q bitsandbytes\n",
        "!pip install -q datasets\n",
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "0-QxfiDVyT74",
        "trusted": true,
        "outputId": "69d52dc0-ca27-4fd1-81c4-bc2f70300670",
        "execution": {
          "iopub.status.busy": "2024-12-07T01:16:49.488679Z",
          "iopub.execute_input": "2024-12-07T01:16:49.489360Z",
          "iopub.status.idle": "2024-12-07T01:17:50.237168Z",
          "shell.execute_reply.started": "2024-12-07T01:16:49.489288Z",
          "shell.execute_reply": "2024-12-07T01:17:50.235875Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.12.2 requires transformers<4.47.0, but you have transformers 4.47.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "from random import randint\n",
        "from itertools import zip_longest\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig,\n",
        "  DataCollatorForSeq2Seq,\n",
        "  TextStreamer\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  PeftModel,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "TIgNx9Orx0It",
        "trusted": true,
        "outputId": "248c8f10-5eae-49a5-ba03-c6c30698404f",
        "execution": {
          "iopub.status.busy": "2024-12-07T01:17:50.239196Z",
          "iopub.execute_input": "2024-12-07T01:17:50.239519Z",
          "iopub.status.idle": "2024-12-07T01:17:57.600676Z",
          "shell.execute_reply.started": "2024-12-07T01:17:50.239490Z",
          "shell.execute_reply": "2024-12-07T01:17:57.599652Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "GPU is available!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "i-nwkyTDybqY",
        "trusted": true,
        "outputId": "f7789872-8053-4e26-a665-0c4f94689529",
        "execution": {
          "iopub.status.busy": "2024-12-07T01:17:57.602104Z",
          "iopub.execute_input": "2024-12-07T01:17:57.602403Z",
          "iopub.status.idle": "2024-12-07T01:17:57.924641Z",
          "shell.execute_reply.started": "2024-12-07T01:17:57.602376Z",
          "shell.execute_reply": "2024-12-07T01:17:57.923382Z"
        }
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01 Import Model"
      ],
      "metadata": {
        "id": "grIeJpUdyX0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Qwen/Qwen2.5-0.5B'"
      ],
      "metadata": {
        "id": "14Lkvw4cyZkY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:17:57.926447Z",
          "iopub.execute_input": "2024-12-07T01:17:57.926734Z",
          "iopub.status.idle": "2024-12-07T01:17:57.939027Z",
          "shell.execute_reply.started": "2024-12-07T01:17:57.926706Z",
          "shell.execute_reply": "2024-12-07T01:17:57.938276Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, base = True):\n",
        "  if base == True:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype = torch.float16,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "  else:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = True,\n",
        "      bnb_4bit_quant_type = 'nf4',\n",
        "      bnb_4bit_compute_dtype = torch.float16,\n",
        "      bnb_4bit_use_double_quant = True,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config = bnb_config,\n",
        "      trust_remote_code = True\n",
        "    ).to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GlskFscYyeco",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:17:57.940258Z",
          "iopub.execute_input": "2024-12-07T01:17:57.940651Z",
          "iopub.status.idle": "2024-12-07T01:17:57.951754Z",
          "shell.execute_reply.started": "2024-12-07T01:17:57.940612Z",
          "shell.execute_reply": "2024-12-07T01:17:57.950960Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "id": "HIYgZ1xF1qsl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:17:57.952700Z",
          "iopub.execute_input": "2024-12-07T01:17:57.952934Z",
          "iopub.status.idle": "2024-12-07T01:18:00.185269Z",
          "shell.execute_reply.started": "2024-12-07T01:17:57.952912Z",
          "shell.execute_reply": "2024-12-07T01:18:00.184427Z"
        },
        "outputId": "ea5991a5-0d07-4b94-ebce-0b8b592f676d"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "j6d6uYBfzCC4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:00.186225Z",
          "iopub.execute_input": "2024-12-07T01:18:00.186490Z",
          "iopub.status.idle": "2024-12-07T01:18:00.193826Z",
          "shell.execute_reply.started": "2024-12-07T01:18:00.186463Z",
          "shell.execute_reply": "2024-12-07T01:18:00.192917Z"
        },
        "outputId": "8c898c51-4013-40c4-c624-ac29c0d996bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 315119488\nTrainable parameters : 136178560\nTrainable percentage: 43.21%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02 Import Tokenizer"
      ],
      "metadata": {
        "id": "MU_19rT5zEIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer"
      ],
      "metadata": {
        "id": "lpB5JUjSzGtJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:00.194931Z",
          "iopub.execute_input": "2024-12-07T01:18:00.195283Z",
          "iopub.status.idle": "2024-12-07T01:18:00.653975Z",
          "shell.execute_reply.started": "2024-12-07T01:18:00.195247Z",
          "shell.execute_reply": "2024-12-07T01:18:00.653097Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03 Import Dataset"
      ],
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'microsoft/orca-math-word-problems-200k'"
      ],
      "metadata": {
        "id": "U01UXJdLzPXS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:00.655131Z",
          "iopub.execute_input": "2024-12-07T01:18:00.655501Z",
          "iopub.status.idle": "2024-12-07T01:18:00.660116Z",
          "shell.execute_reply.started": "2024-12-07T01:18:00.655464Z",
          "shell.execute_reply": "2024-12-07T01:18:00.659242Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 384"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:00.662865Z",
          "iopub.execute_input": "2024-12-07T01:18:00.663133Z",
          "iopub.status.idle": "2024-12-07T01:18:00.671816Z",
          "shell.execute_reply.started": "2024-12-07T01:18:00.663108Z",
          "shell.execute_reply": "2024-12-07T01:18:00.671061Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ],
      "metadata": {
        "id": "0ucM3l_FzUkp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:00.672729Z",
          "iopub.execute_input": "2024-12-07T01:18:00.673022Z",
          "iopub.status.idle": "2024-12-07T01:18:02.498674Z",
          "shell.execute_reply.started": "2024-12-07T01:18:00.672992Z",
          "shell.execute_reply": "2024-12-07T01:18:02.497820Z"
        },
        "outputId": "46aab32a-3934-45bc-aba0-2c9c820d8c6a"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(20000))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.499849Z",
          "iopub.execute_input": "2024-12-07T01:18:02.500131Z",
          "iopub.status.idle": "2024-12-07T01:18:02.506875Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.500105Z",
          "shell.execute_reply": "2024-12-07T01:18:02.506116Z"
        },
        "id": "lYJ_64dqrWiT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "FLRSMhJDzY5Z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.507884Z",
          "iopub.execute_input": "2024-12-07T01:18:02.508149Z",
          "iopub.status.idle": "2024-12-07T01:18:02.531174Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.508124Z",
          "shell.execute_reply": "2024-12-07T01:18:02.530285Z"
        },
        "outputId": "a72e1981-0721-472c-a9f1-6d52560fe29b"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "3exPEy0JdLyI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.532161Z",
          "iopub.execute_input": "2024-12-07T01:18:02.532445Z",
          "iopub.status.idle": "2024-12-07T01:18:02.538347Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.532417Z",
          "shell.execute_reply": "2024-12-07T01:18:02.537491Z"
        },
        "outputId": "c55a3082-dfda-48fd-cb45-788737083ca8"
      },
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ],
      "metadata": {
        "id": "xYKmTDtkAnt5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.539250Z",
          "iopub.execute_input": "2024-12-07T01:18:02.539542Z",
          "iopub.status.idle": "2024-12-07T01:18:02.551358Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.539517Z",
          "shell.execute_reply": "2024-12-07T01:18:02.550528Z"
        },
        "outputId": "d83209b2-2a74-4dc8-c1d0-3cb203ae7a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['question', 'answer']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04 Text Formatting"
      ],
      "metadata": {
        "id": "Wq59WgYJCDY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.552345Z",
          "iopub.execute_input": "2024-12-07T01:18:02.552626Z",
          "iopub.status.idle": "2024-12-07T01:18:02.563212Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.552602Z",
          "shell.execute_reply": "2024-12-07T01:18:02.562477Z"
        },
        "id": "ZQst6ZELrWiU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def preprocess(examples):\n",
        "  input = examples['question']\n",
        "  output = examples['answer']\n",
        "\n",
        "  text = prompt_format.format(input, output) + EOS_TOKEN\n",
        "  return {'prompt' : text}"
      ],
      "metadata": {
        "id": "0wXJNFBWWNYP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.564439Z",
          "iopub.execute_input": "2024-12-07T01:18:02.565112Z",
          "iopub.status.idle": "2024-12-07T01:18:02.573262Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.565071Z",
          "shell.execute_reply": "2024-12-07T01:18:02.572598Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset.map(preprocess, remove_columns = features)\n",
        "formatted_dataset"
      ],
      "metadata": {
        "id": "7TFGpGhoWS9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.574368Z",
          "iopub.execute_input": "2024-12-07T01:18:02.575233Z",
          "iopub.status.idle": "2024-12-07T01:18:02.587150Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.575192Z",
          "shell.execute_reply": "2024-12-07T01:18:02.586375Z"
        },
        "outputId": "c0bb5557-d6c9-4f92-90d3-6acb96796356"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt'],\n    num_rows: 20000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(formatted_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "Kidf8H5zefDC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.588380Z",
          "iopub.execute_input": "2024-12-07T01:18:02.589111Z",
          "iopub.status.idle": "2024-12-07T01:18:02.595715Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.589070Z",
          "shell.execute_reply": "2024-12-07T01:18:02.594837Z"
        },
        "outputId": "e1672a0d-99a3-464e-8c0e-2bfb7548a5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05 Tokenization"
      ],
      "metadata": {
        "id": "UMhGDyBpCHoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)"
      ],
      "metadata": {
        "id": "m7bxU8fiewb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.596721Z",
          "iopub.execute_input": "2024-12-07T01:18:02.596946Z",
          "iopub.status.idle": "2024-12-07T01:18:02.605940Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.596924Z",
          "shell.execute_reply": "2024-12-07T01:18:02.605271Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "M3BO26k-BmdS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.607021Z",
          "iopub.execute_input": "2024-12-07T01:18:02.607363Z",
          "iopub.status.idle": "2024-12-07T01:18:02.732945Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.607327Z",
          "shell.execute_reply": "2024-12-07T01:18:02.731969Z"
        },
        "outputId": "9de9c55b-78e9-4e52-a3ac-1b410ea1b1f7"
      },
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 20000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "wEHhMdV4pEFH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.734053Z",
          "iopub.execute_input": "2024-12-07T01:18:02.734355Z",
          "iopub.status.idle": "2024-12-07T01:18:02.741658Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.734290Z",
          "shell.execute_reply": "2024-12-07T01:18:02.740934Z"
        },
        "outputId": "47aef388-6287-41f1-b0dd-1a71049a45ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "C2m-e-ivDn1A",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.742805Z",
          "iopub.execute_input": "2024-12-07T01:18:02.743172Z",
          "iopub.status.idle": "2024-12-07T01:18:02.756177Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.743133Z",
          "shell.execute_reply": "2024-12-07T01:18:02.755366Z"
        },
        "outputId": "ff4f5797-14d3-442b-c78b-c39d2ada755f"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 18000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 2000\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_dataset['train']\n",
        "test_dataset = tokenized_dataset['test']\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "QHs-BnR_zd9C",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.757283Z",
          "iopub.execute_input": "2024-12-07T01:18:02.757644Z",
          "iopub.status.idle": "2024-12-07T01:18:02.763680Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.757607Z",
          "shell.execute_reply": "2024-12-07T01:18:02.762789Z"
        },
        "outputId": "14d8c234-8225-4750-cc8e-de255c002953"
      },
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 18000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ],
      "metadata": {
        "id": "-CUZuEENF2mW",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.764746Z",
          "iopub.execute_input": "2024-12-07T01:18:02.765019Z",
          "iopub.status.idle": "2024-12-07T01:18:02.788002Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.764995Z",
          "shell.execute_reply": "2024-12-07T01:18:02.787130Z"
        },
        "outputId": "b684afea-6e9c-4285-b0b4-fba51f3ecc7b"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              prompt  \\\n0  ### Question:\\nCompared to the amount of water...   \n1  ### Question:\\nEunseol divided 28 candies equa...   \n2  ### Question:\\nNancy and Rose are making brace...   \n3  ### Question:\\nWithout factoring in the cost o...   \n4  ### Question:\\nThere are 250 books inside a li...   \n\n                                           input_ids  \\\n0  [14374, 15846, 510, 1092, 7212, 311, 279, 3311...   \n1  [14374, 15846, 510, 36, 359, 325, 337, 17779, ...   \n2  [14374, 15846, 510, 45, 6572, 323, 15964, 525,...   \n3  [14374, 15846, 510, 26040, 2097, 5503, 304, 27...   \n4  [14374, 15846, 510, 3862, 525, 220, 17, 20, 15...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nCompared to the amount of water...</td>\n      <td>[14374, 15846, 510, 1092, 7212, 311, 279, 3311...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nEunseol divided 28 candies equa...</td>\n      <td>[14374, 15846, 510, 36, 359, 325, 337, 17779, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nNancy and Rose are making brace...</td>\n      <td>[14374, 15846, 510, 45, 6572, 323, 15964, 525,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nWithout factoring in the cost o...</td>\n      <td>[14374, 15846, 510, 26040, 2097, 5503, 304, 27...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere are 250 books inside a li...</td>\n      <td>[14374, 15846, 510, 3862, 525, 220, 17, 20, 15...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['prompt'])"
      ],
      "metadata": {
        "id": "6PxxrK5Rd4gk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.789081Z",
          "iopub.execute_input": "2024-12-07T01:18:02.789384Z",
          "iopub.status.idle": "2024-12-07T01:18:02.794527Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.789357Z",
          "shell.execute_reply": "2024-12-07T01:18:02.793646Z"
        },
        "outputId": "791ca52b-a284-42cd-9bef-29f704b83e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Question:\nCompared to the amount of water she drank, Carla drank three times as much soda minus a certain number of ounces. She drank 54 ounces of liquid total. If Carla drank 15 ounces of water, how many ounces were subtracted from the amount of soda she drank?\n### Answer:\nLet's call the number of ounces subtracted from the soda \"x.\"\n\nCarla drank 15 ounces of water. Since she drank three times as much soda as water, she would have drunk 3 * 15 = 45 ounces of soda without any subtraction.\n\nHowever, we know that she drank a total of 54 ounces of liquid. So, the water and the actual amount of soda she drank (after subtracting x ounces) should add up to 54 ounces.\n\nSo, we have:\n15 ounces (water) + (45 ounces - x) (soda) = 54 ounces (total)\n\nNow, let's solve for x:\n15 + 45 - x = 54\n60 - x = 54\nx = 60 - 54\nx = 6 ounces\n\nTherefore, 6 ounces were subtracted from the amount of soda she would have drunk.<|endoftext|>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "HR79ppIiE78f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.795516Z",
          "iopub.execute_input": "2024-12-07T01:18:02.795761Z",
          "iopub.status.idle": "2024-12-07T01:18:02.805795Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.795737Z",
          "shell.execute_reply": "2024-12-07T01:18:02.804972Z"
        },
        "outputId": "e3fadcef-d6ed-4ebd-bb3e-2190c1d63600"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[14374, 15846, 510, 1092, 7212, 311, 279, 3311, 315, 3015, 1340, 53144, 11, 93770, 53144, 2326, 3039, 438, 1753, 38862, 27283, 264, 3654, 1372, 315, 48038, 13, 2932, 53144, 220, 20, 19, 48038, 315, 14473, 2790, 13, 1416, 93770, 53144, 220, 16, 20, 48038, 315, 3015, 11, 1246, 1657, 48038, 1033, 32256, 291, 504, 279, 3311, 315, 38862, 1340, 53144, 5267, 14374, 21806, 510, 10061, 594, 1618, 279, 1372, 315, 48038, 32256, 291, 504, 279, 38862, 330, 87, 2217, 8852, 4260, 53144, 220, 16, 20, 48038, 315, 3015, 13, 8704, 1340, 53144, 2326, 3039, 438, 1753, 38862, 438, 3015, 11, 1340, 1035, 614, 28750, 220, 18, 353, 220, 16, 20, 284, 220, 19, 20, 48038, 315, 38862, 2041, 894, 75240, 382, 11209, 11, 582, 1414, 429, 1340, 53144, 264, 2790, 315, 220, 20, 19, 48038, 315, 14473, 13, 2055, 11, 279, 3015, 323, 279, 5042, 3311, 315, 38862, 1340, 53144, 320, 10694, 32256, 287, 856, 48038, 8, 1265, 912, 705, 311, 220, 20, 19, 48038, 382, 4416, 11, 582, 614, 510, 16, 20, 48038, 320, 12987, 8, 488, 320, 19, 20, 48038, 481, 856, 8, 320, 82, 13993, 8, 284, 220, 20, 19, 48038, 320, 5035, 692, 7039, 11, 1077, 594, 11625, 369, 856, 510, 16, 20, 488, 220, 19, 20, 481, 856, 284, 220, 20, 19, 198, 21, 15, 481, 856, 284, 220, 20, 19, 198, 87, 284, 220, 21, 15, 481, 220, 20, 19, 198, 87, 284, 220, 21, 48038, 271, 54815, 11, 220, 21, 48038, 1033, 32256, 291, 504, 279, 3311, 315, 38862, 1340, 1035, 614, 28750, 13, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0]['attention_mask'])"
      ],
      "metadata": {
        "id": "xGmCvvZTE82D",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.809987Z",
          "iopub.execute_input": "2024-12-07T01:18:02.810235Z",
          "iopub.status.idle": "2024-12-07T01:18:02.817878Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.810212Z",
          "shell.execute_reply": "2024-12-07T01:18:02.817106Z"
        },
        "outputId": "6dc6899f-f893-422f-cc26-67a986641fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06 Data Collator Set Up"
      ],
      "metadata": {
        "id": "JFX4u0vc0UkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ],
      "metadata": {
        "id": "F-mkiTYw0cZi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.818943Z",
          "iopub.execute_input": "2024-12-07T01:18:02.819964Z",
          "iopub.status.idle": "2024-12-07T01:18:02.827141Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.819923Z",
          "shell.execute_reply": "2024-12-07T01:18:02.826259Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07 Evaluation Metrics Set Up"
      ],
      "metadata": {
        "id": "hP1Mu0J6CTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "wzNdWpCI0c7a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.828402Z",
          "iopub.execute_input": "2024-12-07T01:18:02.829075Z",
          "iopub.status.idle": "2024-12-07T01:18:02.840009Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.829036Z",
          "shell.execute_reply": "2024-12-07T01:18:02.839341Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tEkgHY4fxFIJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.841062Z",
          "iopub.execute_input": "2024-12-07T01:18:02.841369Z",
          "iopub.status.idle": "2024-12-07T01:18:02.851174Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.841342Z",
          "shell.execute_reply": "2024-12-07T01:18:02.850210Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Set Up PEFT / LoRA / QLoRA"
      ],
      "metadata": {
        "id": "VLFCnU8-ZoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "\n",
        "#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "  lora_alpha = lora_alpha,\n",
        "  lora_dropout = lora_dropout,\n",
        "  r = lora_r,\n",
        "  bias = 'none',\n",
        "  task_type = 'CAUSAL_LM',\n",
        "  target_modules = target_modules,\n",
        ")"
      ],
      "metadata": {
        "id": "67HK09faZqQh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.852321Z",
          "iopub.execute_input": "2024-12-07T01:18:02.852673Z",
          "iopub.status.idle": "2024-12-07T01:18:02.863437Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.852634Z",
          "shell.execute_reply": "2024-12-07T01:18:02.862580Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "3ZPOifXCZuhg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:02.864484Z",
          "iopub.execute_input": "2024-12-07T01:18:02.864785Z",
          "iopub.status.idle": "2024-12-07T01:18:03.441965Z",
          "shell.execute_reply.started": "2024-12-07T01:18:02.864758Z",
          "shell.execute_reply": "2024-12-07T01:18:03.441008Z"
        },
        "outputId": "6c70bc45-f7f1-40f3-def6-85148c9b93ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 35,192,832 || all params: 529,225,600 || trainable%: 6.6499\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09 Training Model"
      ],
      "metadata": {
        "id": "CVr-LToX1XCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "id": "uhliEMyp1thd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:03.443546Z",
          "iopub.execute_input": "2024-12-07T01:18:03.443913Z",
          "iopub.status.idle": "2024-12-07T01:18:03.456662Z",
          "shell.execute_reply.started": "2024-12-07T01:18:03.443868Z",
          "shell.execute_reply": "2024-12-07T01:18:03.455938Z"
        },
        "outputId": "f53d37ed-ce61-4eb6-a240-81f8045761f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 350312320\nTrainable parameters : 35192832\nTrainable percentage: 10.05%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Xn5zb6xWJtu-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:03.457452Z",
          "iopub.execute_input": "2024-12-07T01:18:03.457768Z",
          "iopub.status.idle": "2024-12-07T01:18:03.467244Z",
          "shell.execute_reply.started": "2024-12-07T01:18:03.457738Z",
          "shell.execute_reply": "2024-12-07T01:18:03.466612Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 4\n",
        "max_steps = 200\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = 4,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = 4,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = True,\n",
        "  logging_steps = 20,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 20,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ],
      "metadata": {
        "id": "93ffvb0d4cG6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:03.468582Z",
          "iopub.execute_input": "2024-12-07T01:18:03.468875Z",
          "iopub.status.idle": "2024-12-07T01:18:03.509613Z",
          "shell.execute_reply.started": "2024-12-07T01:18:03.468833Z",
          "shell.execute_reply": "2024-12-07T01:18:03.508619Z"
        },
        "outputId": "c1d976ef-dde6-4fe3-da9a-b4af523b1fb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec07_01-18-03_483922eafc63,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset,#.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(200)),\n",
        "  dataset_text_field = 'prompt',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        "  peft_config = peft_config,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "EsKeJE3SMdk7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:03.510761Z",
          "iopub.execute_input": "2024-12-07T01:18:03.511136Z",
          "iopub.status.idle": "2024-12-07T01:18:04.778727Z",
          "shell.execute_reply.started": "2024-12-07T01:18:03.511095Z",
          "shell.execute_reply": "2024-12-07T01:18:04.777903Z"
        },
        "outputId": "cc2d55fd-0d9f-4073-bcc1-73274f738bdd"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n",
          "output_type": "stream"
        },
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<trl.trainer.sft_trainer.SFTTrainer at 0x7982da4d57e0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MZVoQX8V1cI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:18:04.779756Z",
          "iopub.execute_input": "2024-12-07T01:18:04.780026Z",
          "iopub.status.idle": "2024-12-07T01:31:49.320058Z",
          "shell.execute_reply.started": "2024-12-07T01:18:04.779999Z",
          "shell.execute_reply": "2024-12-07T01:31:49.319219Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Model Evaluation"
      ],
      "metadata": {
        "id": "v5N6fZsU1xiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ],
      "metadata": {
        "id": "5d6DT3o0113O",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:31:49.321236Z",
          "iopub.execute_input": "2024-12-07T01:31:49.321558Z",
          "iopub.status.idle": "2024-12-07T01:32:06.814484Z",
          "shell.execute_reply.started": "2024-12-07T01:31:49.321517Z",
          "shell.execute_reply": "2024-12-07T01:32:06.813572Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Save Model"
      ],
      "metadata": {
        "id": "PjTPWhCj4JQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "save_model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "OKAmko8h2VeV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:06.815715Z",
          "iopub.execute_input": "2024-12-07T01:32:06.816364Z",
          "iopub.status.idle": "2024-12-07T01:32:07.867041Z",
          "shell.execute_reply.started": "2024-12-07T01:32:06.816320Z",
          "shell.execute_reply": "2024-12-07T01:32:07.866359Z"
        },
        "outputId": "91997a0e-d17d-42f1-c9b5-9181a37fe226"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 Load PEFT Model"
      ],
      "metadata": {
        "id": "3NhWAM5h9Rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dlTaH2HoC26T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:07.868367Z",
          "iopub.execute_input": "2024-12-07T01:32:07.869132Z",
          "iopub.status.idle": "2024-12-07T01:32:08.156557Z",
          "shell.execute_reply.started": "2024-12-07T01:32:07.869089Z",
          "shell.execute_reply": "2024-12-07T01:32:08.155728Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = save_path + '/LoRA'\n",
        "peft_path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:08.157634Z",
          "iopub.execute_input": "2024-12-07T01:32:08.157914Z",
          "iopub.status.idle": "2024-12-07T01:32:08.170905Z",
          "shell.execute_reply.started": "2024-12-07T01:32:08.157888Z",
          "shell.execute_reply": "2024-12-07T01:32:08.170197Z"
        },
        "id": "Culy5BixrWiY",
        "outputId": "9ac534e9-050b-4431-dbe6-1d1ebc8dbe06"
      },
      "outputs": [
        {
          "execution_count": 42,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'./model/LoRA'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = PeftModel.from_pretrained(model, peft_path)"
      ],
      "metadata": {
        "id": "Nz2HT8nb9XJa",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:08.171917Z",
          "iopub.execute_input": "2024-12-07T01:32:08.172177Z",
          "iopub.status.idle": "2024-12-07T01:32:08.840746Z",
          "shell.execute_reply.started": "2024-12-07T01:32:08.172152Z",
          "shell.execute_reply": "2024-12-07T01:32:08.839787Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 Reload & Recheck Base Model"
      ],
      "metadata": {
        "id": "9AKHTM2HrWiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_name, base = False)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:08.841931Z",
          "iopub.execute_input": "2024-12-07T01:32:08.842206Z",
          "iopub.status.idle": "2024-12-07T01:32:10.818621Z",
          "shell.execute_reply.started": "2024-12-07T01:32:08.842180Z",
          "shell.execute_reply": "2024-12-07T01:32:10.817650Z"
        },
        "id": "KWTuTUCarWiY",
        "outputId": "4bcec294-e7af-46e7-c2bd-bfad3d1e4662"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"max_new_tokens\": 2048\n}\n\n",
          "output_type": "stream"
        },
        {
          "execution_count": 44,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:10.819752Z",
          "iopub.execute_input": "2024-12-07T01:32:10.820030Z",
          "iopub.status.idle": "2024-12-07T01:32:10.831457Z",
          "shell.execute_reply.started": "2024-12-07T01:32:10.820002Z",
          "shell.execute_reply": "2024-12-07T01:32:10.830477Z"
        },
        "id": "mWB6gJgjrWiY",
        "outputId": "fb64ddba-349e-4395-eb7b-ef874a5d7645"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 315119488\nTrainable parameters : 136178560\nTrainable percentage: 43.21%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:10.832830Z",
          "iopub.execute_input": "2024-12-07T01:32:10.833334Z",
          "iopub.status.idle": "2024-12-07T01:32:10.854391Z",
          "shell.execute_reply.started": "2024-12-07T01:32:10.833270Z",
          "shell.execute_reply": "2024-12-07T01:32:10.853514Z"
        },
        "id": "0kLmseI6rWiY",
        "outputId": "7107eade-304f-4a11-ad3e-6f7083c2d792"
      },
      "outputs": [
        {
          "execution_count": 46,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 896)\n        (layers): ModuleList(\n          (0-23): 24 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=896, bias=False)\n                  (default): Linear(in_features=64, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=128, bias=False)\n                  (default): Linear(in_features=64, out_features=128, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=128, bias=False)\n                  (default): Linear(in_features=64, out_features=128, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=896, bias=False)\n                  (default): Linear(in_features=64, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=4864, bias=False)\n                  (default): Linear(in_features=64, out_features=4864, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=896, out_features=64, bias=False)\n                  (default): Linear(in_features=896, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=4864, bias=False)\n                  (default): Linear(in_features=64, out_features=4864, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4864, out_features=896, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4864, out_features=64, bias=False)\n                  (default): Linear(in_features=4864, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=896, bias=False)\n                  (default): Linear(in_features=64, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:10.855440Z",
          "iopub.execute_input": "2024-12-07T01:32:10.855731Z",
          "iopub.status.idle": "2024-12-07T01:32:10.880394Z",
          "shell.execute_reply.started": "2024-12-07T01:32:10.855705Z",
          "shell.execute_reply": "2024-12-07T01:32:10.879366Z"
        },
        "id": "1oJTmvoKrWiZ",
        "outputId": "513eda0f-833c-4912-aa3f-f83b2d9c06d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total parameters : 385505152\nTrainable parameters : 0\nTrainable percentage: 0.00%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14 Pre Test & Post Test"
      ],
      "metadata": {
        "id": "GrXYkyb89UJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_assistant(prompt):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:10.881540Z",
          "iopub.execute_input": "2024-12-07T01:32:10.881786Z",
          "iopub.status.idle": "2024-12-07T01:32:10.891915Z",
          "shell.execute_reply.started": "2024-12-07T01:32:10.881762Z",
          "shell.execute_reply": "2024-12-07T01:32:10.891026Z"
        },
        "id": "viFvcOoNrWiZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def post_assistant(prompt):\n",
        "  inputs = tokenizer(\n",
        "  [\n",
        "    prompt_format.format(\n",
        "      prompt,\n",
        "      ''\n",
        "    )\n",
        "  ], return_tensors = 'pt').to(device)\n",
        "  generation_config = GenerationConfig(\n",
        "    do_sample = True,\n",
        "    top_k = 1,\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 1024,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "  outputs = peft_model.generate(\n",
        "    **inputs,\n",
        "    generation_config = generation_config\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "lgVU8Ci9RMu6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:10.893040Z",
          "iopub.execute_input": "2024-12-07T01:32:10.893623Z",
          "iopub.status.idle": "2024-12-07T01:32:10.903372Z",
          "shell.execute_reply.started": "2024-12-07T01:32:10.893594Z",
          "shell.execute_reply": "2024-12-07T01:32:10.902659Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def print_side_by_side(pre_text, post_text, width = 50):\n",
        "  pre_wrapped = textwrap.wrap(pre_text, width)\n",
        "  post_wrapped = textwrap.wrap(post_text, width)\n",
        "\n",
        "  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n",
        "  print(\n",
        "    str(sum(p.numel() for p in model.parameters())).center(width),\n",
        "    '|',\n",
        "    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n",
        "  )\n",
        "  print('=' * width, '|', '=' * width)\n",
        "\n",
        "  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n",
        "    print(pre.ljust(width), ' | ', post.ljust(width))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:10.904362Z",
          "iopub.execute_input": "2024-12-07T01:32:10.904644Z",
          "iopub.status.idle": "2024-12-07T01:32:10.915292Z",
          "shell.execute_reply.started": "2024-12-07T01:32:10.904619Z",
          "shell.execute_reply": "2024-12-07T01:32:10.914473Z"
        },
        "id": "DfZZGR2vrWiZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "JlEhdEGGTN6T",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:42:36.928690Z",
          "iopub.execute_input": "2024-12-07T01:42:36.929575Z",
          "iopub.status.idle": "2024-12-07T01:43:07.627699Z",
          "shell.execute_reply.started": "2024-12-07T01:42:36.929538Z",
          "shell.execute_reply": "2024-12-07T01:43:07.626815Z"
        },
        "outputId": "c735666c-153b-4966-c74e-2432daac314c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Apple sold 100 iPhones at their New   |  ### Question: Apple sold 100 iPhones at their New \nYork store today for an average cost of $1000.      |  York store today for an average cost of $1000.    \nThey also sold 20 iPads for an average cost of      |  They also sold 20 iPads for an average cost of    \n$900 and a certain number of Apple TVs for an       |  $900 and a certain number of Apple TVs for an     \naverage cost of $200. The average cost across all   |  average cost of $200. The average cost across all \nproducts sold today was $670. How many Apple TVs    |  products sold today was $670. How many Apple TVs  \nwere sold? ### Answer: To find the number of Apple  |  were sold? ### Answer: To find the number of Apple\nTVs sold, we can use the formula for the average    |  TVs sold, we can use the formula for the average  \ncost of a product:  Average cost = (Number of       |  cost of a product:  Average cost = (Number of     \nproducts sold * Average cost per product) / Number  |  products sold * Average cost per product) / Number\nof products  We know the average cost of iPhones    |  of products  We know the average cost of iPhones  \nis $1000 and the number of iPhones sold is 100, so  |  is $1000 and the number of iPhones sold is 100, so\nwe can plug these values into the formula:  $670 =  |  we can plug these values into the formula:  $670 =\n(100 * $1000) / Number of products  Now, we can     |  (100 * $1000) / Number of products  Now, we can   \nsolve for the number of products sold:  Number of   |  solve for the number of products sold:  Number of \nproducts sold = (100 * $1000) / $670  Number of     |  products sold = (100 * $1000) / $670  Number of   \nproducts sold = 100000 / $670  Number of products   |  products sold = 100000 / $670  Number of products \nsold = 141.42857142857143  Since we cannot have a   |  sold = 142.41  Since we cannot sell a fraction of \nfraction of a product, we can round this to the     |  a product, we can round this to the nearest whole \nnearest whole number:  Number of products sold =    |  number, which is 142 products sold.  Now, we can  \n141  So, 141 Apple TVs were sold today.             |  find the number of Apple TVs sold by subtracting  \n                                                    |  the number of iPhones sold from the total number  \n                                                    |  of products sold:  Number of Apple TVs sold =     \n                                                    |  Total number of products sold - Number of iPhones \n                                                    |  sold  Number of Apple TVs sold = 142 - 100  Number\n                                                    |  of Apple TVs sold = 42  So, 42 Apple TVs were sold\n                                                    |  today.                                            \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BxmnFTADTQsT",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T02:27:23.081909Z",
          "iopub.execute_input": "2024-12-07T02:27:23.082273Z",
          "iopub.status.idle": "2024-12-07T02:28:08.870143Z",
          "shell.execute_reply.started": "2024-12-07T02:27:23.082239Z",
          "shell.execute_reply": "2024-12-07T02:28:08.869169Z"
        },
        "outputId": "f94917c9-d967-4055-b740-a16c8e340ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Oula and Tona work for a delivery     |  ### Question: Oula and Tona work for a delivery   \nservice company, supplying different parts of       |  service company, supplying different parts of     \ntheir state with milk products. They are paid a     |  their state with milk products. They are paid a   \ncertain amount for each delivery made in a day. In  |  certain amount for each delivery made in a day. In\na particular month, Oula made 96 deliveries while   |  a particular month, Oula made 96 deliveries while \nTona made 3/4 times as many deliveries as Oula.     |  Tona made 3/4 times as many deliveries as Oula.   \nTheir difference in pay that month was $2400. How   |  Their difference in pay that month was $2400. How \nmuch are they paid for each delivery? ### Answer:   |  much are they paid for each delivery? ### Answer: \nTo find out how much Oula and Tona are paid for     |  To find out how much Oula and Tona are paid for   \neach delivery, we need to follow these steps:  1.   |  each delivery, we need to follow these steps:  1. \nCalculate Tona's number of deliveries. 2.           |  Calculate Tona's number of deliveries. 2.         \nDetermine the difference in pay between Oula and    |  Determine the difference in pay between Oula and  \nTona. 3. Calculate Oula's pay for each delivery.    |  Tona. 3. Calculate Oula's pay for each delivery.  \n4. Calculate Tona's pay for each delivery.  Let's   |  4. Calculate Tona's pay for each delivery.  Let's \nstart with the calculations:  1. Oula made 96       |  start with the calculations:  1. Oula made 96     \ndeliveries. 2. Tona made 3/4 times as many          |  deliveries. 2. Tona made 3/4 times as many        \ndeliveries as Oula, so Tona made \\( \\frac{3}{4}     |  deliveries as Oula, so Tona made \\( \\frac{3}{4}   \n\\times 96 = 72 \\) deliveries. 3. The difference in  |  \\times 96 = 72 \\) deliveries. 3. The difference in\npay between Oula and Tona is $2400. Since Oula      |  pay between Oula and Tona is $2400. Since they are\nmade 96 deliveries, her pay for each delivery is    |  paid the same amount for each delivery, we can    \n\\( \\frac{2400}{96} \\) dollars. 4. Tona's pay for    |  calculate Tona's pay for each delivery by dividing\neach delivery is \\( \\frac{2400}{72} \\) dollars.     |  the difference by the number of deliveries Tona   \nLet's calculate these values using Python code.     |  made:    \\[    \\text{Tona's pay for each delivery}\n```python # Number of deliveries made by Oula and   |  = \\frac{2400}{72} = 33.33 \\text{ (rounded to two  \nTona olula_deliveries = 96 tona_deliveries = (3/4)  |  decimal places)}    \\] 4. Oula's pay for each     \n* olula_deliveries  # Difference in pay             |  delivery is the same as Tona's pay for each       \ndifference_in_pay = 2400  # Calculate Oula's pay    |  delivery, so Oula's pay for each delivery is also \nfor each delivery olula_pay_per_delivery =          |  $33.33.  Therefore, Oula and Tona are paid $33.33 \ndifference_in_pay / olula_deliveries  # Calculate   |  for each delivery.                                \nTona's pay for each delivery tona_pay_per_delivery  |                                                    \n= difference_in_pay / tona_deliveries               |                                                    \nprint((olula_deliveries, tona_deliveries,           |                                                    \ndifference_in_pay, olula_pay_per_delivery,          |                                                    \ntona_pay_per_delivery)) ``` ```output (96, 72.0,    |                                                    \n2400.0, 250.0, 33.333333333333336) ``` The          |                                                    \ncalculations are as follows:  1. Oula made 96       |                                                    \ndeliveries. 2. Tona made 72 deliveries. 3. The      |                                                    \ndifference in pay between Oula and Tona is $2400.   |                                                    \n4. Oula's pay for each delivery is approximately    |                                                    \n$250. 5. Tona's pay for each delivery is            |                                                    \napproximately $33.33.  So, the final answer is: \\[  |                                                    \n\\boxed{250 \\text{ dollars per delivery for Oula, }  |                                                    \n33.33 \\text{ dollars per delivery for Tona}} \\]     |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "RDONbvZKTTqs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T01:32:14.762045Z",
          "iopub.execute_input": "2024-12-07T01:32:14.762357Z",
          "iopub.status.idle": "2024-12-07T01:32:49.259742Z",
          "shell.execute_reply.started": "2024-12-07T01:32:14.762316Z",
          "shell.execute_reply": "2024-12-07T01:32:49.258890Z"
        },
        "outputId": "a743dd5c-556a-4f17-bf5d-96f47f3ba8e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Libby is building an igloo in her     |  ### Question: Libby is building an igloo in her   \nbackyard using bricks of snow. She builds her       |  backyard using bricks of snow. She builds her     \nigloo in rows, using a total of 10 rows of bricks   |  igloo in rows, using a total of 10 rows of bricks \nof snow. The bottom half of the igloo has a         |  of snow. The bottom half of the igloo has a       \ncertain number of bricks of snow in each row while  |  certain number of bricks of snow in each row while\nthe top half of the igloo has 8 bricks of snow in   |  the top half of the igloo has 8 bricks of snow in \neach row. Libby used 100 bricks of snow for her     |  each row. Libby used 100 bricks of snow for her   \nigloo. How many bricks of snow are in each row of   |  igloo. How many bricks of snow are in each row of \nthe bottom half of the igloo? ### Answer: To solve  |  the bottom half of the igloo? ### Answer: To solve\nthis problem, we need to determine the number of    |  this problem, we need to determine the number of  \nbricks of snow in each row of the bottom half of    |  bricks of snow in each row of the bottom half of  \nthe igloo. Let's break down the steps:  1.          |  the igloo. Let's break down the steps:  1.        \n**Determine the total number of rows in the         |  **Determine the total number of rows in the       \nigloo:**    - Libby built the igloo with 10 rows    |  igloo:**    - Libby built the igloo with 10 rows  \nof bricks of snow.  2. **Determine the number of    |  of bricks of snow.  2. **Determine the number of  \nrows in the bottom half of the igloo:**    - Since  |  rows in the bottom half of the igloo:**    - Since\nthe bottom half has half the number of rows as the  |  the bottom half has half the number of rows as the\ntop half, we need to divide the total number of     |  top half, we need to divide the total number of   \nrows by 2 to find the number of rows in the bottom  |  rows by 2 to find the number of rows in the bottom\nhalf.    - Number of rows in the bottom half = 10   |  half.    - Number of rows in the bottom half = 10 \nrows / 2 = 5 rows.  3. **Determine the number of    |  rows / 2 = 5 rows.  3. **Determine the number of  \nbricks of snow in each row of the bottom half:**    |  bricks of snow in each row of the bottom half:**  \n- Each row in the bottom half has 100 bricks of     |  - Each row in the bottom half has 100 bricks of   \nsnow.  4. **Calculate the number of bricks of snow  |  snow.    - Since the bottom half has 5 rows, we   \nin each row of the bottom half:**    - Since each   |  divide the total number of bricks of snow by the  \nrow has 100 bricks of snow and there are 5 rows in  |  number of rows to find the number of bricks of    \nthe bottom half, the number of bricks of snow in    |  snow per row.    - Number of bricks of snow per   \neach row is \\( \\frac{100 \\text{ bricks}}{5 \\text{   |  row = 100 bricks / 5 rows = 20 bricks per row.    \nrows}} = 20 \\text{ bricks per row} \\).  Therefore,  |  Therefore, the number of bricks of snow in each   \nthe number of bricks of snow in each row of the     |  row of the bottom half of the igloo is 20 bricks. \nbottom half of the igloo is \\(\\boxed{20}\\).         |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "p7dBkB_7TjZY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T02:47:25.022483Z",
          "iopub.execute_input": "2024-12-07T02:47:25.023423Z",
          "iopub.status.idle": "2024-12-07T02:48:16.546468Z",
          "shell.execute_reply.started": "2024-12-07T02:47:25.023367Z",
          "shell.execute_reply": "2024-12-07T02:48:16.545576Z"
        },
        "outputId": "88d8856f-c8e0-4013-f28d-829a330a12fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: Jenny wants to read 3 books in a      |  ### Question: Jenny wants to read 3 books in a    \ncertain number of days. She can read 100 words per  |  certain number of days. She can read 100 words per\nhour. The first book has 200 words, the second      |  hour. The first book has 200 words, the second    \nbook has 400 words, and the third book has 300      |  book has 400 words, and the third book has 300    \nwords. She should spend, on average, 54 minutes     |  words. She should spend, on average, 54 minutes   \nper day reading. Over how many days does she plan   |  per day reading. Over how many days does she plan \nto read these books? ### Answer: To solve this      |  to read these books? ### Answer: To solve this    \nproblem, we need to determine the total number of   |  problem, we need to determine the total number of \ndays it will take for Jenny to read all three       |  days it will take for Jenny to read all three     \nbooks, given her reading speed and the time she     |  books, given her reading speed and the time she   \nspends per day. Here's the step-by-step reasoning:  |  spends per day. Here's the step-by-step reasoning:\n1. **Calculate the total number of words in all     |  1. **Calculate the total number of words in all   \nthree books:**    - First book: 200 words    -      |  three books:**    - First book: 200 words    -    \nSecond book: 400 words    - Third book: 300 words   |  Second book: 400 words    - Third book: 300 words \n- Total words = 200 + 400 + 300 = 900 words  2.     |  - Total words = 200 + 400 + 300 = 900 words  2.   \n**Determine the reading speed in words per hour:**  |  **Determine the reading speed in words per hour:**\n- Jenny's reading speed is 100 words per hour.  3.  |  - Jenny's reading speed is 100 words per hour.  3.\n**Calculate the total time required to read all     |  **Calculate the total time required to read all   \nthree books:**    - Time per day = 54 minutes    -  |  three books:**    - Time per day = 54 minutes    -\nTotal time for all three books = 54 minutes/day *   |  Convert minutes to hours: 54 minutes / 60 minutes \n3 days = 162 minutes  4. **Convert the total time   |  per hour = 0.9 hours per day    - Total time for  \nfrom minutes to hours:**    - 162 minutes = 162 /   |  all three books = 0.9 hours/day * 3 days = 2.7    \n60 hours    - Total hours required = 162 / 60  5.   |  hours  4. **Calculate the average reading time per\n**Calculate the average reading time per day:**     |  day:**    - Average reading time per day = Total  \n- Average reading time per day = Total hours        |  time for all three books / Number of days    -    \nrequired / Number of days  Let's implement this in  |  Average reading time per day = 2.7 hours / 3 days \nPython to get the exact number of days required.    |  = 0.9 hours per day  5. **Determine the number of \n```python # Constants words_per_hour = 100          |  days it will take to read all three books:**    - \nfirst_book_words = 200 second_book_words = 400      |  Number of days = Total words / Average reading    \nthird_book_words = 300 days = 3                     |  time per day    - Number of days = 900 words / 0.9\nreading_speed_minutes_per_hour = 54  # Total words  |  hours = 1000 days  Therefore, Jenny will need to  \nin all books total_words = first_book_words +       |  read these books over 1000 days to complete them. \nsecond_book_words + third_book_words  # Total time  |                                                    \nrequired in minutes total_time_minutes = days *     |                                                    \nreading_speed_minutes_per_hour  # Convert total     |                                                    \ntime to hours total_time_hours =                    |                                                    \ntotal_time_minutes / 60  # Calculate average        |                                                    \nreading time per day average_reading_time_per_day   |                                                    \n= total_time_hours / days                           |                                                    \nprint(average_reading_time_per_day) ``` ```output   |                                                    \n5.4 ``` The calculation shows that Jenny will need  |                                                    \nto read the first book for approximately 5.4        |                                                    \nhours, the second book for approximately 5.4        |                                                    \nhours, and the third book for approximately 5.4     |                                                    \nhours to read all three books. Therefore, over the  |                                                    \nnumber of days she plans to read these books, she   |                                                    \nwill spend an average of \\(\\boxed{5.4}\\) hours per  |                                                    \nday.                                                |                                                    \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "loc = randint(0, len(dataset))\n",
        "prompt = dataset[loc]['question']\n",
        "pre_text = pre_assistant(prompt)\n",
        "post_text = post_assistant(prompt)\n",
        "print_side_by_side(pre_text, post_text)"
      ],
      "metadata": {
        "id": "BUPoJAmnTnCq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-07T02:49:33.765285Z",
          "iopub.execute_input": "2024-12-07T02:49:33.765674Z",
          "iopub.status.idle": "2024-12-07T02:50:10.708223Z",
          "shell.execute_reply.started": "2024-12-07T02:49:33.765643Z",
          "shell.execute_reply": "2024-12-07T02:50:10.707268Z"
        },
        "outputId": "4b77fceb-db52-4dec-d7e9-0747d762db6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                     PRE-TEST                       |                      POST-TEST                     \n                    315119488                      |                     385505152                     \n================================================== | ==================================================\n### Question: A cleaning company produces two       |  ### Question: A cleaning company produces two     \nsanitizer sprays. One spray kills 50% of germs,     |  sanitizer sprays. One spray kills 50% of germs,   \nand another spray kills 25% of germs. However, 5%   |  and another spray kills 25% of germs. However, 5% \nof the germs they kill are the same ones. What      |  of the germs they kill are the same ones. What    \npercentage of germs would be left after using both  |  percentage of germs would be left after using both\nsanitizer sprays together? ### Answer: To solve     |  sanitizer sprays together? ### Answer: To solve   \nthis problem, we need to calculate the percentage   |  this problem, we need to calculate the percentage \nof germs that are killed by each sanitizer spray    |  of germs that are killed by each sanitizer spray  \nand then find the percentage of germs that are      |  and then find the percentage of germs that are    \nleft after using both sprays together.  First,      |  left after using both sprays together.  First,    \nlet's calculate the percentage of germs killed by   |  let's calculate the percentage of germs killed by \neach sanitizer spray: - The first spray kills 50%   |  each sanitizer spray: - The first spray kills 50% \nof germs, so it kills 50% of the germs it kills. -  |  of germs, so it kills 50% of the germs it kills. -\nThe second spray kills 25% of germs, so it kills    |  The second spray kills 25% of germs, so it kills  \n25% of the germs it kills. - However, 5% of the     |  25% of the germs it kills. - However, 5% of the   \ngerms killed by the first spray are also killed by  |  germs killed by the first spray are also killed by\nthe second spray, so we need to subtract 5% from    |  the second spray, so we need to subtract 5% from  \nthe total percentage of germs killed by each        |  the total percentage of germs killed by each      \nspray.  Now, let's calculate the percentage of      |  spray.  Now, let's calculate the percentage of    \ngerms left after using both sprays together: - The  |  germs that are left after using both sprays       \nfirst spray kills 50% of germs, so it kills 50% of  |  together: - The first spray kills 50% of germs, so\nthe germs it kills. - The second spray kills 25%    |  it kills 50% of the germs it kills. - The second  \nof germs, so it kills 25% of the germs it kills. -  |  spray kills 25% of germs, so it kills 25% of the  \nThe total percentage of germs killed by both        |  germs it kills. - The total percentage of germs   \nsprays is 50% + 25% = 75%. - The percentage of      |  killed by both sprays is 50% + 25% = 75%. - The   \ngerms left after using both sprays together is      |  percentage of germs that are left after using both\n100% - 75% = 25%.  Therefore, 25% of the germs      |  sprays together is 100% - 75% = 25%.  Therefore,  \nwould be left after using both sanitizer sprays     |  25% of the germs would be left after using both   \ntogether.                                           |  sanitizer sprays together.                        \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}