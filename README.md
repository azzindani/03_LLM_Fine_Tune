[![Gmail](https://img.shields.io/badge/Gmail-D14836?logo=gmail&logoColor=white)](mailto:422indani@gmail.com)
[![LinkedIn](https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff)](https://www.linkedin.com/in/azzindan1/)
[![GitHub Pages](https://img.shields.io/badge/GitHub%20Pages-121013?logo=github&logoColor=white)](https://azzindani.github.io/)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-FFD21E?logo=huggingface&logoColor=000)](https://huggingface.co/Azzindani)
[![GitHub](https://img.shields.io/badge/GitHub-%23121011.svg?logo=github&logoColor=white)](https://github.com/azzindani)
---

# ðŸ”§ LLM LoRA Fine-Tuning

This repository provides a step-by-step guide and notebook to fine-tune **Large Language Models (LLMs)** using **LoRA (Low-Rank Adaptation)** â€” an efficient technique for training large models with limited resources.

---

## ðŸ“˜ What is LoRA?

**LoRA** fine-tunes only a small number of additional weights while freezing the main model. This allows:
- Faster training
- Lower memory usage (GPU-friendly)
- Reusability across different tasks

---

## ðŸš€ How to Run (Colab/Kaggle)

### Google Colab
1. Upload the `.ipynb` notebook to [Colab](https://colab.research.google.com).
2. Enable GPU:  
   `Runtime > Change runtime type > Hardware accelerator > GPU`
3. Run all cells.

### Kaggle Notebooks
1. Upload the `.ipynb` file to [Kaggle Notebooks](https://www.kaggle.com/code).
2. Enable GPU and Internet Access.
3. Run all cells.

---
